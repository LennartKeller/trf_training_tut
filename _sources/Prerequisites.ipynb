{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0160197",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import set_caching_enabled\n",
    "set_caching_enabled(False)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6, compact=True)\n",
    "print = pp.pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318f0ad9",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "The following experiments share the same general logic, but the concrete implementation will differ in minor details since each framework has another structural approach.\n",
    "So before we start, we will take a brief look at the general logic for the data loading parts of the experiment and the computation of the loss function.\n",
    "\n",
    "## Dataset-preparation\n",
    "\n",
    "To load the stories, shuffle the sentences, and further prepare, we use Huggingface's Datasets library, which provides various useful functions for manipulating text data.\n",
    "Because Huggingface Datasets are fully compatible with PyTorch's class for data-loading, they can also be used by all non-Huggingface libraries without further adjustments.\n",
    "\n",
    "The preparation itself is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755b43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c3f60",
   "metadata": {},
   "source": [
    "At first, we load the dataset in its original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1413df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_csv('../scripts/data/ROCStories_winter2017 - ROCStories_winter2017.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cedc90f",
   "metadata": {},
   "source": [
    "We got 52.665 stories. Each one has a length of five sentences. Additionally, each text has a short title, but we discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38013432",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23634f3",
   "metadata": {},
   "source": [
    "Next, we create the training data by shuffling the sentences and creating labels indicating the original order. Also, we add special tokens to each sentence.\n",
    "\n",
    "We implement the shuffling process using the `.map`-method of the `Dataset`-class.\n",
    "Following the library's out-of-place policy, the `.map`-method returns a new dataset containing the changes instead of changing the dataset it was called on.\n",
    "\n",
    "The `.map`-method has two modes: batch-mode or single entry mode. Either way, it receives a dictionary as input where each key represents a column of the dataset.\n",
    "In single entry mode, the values of the input dictionary hold one entry in the dataset.\n",
    "In batch mode, the values are lists containing more than one entry.\n",
    "The following function only works in both modes since it converts both input formats to the same intermediate form, but in general, the batch mode should be preferred to save time.\n",
    "The output of the function has to be a dictionary in the same format as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from random import seed as set_seed\n",
    "\n",
    "def make_shuffle_func(sep_token):\n",
    "    def shuffle_stories(entries, seed=42):\n",
    "        set_seed(seed)\n",
    "        entries_as_dicts = [\n",
    "            dict(zip(entries, values))\n",
    "            for values in zip(*entries.values())\n",
    "        ]\n",
    "        converted_entries = []\n",
    "        for entry in entries_as_dicts:\n",
    "            sents = [\n",
    "                entry[key]\n",
    "                for key in sorted(\n",
    "                    [key for key in entry.keys() if key.startswith('sentence')\n",
    "                    ], key=lambda x: int(x[-1])\n",
    "                )\n",
    "            ]\n",
    "            sent_idx = list(range(len(sents)))\n",
    "            sents_with_idx = list(zip(sents, sent_idx))\n",
    "            shuffle(sents_with_idx)\n",
    "            text = f'{sep_token} ' + f' {sep_token} '.join(\n",
    "                [s[0]for s in sents_with_idx]\n",
    "            ) \n",
    "            so_targets = [s[1] for s in sents_with_idx]\n",
    "            shuffled_entry = {'text': text, 'so_targets': so_targets}\n",
    "            converted_entries.append(shuffled_entry)\n",
    "        new_entry = {\n",
    "            key: [entry[key] for entry in converted_entries]\n",
    "            for key in converted_entries[0]\n",
    "        }\n",
    "        return new_entry\n",
    "    return shuffle_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecd26ba",
   "metadata": {},
   "source": [
    "`[CLS]` is one of the special tokens of models directly descending from BERT. During the pretraining stage, it learns a representation of the whole input sequence and thus only occurs once in each input.\n",
    "Since we do not need a representation of the input as a whole, we use it as the special sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119b52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func = make_shuffle_func('[CLS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aebc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(map_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5e2cfd",
   "metadata": {},
   "source": [
    "After applying the shuffle function, the dataset has two additional columns. The `text` column contains the shuffled and concatenated sentences, and the `so_targets` column contains the indices of the sentences in the original order. For example, in the first text in the dataset, the first sentence in the shuffled text is 4th place in the original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cfc5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ddae4",
   "metadata": {},
   "source": [
    "Lastly, we want to split our dataset into three subsets.\n",
    "The train-set is used for training.\n",
    "The validation set can be used to validate the performance during training or hyperparameter optimization.\n",
    "The test set will be used for the final evaluation of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef78bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "test_validation = train_test['test'].train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_validation['train'],\n",
    "    'val': test_validation['test']})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55249e3",
   "metadata": {},
   "source": [
    "Finally, we save the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03859833",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('rocstories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca32e37",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "! rm -r rocstories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc46b44a",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "```{figure} ./figures/DataFlow.png\n",
    "---\n",
    "name: fig-dataflow\n",
    "---\n",
    "High-level visualization of data-flow from the original dataset to the model.\n",
    "```\n",
    "\n",
    "From a high-level view, a Huggingface `Dataset` can be seen as a table with columns that correspond to attributes (called features) and rows representing one dataset entry.\n",
    "From a more concrete technical perspective, the `Dataset`-instance provides an iterable that yields a dictionary for each entry in the dataset. Each dictionary contains attribute-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba26835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk('../scripts/data/rocstories')\n",
    "print(dataset['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe7824",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19af83a6",
   "metadata": {},
   "source": [
    "We can't feed the model with raw texts, so we have to tokenize them beforehand.\n",
    "\n",
    "As stated before, each model comes with a custom tokenizer, so we have to load it, just like the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbfd7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', return_dict=True)\n",
    "tokenized_text = tokenizer(\"Jimmy went down the road.\")\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceedd232",
   "metadata": {},
   "source": [
    "The tokenizer takes a text or a collection of texts and converts it to a tokenized sequence. Also, it creates additional inputs for the model, such as the attention mask.\n",
    "\n",
    "To tokenize the whole dataset, we can once again use the map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd091b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenization_func(tokenizer, text_column, *args, **kwargs):\n",
    "    def tokenization(entry):\n",
    "        return tokenizer(entry[text_column], *args, **kwargs)\n",
    "    return tokenization\n",
    "\n",
    "tokenization = make_tokenization_func(\n",
    "    tokenizer=tokenizer,\n",
    "    text_column=\"text\",\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors='np'\n",
    ")\n",
    "\n",
    "dataset = dataset.map(tokenization, batched=True)\n",
    "print(dataset['train'][0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b263515e",
   "metadata": {},
   "source": [
    "To feed the data to the neural network, we have to split it up into batches of a fixed size.\n",
    "To do so, PyTorch provides a general class, called `torch.utils.data.DataLoader`, that takes in iterable and returns batches just in time while training.\n",
    "\n",
    "The `DataLoader` class is agnostic towards the data it receives. To create batches that are compatible with the Huggingface model, we have to pass it a function that takes in multiple entries from our dataset and converts them into the correct format.\n",
    "\n",
    "This function is called `collate_fn` and can be specified while initiating the `DataLoader` object.\n",
    "Using a simple identity function, we see that the `collate_fn` receives a list with $B$ entries where $B$ is the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def identity(batch):\n",
    "    return batch\n",
    "\n",
    "data_loader = DataLoader(dataset['train'], batch_size=2, collate_fn=identity)\n",
    "batch = next(iter(data_loader))\n",
    "print(len(batch))\n",
    "print(type(batch))\n",
    "print(batch[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73da9fb",
   "metadata": {},
   "source": [
    "Huggingface provides a collate function that can convert tokenized data into batches in a suitable format.\n",
    "The Huggingface collation function only works with numeric data such as scalars or arrays. So we have to drop all texts before we pass the dataset into the `Dataloader` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b07c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\n",
    "    [\"text\", \"storyid\", \"storytitle\"] + [f\"sentence{i}\" for i in range(1, 6)]\n",
    ")\n",
    "dataset.set_format(\"torch\")\n",
    "print(dataset[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e52060",
   "metadata": {},
   "source": [
    "After only numeric data is left, we have to face the last problem in the collation problem.\n",
    "The Huggingface collation function only handles arrays of the same shape when collating them into one batch. In theory (e.g., with other datasets), we could have a varying number of labels if we wanted to work shuffled texts with a variable number of sentences. We tackle this problem by introducing a custom collation function to make our preparation pipeline as flexible as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ed4f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def so_data_collator(batch_entries, label_key='so_targets'):\n",
    "    \"\"\"\n",
    "    Custom dataloader to apply padding to the labels.\n",
    "    \"\"\"\n",
    "    label_dicts = []\n",
    "\n",
    "    # We split the labels from the rest to process them independently\n",
    "    for entry in batch_entries:\n",
    "        label_dict = {}\n",
    "        for key in list(entry.keys()):\n",
    "            if label_key in key:\n",
    "                label_dict[key] = entry.pop(key)\n",
    "        label_dicts.append(label_dict)\n",
    "\n",
    "    # Everything except our labels can easily be handled by the \"default collator\"\n",
    "    batch = default_data_collator(batch_entries)\n",
    "\n",
    "    # We need to pad the labels 'manually'\n",
    "    for label in label_dicts[0]:\n",
    "        labels = pad_sequence(\n",
    "            [label_dict[label] for label_dict in label_dicts],\n",
    "            batch_first=True,\n",
    "            padding_value=-100,\n",
    "        )\n",
    "\n",
    "        batch[label] = labels\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ffe608",
   "metadata": {},
   "source": [
    "This function used the Huggingface default collation function to handle everything except the labels. The labels are padded with a batch-wise max length strategy and added to the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14513d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset['train'], batch_size=2, collate_fn=so_data_collator)\n",
    "batch = next(iter(data_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054529d3",
   "metadata": {},
   "source": [
    "Now the data is in the correct format for training.\n",
    "\n",
    "## Loss function\n",
    "\n",
    "As stated in the experimental design, we use a plain Mean-Squared-Error regression loss. Since we only want to consider the special tokens, we must select them before the actual computation. Therefore, we need the `input_ids` to figure out their position in the sequence.\n",
    "\n",
    "To compute the loss for one single batch, we add the loss scores of all sentences of one text and take the average of all batch entries.\n",
    "Due to computational constraints, transformer-based language models typically have a limit on the input size. So our inputs might have to be truncated to fit into the model. In this case, we discard the labels for the sentences left out and only consider the data that fits into the model.\n",
    "\n",
    "The following listing contains a general implementation of the loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c6d08",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def sentence_ordering_loss(batch_logits, batch_targets, batch_input_ids, target_token_id) -> torch.Tensor:\n",
    "    # Since we have varying number of labels per instance, we need to compute the loss manually for each one.\n",
    "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "    batch_loss = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n",
    "    for labels, logits, input_ids in zip(\n",
    "        batch_labels, batch_logits, batch_input_ids\n",
    "    ):\n",
    "        # Firstly, we need to convert the sentence indices to regression targets.\n",
    "        # Also we need to remove the padding entries (-100)\n",
    "        true_labels = labels[labels != -100].reshape(-1)\n",
    "        targets = true_labels.float()\n",
    "\n",
    "        # Secondly, we need to get the logits from each target token in the input sequence\n",
    "        target_logits = logits[input_ids == target_token_id].reshape(-1)\n",
    "\n",
    "        # Sometimes, we will have less target_logits than targets due to trunction of the input.\n",
    "        # In this case, we just consider as many targets as we have logits\n",
    "        if target_logits.size(0) < targets.size(0):\n",
    "            targets = targets[: target_logits.size(0)]\n",
    "\n",
    "        # Finally we compute the loss for the current instance and add it to the batch loss\n",
    "        batch_loss = batch_loss + loss_fn(targets, target_logits)\n",
    "\n",
    "    # The final loss is obtained by averaging over the number of instances per batch\n",
    "    loss = batch_loss / batch_logits.size(0)\n",
    "\n",
    "    return loss"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   23,
   37,
   39,
   43,
   46,
   50,
   54,
   56,
   69,
   104,
   109,
   113,
   115,
   119,
   121,
   128,
   138,
   142,
   147,
   150,
   164,
   171,
   173,
   179,
   185,
   191,
   209,
   219,
   230,
   235,
   241,
   246,
   277,
   281,
   285,
   298
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}