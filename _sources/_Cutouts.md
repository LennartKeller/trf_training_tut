
<!-- Pretraining, a transformer-based neural language model from scratch, requires vast amounts of texts and computational resources.
Because of that, most of the time a, publicly available pretrained model is only finetuned to a specific task.
The most popular source for pretrained language models is Huggingface Transformer ecosystem.
Huggingface is a startup that provides a great variety of different already pretrained language models for free alongside with some additional libraries and tools to use them in all kinds of different contexts.
While Hugginface tries to make its libraries and models work with all modern deep learning libraries, its origins lie in the PyTorch ecosystem.
PyTorch is the most popular framework in academia and used for the majority of publications in the field () -->