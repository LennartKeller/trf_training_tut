
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Experimental Results &#8212; A comparing guide to train language models with Python.</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"processClass": "tex2jax_process|mathjax_process|math|output_area"}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Conclusion" href="Conclusion.html" />
    <link rel="prev" title="Poutyne" href="Poutyne.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">A comparing guide to train language models with Python.</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Home.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="HuggingfaceEcosystem.html">
   The Huggingface ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Experiment.html">
   Experimental Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Prerequisites.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="HuggingFaceTrainer.html">
   Huggingface Trainer
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="PyTorchLightning.html">
   PyTorch Lightning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Poutyne.html">
   Poutyne
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Experimental Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Conclusion.html">
   Conclusion
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/ExperimentalResults.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/LennartKeller/trf_training_tut"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/LennartKeller/trf_training_tut/issues/new?title=Issue%20on%20page%20%2FExperimentalResults.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparemters">
   Hyperparemters
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#results">
   Results
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="experimental-results">
<h1>Experimental Results<a class="headerlink" href="#experimental-results" title="Permalink to this headline">¶</a></h1>
<p>To see if our custom task architecture is able to order the sentences, we run the experiment using the ROCStories Dataset with three different pretrained language models.
Also, we run the experiment using the same set of parameters with each framework to check if they perform consistently or if any under-the-hood magic influences the scores.</p>
<div class="section" id="hyperparemters">
<h2>Hyperparemters<a class="headerlink" href="#hyperparemters" title="Permalink to this headline">¶</a></h2>
<p>We employ <code class="docutils literal notranslate"><span class="pre">distilbert-base-cased</span></code>, <code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code>, and <code class="docutils literal notranslate"><span class="pre">roberta-base</span></code> as pretrained models.
Intuitively, we expect <code class="docutils literal notranslate"><span class="pre">roberta-Base</span></code> to perform best, with <code class="docutils literal notranslate"><span class="pre">bert-case</span></code> reaching a tight second place and <code class="docutils literal notranslate"><span class="pre">distilbert</span></code> to fall short behind the large two models.
We use the AdamW optimizer with a learning rate of <span class="math notranslate nohighlight">\(3e-5\)</span>. We finetune for <span class="math notranslate nohighlight">\(3\)</span> epochs and validate our models on the test set while the validation set is only used for tracking the progress during training.
Furthermore, we use the same random seed across all models and frameworks. Due to varying model sizes, we use different batch sizes to fit the model on the GPU. To ensure that the batch sizes do not affect the performance, we use gradient accumulation with the Huggingface <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> and PyTorch Lightning to ensure that each model makes the same number of backward steps. For Poutyne, which does not support gradient accumulation, we chose the largest batch size possible for each model.
In addition to these basic parameters, each framework has a set custom parameter that we leave untouched and use the default configuration.</p>
</div>
<div class="section" id="results">
<h2>Results<a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h2>
<!--```{list-table}
:header-rows: 1
:name: results-table

* - Framework
  - Model
  - Loss
  - Accuracy
  - Kendall's Tau
* - Huggingface `Trainer`
  - `bert-base-cased`
  - 2.447
  - 0.699
  - 0.792
* -
  - `roberta-base`
  - 1.619
  - 0.786
  - 0.861
* -
  - `distilbert-base-cased`
  - 2.800
  - 0.653
  - 0.753
* - PyTorch Lightning
  - `bert-base-cased`
  - 2.621
  - 0.696
  - 0.785
* -
  - `roberta-base`
  - 1.755
  - 0.776
  - 0.853
* -
  - `distilbert-base-cased`
  - 2.933
  - 0.651
  - 0.748
* - Poutyne
  - `bert-base-cased`
  - 2.714
  - 0.687
  - 0.776
* -
  - `roberta-base`
  - 2.106
  - 0.748
  - 0.830
* -
  - `distilbert-base-cased`
  - 3.024
  - 0.645
  - 0.742
```
-->
<table class="colwidths-auto table" id="results-table">
<caption><span class="caption-number">Table 1 </span><span class="caption-text">Results</span><a class="headerlink" href="#results-table" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Framework</p></th>
<th class="text-align:center head"><p>Model</p></th>
<th class="text-align:center head"><p>Loss</p></th>
<th class="text-align:center head"><p>Accuracy</p></th>
<th class="text-align:center head"><p><span class="math notranslate nohighlight">\(\tau_{\textrm{Kendall}}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>Huggingface <code class="docutils literal notranslate"><span class="pre">Trainer</span></code></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code></p></td>
<td class="text-align:center"><p>2.447</p></td>
<td class="text-align:center"><p>0.699</p></td>
<td class="text-align:center"><p>0.792</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">roberta-base</span></code></p></td>
<td class="text-align:center"><p>1.619</p></td>
<td class="text-align:center"><p>0.786</p></td>
<td class="text-align:center"><p>0.861</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">distilbert-base-cased</span></code></p></td>
<td class="text-align:center"><p>2.800</p></td>
<td class="text-align:center"><p>0.653</p></td>
<td class="text-align:center"><p>0.753</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>PyTorch Lightning</p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code></p></td>
<td class="text-align:center"><p>2.621</p></td>
<td class="text-align:center"><p>0.696</p></td>
<td class="text-align:center"><p>0.785</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">roberta-base</span></code></p></td>
<td class="text-align:center"><p>1.755</p></td>
<td class="text-align:center"><p>0.776</p></td>
<td class="text-align:center"><p>0.853</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">distilbert-base-cased</span></code></p></td>
<td class="text-align:center"><p>2.933</p></td>
<td class="text-align:center"><p>0.651</p></td>
<td class="text-align:center"><p>0.748</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>Poutyne</p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code></p></td>
<td class="text-align:center"><p>2.714</p></td>
<td class="text-align:center"><p>0.687</p></td>
<td class="text-align:center"><p>0.776</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">roberta-base</span></code></p></td>
<td class="text-align:center"><p>2.106</p></td>
<td class="text-align:center"><p>0.748</p></td>
<td class="text-align:center"><p>0.830</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p></p></td>
<td class="text-align:center"><p><code class="docutils literal notranslate"><span class="pre">distilbert-base-cased</span></code></p></td>
<td class="text-align:center"><p>3.024</p></td>
<td class="text-align:center"><p>0.645</p></td>
<td class="text-align:center"><p>0.742</p></td>
</tr>
</tbody>
</table>
<p><a class="reference internal" href="#results-table"><span class="std std-numref">Table 1</span></a> shows the results of the runs. As expected, <code class="docutils literal notranslate"><span class="pre">roberta-base</span></code> achieved the best results. However, the margin by which it outperforms the standard <code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code> model is surprising. In contrast to Bert models, Roberta models do not use the next sentence prediction objective during their finetuning stages. Next sentence prediction is the task of deciding if two consecutive sentences in the input sequence are natural successors or not. Intuitively, the knowledge obtained from this task should help order sentences too. Obviously, without additional inspection, each further presumption is mere speculation. Still, it seems like the fact that Roberta models are pretrained on a larger dataset outweighs the missing next sentence objective during pretraining.</p>
<!--Maybe noch ein Satz zu Distilbert-->
<p>Comparing the results by each framework, the Huggingface <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> achieves a clear victory. It reaches the best results across all models and, depending on the metric, outperforms the other frameworks significantly. However, while PyTorch Lightning manages to keep up the the <code class="docutils literal notranslate"><span class="pre">Trainer,</span></code> Poutyne falls clear behind its competitors, especially when training the larger models.
This difference in performance indicates the benefit of gradient accumulation when training models that only allow small batch sizes.
However, although more subtle, the gap between the Huggingface <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> and PyTorch Lightning is surprising since both frameworks offer roughly the same feature set.
Different parameters can probably explain it since we only changed some parameters and used the default values provided by the developers for the rest.
However, due to its focus on language models, the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> should have a preset of parameters well optimized for language models.
The most plausible candidate for explaining the observed difference is the learning schedule. While PyTorch Lightning uses a fixed learning rate if not specified otherwise, the Huggingface <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> employs a learning scheme with a linear decay and additional warmup steps. This strategy, which is proven to be helpful when training large models, might cause the superior results of the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.</p>
<p>Despite their differences, it has to be stated that all frameworks achieved robust results.
The relative order of the different models is consistent across all frameworks. Therefore, the results of each framework are sufficient not to shade the capabilities of the models or language models in general for this task.</p>
<p>Comparing to the other works, which employ a pretrained language model, we beat the regression baseline from <span id="id1">Kumar <em>et al.</em> [<a class="reference internal" href="Bibliography.html#id9">2020</a>]</span> (<span class="math notranslate nohighlight">\(\tau_{\textrm{Kendall}}=0.736\)</span>) by quite a large margin but still fall clearly behind the current state of art model by <span id="id2">Zhu <em>et al.</em> [<a class="reference internal" href="Bibliography.html#id7">2021</a>]</span> (<span class="math notranslate nohighlight">\(\tau_{\textrm{Kendall}}=0.849\)</span>). Since the results of both papers were achieved using a standard Bert model, we also have to compare them against our Bert scores. By looking at the Roberta scores, it becomes clear that choosing a larger or better language model seems to have a large influence regardless of the approach. But all of these results have to be taken with a  grain of salt since we only ran the experiment once using a fixed random seed and a custom train-test-validation split, making our results less reliable.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Poutyne.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Poutyne</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Conclusion.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Conclusion</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Lennart Keller<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>