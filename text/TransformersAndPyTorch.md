# Hugginface Transformers
Huggingface provides the foundation for this work: The Transformers-Library. This library offers a great variety of different language models. The development started soon after the first transformers-based language models were published and quickly gained track in the world of NLP. It is open source so that everyone can submit new model architectures. Alongside the code for the models Huggingface also offers a Hub where one can upload checkpoints of pretrained models to share them with the community. Their explained goal is to be as fast as possible in adding new models or architectures. This strategy can be both a curse and a blessing. On the one hand, it enables researchers to quickly experiment out newly published models without relying on the often very narrowly designed original research implementations. On the other hand, this strategy also leads to frequent code changes that often lead to bugs or little quirks, which can be very annoying and time-consuming to fix, mainly because the documentation does not grow as fast as the number of new additions. Nonetheless, the Transformers library has become the defacto standard library for neural language models. This claim can easily be backed by the number of stars, forks, and pull requests at Github.
Originating as a pure PyTorch library, Hugginface widened its scope over the last two years and began integrating other deep learning frameworks such as Tensorflow or Flax. Since



<!--To make all the different model architectures as easily interchangeable as possible, their implementations follow consistent guidelines. Each model consists of three parts: A config object that holds all the different hyperparameters and other configurations, a tokenizer, and the language model itself. This ensemble can be initiated either from scratch or from a pretrained checkpoint (either a local one or a checkpoint from the model hub).-->

