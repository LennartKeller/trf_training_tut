# Introduction

Transformer-based neural networks have revolutionized the world of NLP.
Their ability to process dependencies within texts and gain language processing abilities via self-supervised pretraining allowed them to set-state-of the art results across many different tasks.
Thus they are of great interest for many different scientific disciplines and the industry.
Pretraining, a transformer-based language model, is costly and requires vast amounts of computational resources.

The most popular source for pretrained language models is Huggingface Transformer Ecosystem. Huggingface is a startup that provides a great variety of different pretrained language models for free alongside an open-source library to use them.
The Huggingface Transformers library itself is based on PyTorch. PyTorch is the most dominant deep learning framework for research in deep learning.
PyTorch combines a Tensor implementation, linear algebra routines on these tensors with an engine for automatic differentiation.

