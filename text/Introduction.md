# Introduction

Transformer-based neural language model have revolutionized the world of NLP.
Their ability to process long range dependencies within texts and to gain language processing abilities via self-supervised pretraining allowed them to set-state-of the art results across many different tasks.
Due to their superior performance they are widely used across many scientific disciplines and in the industry.
The most popular provider of pretrained language models is Huggingface, a startup which provides a variety of different models alongside with a set of libraries to use them.
Although most of the time is is not necessary to train a language model from scratch it has to be finetuned to the desired task and/ or domain.
Finetuning a transformer-based language model (or training any neural network in general) is a complex process which requires all sorts of different steps and tricks to make it efficient und no less important reproducible.
There are numerous frameworks which try to faciliate this process and to offer help for complex steps like distributed training.
The goal of this work is to 


<!-- Pretraining, a transformer-based neural language model from scratch, requires vast amounts of texts and computational resources.
Because of that, most of the time a, publicly available pretrained model is only finetuned to a specific task.
The most popular source for pretrained language models is Huggingface Transformer ecosystem.
Huggingface is a startup that provides a great variety of different already pretrained language models for free alongside with some additional libraries and tools to use them in all kinds of different contexts.
While Hugginface tries to make its libraries and models work with all modern deep learning libraries, its origins lie in the PyTorch ecosystem.
PyTorch is the most popular framework in academia and used for the majority of publications in the field () -->

# Methodology



