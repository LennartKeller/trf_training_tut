{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a05dcc",
   "metadata": {},
   "source": [
    "# PyTorch Lightning\n",
    "\n",
    "In contrast to the Huggingface `Trainer`, which handles the complete training itself, PyTorch Lightning ({cite:t}`falcon2019pytorchlightning`) takes a different approach.\n",
    "It not only aims at handling the training but also at structuring the creation of a model too.\n",
    "Its main goal is not to hide complexity from the user but to provide a well-structured API for building neural networks of all kinds.\n",
    "The most striking aspect of this is that in PyTorch Lightning's philosophy, a model and its inference-, training and prediction logic are not separate things that can be exchanged independently.\n",
    "Instead, it binds all these parts directly to the model itself.\n",
    "In doing so, PyTorch-Lightning does not make any assumptions on the nature of the model or the training itself. Thus it allows covering many tasks and domains with maximum flexibility.\n",
    "\n",
    "However, this approach comes at the cost that the user again must implement many things manually.\n",
    "Naturally, this approach is keener to researchers who implement and test custom models, while practitioners who only want to employ pre-built models must deal with some implementational overhead.\n",
    "PyTorch Lightning's steep learning curve compounds this issue.\n",
    "However, there is exhaustive documentation with many tutorials (as texts and videos), best practices, and user guides on building various models across different domains.\n",
    "Also, if an experiment is implemented in PyTorch Lightning, there are a lot of helpful tweaks and techniques to improve or speed up the training.\n",
    "So that it can be worthwhile even when using pre-built models.\n",
    "These facilitation features include tweaks like training with half-precision, automatic tuning of the learning rate, and integrations into hyperparameter tuning frameworks or creating command-line interfaces to control the parameters.\n",
    "In addition to that, there is support for different computational backends that help to dispatch the training on multiple accelerators like GPUs and TPUs.\n",
    "If these features are not enough, there is a growing ecosystem of third-party extensions, widening the scope and functionality of the framework.\n",
    "\n",
    "\n",
    "\n",
    "## Classes\n",
    "From a technical point of view, Pytorch Lightning provides an API composed of three different main classes, dividing the training process into a sequence of single atomic steps.\n",
    "These classes implement the model, the logic for storing and processing the training data, and the training process itself.\n",
    "\n",
    "### `LightningModule`\n",
    "\n",
    "A subclass of a `LightningModule` implements the model.\n",
    "A `LightningModule` is an extended version of PyTorch's `nn.Module` class.\n",
    "`nn.Modules` are the basic building blocks of neural networks in PyTorch. In essence, they store a set of parameters, for example, weights of a single layer alongside with `.forward`-method that defines the computational logic when data flows through the module. They are designed to work recursively. One module can be composed of several submodules so that each building block of a neural network, starting from single layers up to a complete network, can be implemented in this one class.\n",
    "The following listing shows an exemplary implementation of a simple linear layer as `nn.Module`.\n",
    "By chaining multiple instances of the dense layer in a `nn. Sequential` class, it is possible to create a simple feed-forward network. \n",
    "This network is again a subclass of the `nn.Module` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b677cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"Fully connected linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(in_shape, out_shape), requires_grad=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return torch.matmul(inputs, self.weights)\n",
    "\n",
    "network = nn.Sequential(\n",
    "    DenseLayer(512, 16),\n",
    "    nn.ReLU(),\n",
    "    DenseLayer(16, 8),\n",
    "    nn.ReLU(),\n",
    "    DenseLayer(8, 2)\n",
    ")\n",
    "\n",
    "inputs = torch.randn(8, 512)  # Batchsize 8\n",
    "outputs = network(inputs)\n",
    "print(outputs.size())\n",
    "print(issubclass(nn.Sequential, nn.Module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84166179",
   "metadata": {},
   "source": [
    "A `LightningModule` is intended to replace the outmost `nn.Module` instance of a model, which holds the complete network.\n",
    "It extends the `nn.Module` class with new methods, designed to structure not only the logic of a single forward pass but also other steps like a complete train-, test- or validation-steps.\n",
    "With this extension, it becomes possible to define a single forward step through the network and how the models should be trained and tested as well.\n",
    "In essence, it provides a way to incorporate the training loop into the model itself.\n",
    "This strategy has one massive advantage over the standard PyTorch practice of writing an external function that implements the training loop.\n",
    "It helps to make the model self-contained, meaning that it holds all necessary logic itself. This property alleviates sharing models since only one class carries all information to train and test the model.\n",
    "\n",
    "#### Training, Testing, Validation\n",
    "\n",
    "The methods for training, testing and prediction are called `.train_step`,  `.validation_step` and `.test_step` respectively.\n",
    "They all define how a single batch of data should be handled for these steps.\n",
    "Typically the `.train_step`-method computes the loss score, and the other two methods compute other validation metrics.\n",
    "Design-wise, only the `.train_step`-method is required to return the loss averaged loss score for the current batch.\n",
    "The test- and validation methods are not required to return anything. Instead, they can use the built-in logging capabilities of the `LightningModule`.\n",
    "Similar to the `.train_step`-method, they should return their scores averaged over the complete batch too.\n",
    "\n",
    "#### Model Hyperparameters and checkpoints\n",
    "\n",
    "Much effort has been put into organizing the hyperparameters of an experiment.\n",
    "Like the train and test routines, PyTorch Lightning binds all hyperparameters that control the model directly to the object.\n",
    "This strategy ensures that saved models also contain the combination of parameters used for training.\n",
    "There are two ways to define the hyperparameters of a model.\n",
    "By default, each argument in the signature of the model's constructor is regarded as a hyperparameter.\n",
    "By calling a `.save_hyperparameters`-method in the constructor, these arguments get serialized into a `.hparams`-attribute.\n",
    "The `.hparams`-attribute is saved as a `YAML`-file for each checkpoint of the model, making it easy to see which parameters were used without loading the whole model.\n",
    "If the constructor contains non-hyperparameters arguments, these can be excluded from serialization using the saving method's `ignore` flag.\n",
    "\n",
    "Another more explicit way of defining the parameters of a model is to store them all in a dictionary into the constructor and to pass this dictionary to the `-save_hyperparameters`-method.\n",
    "This strategy is suitable in cases where many arguments of the constructor are non-hyperparameters.\n",
    "\n",
    "#### Logging\n",
    "\n",
    "Logging in PyTorch-Lightning is a two-stage procedure.\n",
    "Inside the `LightningModule`, various metrics can be logged at different steps while training using the `.log`- or `log_dict`-methods.\n",
    "The `.log`-methods can log a single score, while the `log-dict`-method can log multiple scores stored in a dictionary (with names as keys and the score as values).\n",
    "These logs are extracted by `Trainer` and written out in various formats (see Trainer section for further details.)\n",
    "One benefit of using an autonomous logging function is that it gives flexibility to the user in deciding when to log which metrics, for example, making it possible to log something only when a condition applies.\n",
    "\n",
    "\n",
    "### `LighningDataModule`\n",
    "\n",
    "`PyTorchLightning` also comes with a custom solution to bundle data-related operations into a single object.\n",
    "It is called `LighningDataModule` and should contain the code to load and prepare the data for training and testing.\n",
    "A class derived from `LighntningDataModule` must implement four required methods.\n",
    "The `.prepare_data`-method should implement all steps required to load the data and convert it into a correct representation for the model.\n",
    "To return the splits for training, testing and evaluation, there are `.train|.test|.val_loader`-methods. Each of them has to return a `DataLoader` object.\n",
    "Like the `LightningModule`, its data counterpart has the advantage of holding all code to load and prepare the data, which alleviates distribution and publication.\n",
    "Another key feature of the `LightningDataModule` is its ability to adapt to distributed environments.\n",
    "While the `.prepare_data`-method is called once at the beginning of the training, there are also additional `.setup`- and `teardown`-methods.\n",
    "These methods can define operations pre- or post-training data-preparation steps that must be performed independently on each accelerator.\n",
    "\n",
    "### `Trainer`\n",
    "\n",
    "The `Trainer` object handles the actual training.\n",
    "It receives the model and data (wrapped in Lightning modules) alongside all training-specific hyperparameters, like the number of epochs, the devices to train on, or a list of loggers to log the progress.\n",
    "\n",
    "The `Trainer` exposes four high-level methods to the user. Each of them triggers either the training, the validation, the prediction of unseen instances, and the hyperparameter-tuning.\n",
    "Like the `LightningModule`, an instance of the `Trainer` is initialized with all hyperparameters relevant to the training, like the batch size or a number of epochs.\n",
    "\n",
    "The different stages of the training (training and testing)\n",
    "\n",
    "#### Extending the `Trainer`\n",
    "\n",
    "In contrast to the `LightningModule` and `LightningDataModule` the `Trainer` itself is not intended to be customized in any way. Since their respective objects contain all model or data-related code, the `Trainer` is better kept untouched.\n",
    "Instead, if necessary, the functions of the `Trainer` can be extended with callbacks and plugins. Both of them can add custom operations to different stages of the training.\n",
    "Callbacks implement steps that are not strictly necessary for training. Instead, they can be used to define things like logging or applying non-essential operations to the model (i.e., weigh pruning after each epoch) that add new functions but are not required to perform training.\n",
    "On the other hand, plugins are meant to extend the `Trainer` with new functionalities like adding support for new accelerators or computational backends. So by their scope, they are meant to be used by experienced users who need to extend the Trainer.\n",
    "However, since their API is still in beta and subject to changes in the future, it should be used with caution.\n",
    "\n",
    "Also, it contains a handful of tweaks to improve the results, like gradient accumulation or gradient .\n",
    "In addition to that, the `Trainer` also supports tuning the learning rate and batch size out of the box. Both tuning features must be enabled while initializing the `Trainer` and can be invoked by calling the `.tune`-method.\n",
    "\n",
    "#### Logging\n",
    "\n",
    "While the model defines what measures are logged, the `Trainer` is responsible for writing out these logs.\n",
    "By default, it logs the standard output.\n",
    "In addition to that, it can be extended with additional loggers.\n",
    "PyTorch Lightning provides built-in loggers that log the progress to Tensorboard or other services like Weights and Biases.\n",
    "Further loggers, can be implemented using the Logger base-class.\n",
    "Multiple loggers are passed to the `Trainer` during initialization as a list.\n",
    "\n",
    "### CLI Interface\n",
    "\n",
    "PyTorch Lightning supports the creation of command-line interfaces through the `LightninArgumentParser` class.\n",
    "This class is an extended version of the parser from the `jsonargparse` module, and it can parse the arguments of Lightning classes and other classes out-of-the-box.\n",
    "This feature enables adding parameters of different modules to the parser effortlessly.\n",
    "\n",
    "If more flexibility is needed, for example, when only some parameters of an object should be added to the parser, the best practice is to add a method to the object, which adds these arguments to the parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23a71c",
   "metadata": {
    "name": "lightning-parser-listing",
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "parser.add_lightning_class_args(ModelCheckpoint, \"checkpoint\")\n",
    "parser.add_class_arguments(TensorBoardLogger, nested_key=\"tensorboard\")\n",
    "parser.add_lightning_class_args(Trainer, \"trainer\")\n",
    "parser = PlLanguageModelForSequenceOrdering.add_model_specific_args(parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d5440a",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Model\n",
    "\n",
    "Since we do not build our model from scratch, we need to load the pretrained transformer in the LightningModules's constructor.\n",
    "To be able to load different models, we introduce the name of the model as hyperparameters.\n",
    "Since the model is pretrained, we only have to specify two other hyperparameters, namely the learning rate and the id of the target token.\n",
    "Because Huggingface models are also subclasses of the  `nn.Module` class, loading the transformer model works flawlessly, and the language model is recognized as a submodule of the `PlLanguageModelForSequenceOrdering` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcba6a",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class PlLanguageModelForSequenceOrdering(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.hparams[\"model_name_or_path\"],\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "            num_labels=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db3c266",
   "metadata": {},
   "source": [
    "Next, we define a single forward step. Again, the logic is pretty simple since we only need to exclude the labels from the inputs for the language model and pass the rest of the input data to the language model to obtain the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ddc8d0",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def forward(self, inputs: Dict[Any, Any]) -> Dict[Any, Any]:\n",
    "        # We do not want to compute token classification loss, so we remove the labels temporarily\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = self.base_model(**inputs)\n",
    "\n",
    "        # And reattach them later on ...\n",
    "        inputs[\"labels\"] = labels\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98fc030",
   "metadata": {},
   "source": [
    "Because we want to compute the loss while training and validating the model, we factor out the loss function into a separate method.\n",
    "Implementation-wise, the loss function is only slightly variated from the original implementation. The only changes are that we retrieve the target token id from the hyperparameters of the model.\n",
    "Also, we draw inspiration from the `transformers` API and add a custom version of the forward method. This method computes both the forward step and the loss. The loss is then attached to the output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa42db",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def _compute_loss(self, batch_labels, batch_logits, batch_input_ids) -> float:\n",
    "        # Since we have varying number of labels per instance, \n",
    "        # we need to compute the loss manually for each one.\n",
    "        loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "        batch_loss = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n",
    "        for labels, logits, input_ids in zip(\n",
    "            batch_labels, batch_logits, batch_input_ids\n",
    "        ):\n",
    "\n",
    "            # Firstly, we need to convert the sentence indices to regression targets.\n",
    "            # To avoid exploding gradients, we norm them to be in range 0 <-> 1.\n",
    "            # labels = labels / labels.max()\n",
    "            # Also we need to remove the padding entries (-100).\n",
    "            true_labels = labels[labels != -100].reshape(-1)\n",
    "            targets = true_labels.float()\n",
    "\n",
    "            # Secondly, we need to get the logits \n",
    "            # from each target token in the input sequence\n",
    "            target_logits = logits[\n",
    "                input_ids == self.hparams[\"target_token_id\"]\n",
    "            ].reshape(-1)\n",
    "\n",
    "            # Sometimes we will have less target_logits \n",
    "            # than targets due to truncation of the input.\n",
    "            # In this case, we just consider as many targets as we have logit.\n",
    "            if target_logits.size(0) < targets.size(0):\n",
    "                targets = targets[: target_logits.size(0)]\n",
    "\n",
    "            # Finally we compute the loss for the current instance \n",
    "            # and add it to the batch loss.\n",
    "            batch_loss = batch_loss + loss_fn(targets, target_logits)\n",
    "\n",
    "        # The final loss is obtained by averaging \n",
    "        # over the number of instances per batch.\n",
    "        loss = batch_loss / batch_logits.size(0)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _forward_with_loss(self, inputs):\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Get sentence indices\n",
    "        batch_labels = inputs[\"labels\"]\n",
    "        # Get logits from model\n",
    "        batch_logits = outputs[\"logits\"]\n",
    "        # Get logits for all cls tokens\n",
    "        batch_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        loss = self._compute_loss(\n",
    "            batch_labels=batch_labels,\n",
    "            batch_logits=batch_logits,\n",
    "            batch_input_ids=batch_input_ids,\n",
    "        )\n",
    "        outputs[\"loss\"] = loss\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62b71da",
   "metadata": {},
   "source": [
    "Using the `_foward_with_loss`-method implementing the `training_step`-method becomes relatively simple.\n",
    "The only thing left to do inside this method is to log the training loss in order to be able to monitor the progress during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef4b67",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def training_step(self, inputs: Dict[Any, Any], batch_idx: int) -> float:\n",
    "        outputs = self._forward_with_loss(inputs)\n",
    "        loss = outputs[\"loss\"]\n",
    "        self.log(\"loss\", loss, logger=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9d196",
   "metadata": {},
   "source": [
    "Like the `_compute_loss`-method, we only need to slightly adapt the validation metrics' computation to use the model's hyperparameters.\n",
    "Since we want to compute the identical scores for testing and validation, we can also use the `validation_step`-method for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d4a8c",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def validation_step(self, inputs, batch_idx):\n",
    "        outputs = self._forward_with_loss(inputs)\n",
    "\n",
    "        # Detach all torch.tensors and convert them to np.arrays.\n",
    "        for key, value in outputs.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                outputs[key] = value.detach().cpu().numpy()\n",
    "        for key, value in inputs.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                inputs[key] = value.detach().cpu().numpy()\n",
    "\n",
    "        # Get sentence indices\n",
    "        batch_labels = inputs[\"labels\"]\n",
    "        # Get logits from model\n",
    "        batch_logits = outputs[\"logits\"]\n",
    "        # Get logits for all cls tokens\n",
    "        batch_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        metrics = defaultdict(list)\n",
    "        for sent_idx, input_ids, logits in zip(\n",
    "            batch_labels, batch_input_ids, batch_logits\n",
    "        ):\n",
    "            sent_idx = sent_idx.reshape(-1)\n",
    "            input_ids = input_ids.reshape(-1)\n",
    "            logits = logits.reshape(-1)\n",
    "\n",
    "            sent_idx = sent_idx[sent_idx != 100]\n",
    "            target_logits = logits[input_ids == self.hparams[\"target_token_id\"]]\n",
    "            if sent_idx.shape[0] > target_logits.shape[0]:\n",
    "                sent_idx = sent_idx[: target_logits.shape[0]]\n",
    "\n",
    "            # Calling argsort twice on the logits \n",
    "            # gives us their ranking in ascending order.\n",
    "            predicted_idx = np.argsort(np.argsort(target_logits))\n",
    "            tau, pvalue = kendalltau(sent_idx, predicted_idx)\n",
    "            acc = accuracy_score(sent_idx, predicted_idx)\n",
    "            metrics[\"kendalls_tau\"].append(tau)\n",
    "            metrics[\"acc\"].append(acc)\n",
    "            metrics[\"mean_logits\"].append(logits.mean().item())\n",
    "            metrics[\"std_logits\"].append(logits.std().item())\n",
    "\n",
    "        metrics[\"loss\"] = outputs[\"loss\"].item()\n",
    "\n",
    "        # Add val prefix to each metric name and compute mean over the batch.\n",
    "        metrics = {\n",
    "            f\"val_{metric}\": np.mean(scores).item()\n",
    "            for metric, scores in metrics.items()\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, inputs, batch_idx):\n",
    "        return self.validation_step(inputs, batch_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47818d7d",
   "metadata": {},
   "source": [
    "Lastly, we need to implement the `configure_optimizers`-method and add the model's hyperparameter to the parser via the `add_model_specific_args`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ab7b93",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=self.hparams[\"lr\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\n",
    "            \"PlLanguageModelForSequenceOrdering\"\n",
    "            )\n",
    "        parser.add_argument(\n",
    "            \"--model.model_name_or_path\", type=str, default=\"bert-base-cased\"\n",
    "        )\n",
    "        parser.add_argument(\"--model.lr\", type=float, default=3e-5)\n",
    "        parser.add_argument(\"--model.target_token_id\", type=int, default=101)\n",
    "        return parent_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64b7c0",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "In contrast to the model class, we design our version of the `LightningDataModule` to work with any Huggingface `Dataset`.\n",
    "Most of the work is done by the `.prepare_data`-method, which implements the processing pipeline for the contained dataset\n",
    "Firstly, it applies all functions to prepare the data via the `.map`-method of the `Dataset` class.\n",
    "Afterward, the text data will be tokenized using the passed instance of the tokenizer.\n",
    "Lastly, it is ensured that the dataset's column containing the target is named `labels` to be compliant with standard `transformers` models.\n",
    "Additionally, we implement a method to use the map functionalities of the contained dataset directly.\n",
    "This method allows the manipulation of the data manually since the `.prepare_data`-method is automatically executed by the `Trainer`.\n",
    "The datasets wrapped in this class should already contain train-/ test- and validation-splits.\n",
    "To create batches of the data, we use the default collation function of the transformers library but allow passing a custom collation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d5748",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class HuggingfaceDatasetWrapper(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        text_column: str,\n",
    "        target_column: str,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        train_batch_size: int = 8,\n",
    "        eval_batch_size: int = 16,\n",
    "        mapping_funcs: List[Callable] = None,\n",
    "        collate_fn: Callable = default_data_collator,\n",
    "        train_split_name: str = \"train\",\n",
    "        eval_split_name: str = \"val\",\n",
    "        test_split_name: str = \"test\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.text_column = text_column\n",
    "        self.target_column = target_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.mapping_funcs = mapping_funcs\n",
    "        self.collate_fn = collate_fn\n",
    "        self.train_split_name = train_split_name\n",
    "        self.eval_split_name = eval_split_name\n",
    "        self.test_split_name = test_split_name\n",
    "\n",
    "    def prepare_data(self, tokenizer_kwargs: Dict[str, str] = None):\n",
    "        # 1. Apply user defined preparation functions\n",
    "        if self.mapping_funcs:\n",
    "            for mapping_func in self.mapping_funcs:\n",
    "                dataset = dataset.map(mapping_func, batched=True)\n",
    "\n",
    "        # 2. Tokenize the text\n",
    "        if tokenizer_kwargs is None:\n",
    "            tokenizer_kwargs = {\n",
    "                \"truncation\": True,\n",
    "                \"padding\": \"max_length\",\n",
    "                \"add_special_tokens\": False,\n",
    "            }\n",
    "        self.dataset = self.dataset.map(\n",
    "            lambda e: self.tokenizer(e[self.text_column], **tokenizer_kwargs),\n",
    "            batched=True,\n",
    "        )\n",
    "        # 3. Set format of important columns to torch\n",
    "        self.dataset.set_format(\n",
    "            \"torch\", columns=[\"input_ids\", \"attention_mask\", self.target_column]\n",
    "        )\n",
    "        # 4. If the target columns is not named 'labels' rename it\n",
    "        try:\n",
    "            self.dataset = self.dataset.rename_column(self.target_column, \"labels\")\n",
    "        except ValueError:\n",
    "            # target column should already have correct name\n",
    "            pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset[self.train_split_name],\n",
    "            batch_size=self.train_batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset[self.eval_split_name],\n",
    "            batch_size=self.eval_batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset[self.test_split_name],\n",
    "            batch_size=self.eval_batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def map(self, *args, **kwargs):\n",
    "        self.dataset = self.dataset.map(*args, **kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bafc5",
   "metadata": {},
   "source": [
    "### Complete code\n",
    "\n",
    "Once again, after factoring out the custom modules, the actual experiment can be implemented in relatively few lines of code.\n",
    "To control the experiment via the command line, we use the `LightningArgumentParser`.\n",
    "We initialize the parser with all arguments from the `Trainer,` `PlLanguageModelForSequenceOrdering`, and `HuggingfaceDatasetWrapper`.\n",
    "Additionally, we add more parameters to give each run a name and control the batch sizes for training and testing.\n",
    "Similar to implementing the experiment with the Huggingface `Trainer`, we need to ensure that the sentences contain the correct special tokens.\n",
    "Replacing these tokens if necessary can be done using the `.map`-method of the `HuggingfaceDatasetWrapper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c26cbc",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import basename\n",
    "from datasets import load_from_disk\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.cli import LightningArgumentParser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from pl_modules import (\n",
    "    HuggingfaceDatasetWrapper,\n",
    "    PlLanguageModelForSequenceOrdering,\n",
    "    so_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "def main(model_args, trainer_args, checkpoint_args, tensorboard_args, run_args):\n",
    "\n",
    "    seed_everything(run_args[\"seed\"])\n",
    "\n",
    "    print(\"Loading tokenizer.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args[\"model_name_or_path\"])\n",
    "\n",
    "    print(\"Loading datasets.\")\n",
    "    data = load_from_disk(\"../data/rocstories\")\n",
    "\n",
    "    # Downsampling for debugging...\n",
    "    # data = data.filter(lambda _, index: index < 10000, with_indices=True)\n",
    "\n",
    "    dataset = HuggingfaceDatasetWrapper(\n",
    "        data,\n",
    "        text_column=\"text\",\n",
    "        target_column=\"so_targets\",\n",
    "        tokenizer=tokenizer,\n",
    "        mapping_funcs=[],\n",
    "        collate_fn=so_data_collator,\n",
    "        train_batch_size=run_args[\"train_batch_size\"],\n",
    "        eval_batch_size=run_args[\"val_batch_size\"],\n",
    "    )\n",
    "\n",
    "    if tokenizer.cls_token != \"[CLS]\":\n",
    "        print(\n",
    "            f\"Model does not a have a [CLS] token. Updating the data with token {tokenizer.cls_token} ...\"\n",
    "        )\n",
    "\n",
    "        def replace_cls_token(entry):\n",
    "            texts = entry[\"text\"]\n",
    "            replaced_texts = []\n",
    "            for text in texts:\n",
    "                replaced_texts.append(text.replace(\"[CLS]\", tokenizer.cls_token))\n",
    "            entry[\"text\"] = replaced_texts\n",
    "            return entry\n",
    "\n",
    "        dataset = dataset.map(replace_cls_token, batched=True)\n",
    "        model_args[\"target_token_id\"] = tokenizer.cls_token_id\n",
    "\n",
    "    print(\"Loading model.\")\n",
    "    model = PlLanguageModelForSequenceOrdering(hparams=model_args)\n",
    "\n",
    "    print(\"Initializing trainer.\")\n",
    "    # Init logger\n",
    "    tensorboard_logger = TensorBoardLogger(**tensorboard_args)\n",
    "\n",
    "    # Init callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(**checkpoint_args)\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    # Remove default args\n",
    "    trainer_args.pop(\"logger\")\n",
    "    trainer_args.pop(\"callbacks\")\n",
    "    trainer = Trainer(logger=tensorboard_logger, callbacks=callbacks, **trainer_args)\n",
    "\n",
    "    print(\"Start training.\")\n",
    "    trainer.fit(model=model, datamodule=dataset)\n",
    "\n",
    "    print(\"Start testing.\")\n",
    "    test_results = trainer.test(model=model, datamodule=dataset, ckpt_path=None)\n",
    "    with open(f\"test_results_{model_args['model_name_or_path']}.json\", \"w\") as f:\n",
    "        json.dump(test_results, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = LightningArgumentParser()\n",
    "    group = parser.add_argument_group()\n",
    "    group.add_argument(\"--run.run_name\", type=str, default=basename(__file__))\n",
    "    group.add_argument(\"--run.seed\", type=int, default=0)\n",
    "    group.add_argument(\"--run.train_batch_size\", type=int, default=8)\n",
    "    group.add_argument(\"--run.val_batch_size\", type=int, default=16)\n",
    "\n",
    "    parser.add_lightning_class_args(ModelCheckpoint, \"checkpoint\")\n",
    "    parser.add_class_arguments(TensorBoardLogger, nested_key=\"tensorboard\")\n",
    "    parser.add_lightning_class_args(Trainer, \"trainer\")\n",
    "    parser = PlLanguageModelForSequenceOrdering.add_model_specific_args(parser)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model_args = args.get(\"model\", {})\n",
    "    trainer_args = args.get(\"trainer\", {})\n",
    "    checkpoint_args = args.get(\"checkpoint\", {})\n",
    "    tensorboard_args = args.get(\"tensorboard\", {})\n",
    "    run_args = args.get(\"run\", {})\n",
    "\n",
    "    main(model_args, trainer_args, checkpoint_args, tensorboard_args, run_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9aa449",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Pytorch Lightning's goal is not to hide complexity from the user. Instead, it provides an API that helps to structure the complexity into a sequence of single steps.\n",
    "This approach is constructive when designing custom models from scratch or implementing new training regimes that differ from the standard training loop.\n",
    "This flexibility comes at the cost of friendliness to beginners. People who have little experience with PyTorch itself will quickly be overwhelmed by PyTorch Lightning API with vast possibilities to customize steps manually.\n",
    "Even though the documentation is extensive and covers nearly all aspects of the library in great detail, it can be frustrating sometimes that there are multiple ways to achieve the same behavior, and there is little to no guidance in choosing between the different parts.\n",
    "Like most modern deep learning frameworks, PyTorch Lightning is rapidly evolving, and thus many parts of it are either in beta and subject to significant changes in the future or deprecated. Unfortunately, this is also noticeable when searching the web for further advice since many tips or tutorials quickly become outdated.\n",
    "Nevertheless, despite these limitations for beginners, experienced users can benefit from using PyTorch Lightning. Not only because of the additional features like built-in logging, tuning, or other tweaks but mainly because the well-thought API enforces them to write self-contained models that contain all the logic for experimenting with them.\n",
    "This approach effortlessly enables sharing of models and also alleviates maintainability."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "source_map": [
   12,
   48,
   74,
   165,
   172,
   183,
   195,
   199,
   209,
   215,
   273,
   278,
   285,
   290,
   345,
   349,
   365,
   379,
   462,
   474,
   580
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}