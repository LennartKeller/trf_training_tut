{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47f3a623",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import set_caching_enabled\n",
    "set_caching_enabled(False)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6, compact=True)\n",
    "print = pp.pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d79e0b",
   "metadata": {},
   "source": [
    "# Poutyne\n",
    "\n",
    "Compared to the other two frameworks, Poutyne ({cite:t}`poutyne`) has a more narrow scope.\n",
    "Instead of trying to make the training of a fixed set of models as easy as possible like Huggingface `Trainer`, or facilitating the creation and training of custom models like PyTorch Lightning, it tries to bring the ease of the Keras API from the realms of Tensorflow to the world of PyTorch.\n",
    "The benefits of the Keras API are its simplicity and orientation at well-established machine learning frameworks like Scikit-Learn.\n",
    "This simplicity lowers the barrier of entry for beginners because it lowers the amount of time needed to get hands-on training for their first model.\n",
    "The following exemplary listing shows the typical workflow in Poutyne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8bd950",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from poutyne import Model\n",
    "\n",
    "...\n",
    "\n",
    "network = make_network()\n",
    "X_train, y_train = load_data(subset=\"train\")\n",
    "X_val, y_val = load_data(subset=\"validation\")\n",
    "X_test, y_test = load_data(subset=\"test\")\n",
    "\n",
    "model = Model(\n",
    "    network,\n",
    "    \"sgd\",\n",
    "    \"cross_entropy\",\n",
    "    batch_metrics=[\"accuracy\"],\n",
    "    epoch_metrics=[\"f1\"],\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5932e82b",
   "metadata": {},
   "source": [
    "Like Keras, Poutyne automates many steps for standard cases like the optimizer configuration or the loss function.\n",
    "However, Poutyne does not mimic the whole Keras API but only the training part.\n",
    "The model’s creation still has to be done in plain PyTorch, which is generally a bit trickier than Keras because the dimensions of all layers have to be chosen manually.\n",
    "In addition to the training functions, Poutyne also provides utilities to conduct and save whole experiments and utilities for creating checkpoints, logging, scheduling of the learning rate, and multi-device training.\n",
    "\n",
    "## Classes\n",
    "\n",
    "### Model\n",
    "\n",
    "The `Model` class wraps a neural network alongside an optimizer, loss function, and validation metrics.\n",
    "It exposes `.fit`-, `.evaluate`-, and `.predict`-methods for training, evaluation, and inference.\n",
    "Each of these methods exits in different variations that consume the data either as a list of batches, PyTorch `Dataset`, or as a generator yielding batches.\n",
    "Additional hyperparameters, like the batch size, or the number of epochs to train, can be passed the methods directly.\n",
    "\n",
    "### Experiment\n",
    "\n",
    "The `Experiment` class is an extended version of the `Model` class that comes with helpful additions for conducting deep learning experiments.\n",
    "Like the `Model` class, an `Experiment` is equipped with the neural network, optimizer, loss function, and metrics into a single object and has methods to start the training, evaluation, or prediction.\n",
    "In contrast to the `Model` class, which only intends to do basic training, the `Experiment` class provides additional features to organize and track the progress.\n",
    "For example, it supports logging the progress to various formats, like a CSV table or Tensorboard.\n",
    "Monitoring allows the `Experiment` class to save checkpoints of the model that perform best concerning one of the validation metrics.\n",
    "Also, it saves all the intermediate results and tracked values to the disk by default.\n",
    "The `Experiment` class can automatically configure all metrics and the loss function for the two primary task types, classification, and regression.\n",
    "\n",
    "### Data\n",
    "\n",
    "Poutyne is data agnostic meaning, that it does not provide any tooling to load, process, and store the training data.\n",
    "The only requirements are that the data comes in one of the supported formats and that each batch consists of two objects: one that holds the training data and one that contains the label.\n",
    "\n",
    "## Additional Features\n",
    "\n",
    "### Metrics\n",
    "\n",
    "Poutyne has a custom API for implementing metrics.\n",
    "It distinguishes between two types of metrics, batch metric, and epoch metrics.\n",
    "Batch metrics are computed per batch and averaged to obtain the results for one single epoch.\n",
    "Epoch metrics are computed on the gathered results of one entire epoch. Thus, they are a good choice for measures that would suffer from averaging over the batch results, like the F-score.\n",
    "Poutyne provides predefined metrics for both types. But, unfortunately, they only cover classification tasks.\n",
    "There are two options to add other metrics. Either they have to be implemented manually or taken from Scikit-Learn and made compatible using a built-in wrapper class.\n",
    "Metrics are passed to `Model` or `Experiment` during their initialization.\n",
    "\n",
    "### Callbacks\n",
    "\n",
    "Callbacks are intended to extend the functions of the `Model` or `Experiment` class. Like the callbacks from the other frameworks, they have access to the model’s current state and can perform actions at various steps while training.\n",
    "There are many predefined callbacks available that perform all kinds of tasks, ranging from logging, keeping track of gradients, scheduling the learning, creating checkpoints, to sending notifications to inform clients about the progress of the training.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Even though the `transformers` models are incompatible with vanilla Poutyne, integrating it does not require complicated changes.\n",
    "Most of the required adaptions change the data in order to convert betweeN the dictionary-based data model of the `transformers` library and Poutyne’s more classical `X, y` format for input data and targets.\n",
    "Since these changes are task agnostic, we factored most of these adaption tools out of the project into a small standalone library. [^gh-link]\n",
    "\n",
    "[^gh-link]: [poutyne-transformers](https://github.com/LennartKeller/poutyne-transformers)\n",
    "\n",
    "### Data\n",
    "\n",
    "We create a custom data collator to convert the data for an experiment from the Hugginface `Dataset` format into a Poutyne compliant representation.\n",
    "The main task of the collator is to convert each batch of dictionaries into batches containing tuples of training data and targets.\n",
    "To do so, the `TransformersCollator` copies one or multiple entries from the input dictionaries into the target objects. Depending on the number of keys, this object is either a single tensor or a dictionary.\n",
    "Additionally, with the `remove_labels`-parameter, the fields that get copied to the target object can be removed from the model's input. By default, they are retained in the input data. This functionality enables using the internal computation of the loss of standard models while also being able to use the built-in metrics of Poutyne for monitoring the training.\n",
    "Other collation operations are handled by the default collator from `transformers` or by a custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0f9911",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "class TransformerCollator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        y_keys: Union[str, List[str]] = None,\n",
    "        custom_collator: Callable = None,\n",
    "        remove_labels: bool = False,\n",
    "    ):\n",
    "        self.y_keys = y_keys\n",
    "        self.custom_collator = (\n",
    "            custom_collator if custom_collator is not None else default_data_collator\n",
    "        )\n",
    "        self.remove_labels = remove_labels\n",
    "\n",
    "    def __call__(self, inputs: Tuple[Dict]) -> Tuple[Dict, Any]:\n",
    "        batch_size = len(inputs)\n",
    "        batch = self.custom_collator(inputs)\n",
    "        if self.y_keys is None:\n",
    "            y = torch.tensor(float(\"nan\")).repeat(batch_size)\n",
    "        elif isinstance(self.y_keys, list):\n",
    "            y = {\n",
    "                key: batch.pop(key)\n",
    "                if \"labels\" in key and self.remove_labels\n",
    "                else batch.get(key)\n",
    "                for key in self.y_keys\n",
    "            }\n",
    "        else:\n",
    "            y = batch.pop(self.y_keys) if self.remove_labels else batch.get(self.y_keys)\n",
    "        return batch, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d14b04",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "As stated in the Prerequisites chapter, tokenizers return a dictionary of data that contains all data required to be fed into the language model unpacked as keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bafd3af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "inputs = tokenizer(\"Poutyne is inspired by Keras\", return_tensors=\"pt\")\n",
    "print(model(**inputs).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bcc2ae",
   "metadata": {},
   "source": [
    "Poutyne instead passes the data to the model in the same format it receives it.\n",
    "To make sure that the data is unpacked correctly, we create a wrapper class.\n",
    "It is also a subclass of the `nn. Module` to ensure that all parameters of the capsulated model can be accessed.\n",
    "Apart from the data handling, this class also exposes the custom `save_pretrained`-model of the underlying `transformers` model.\n",
    "This way, it is possible to create checkpoints of the trained model that can be loaded and used in the `transformers` ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63ab50",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, transformer: PreTrainedModel):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({repr(self.transformer)})\"\n",
    "\n",
    "    def forward(self, inputs) -> Dict[str, Any]:\n",
    "        return self.transformer(**inputs)\n",
    "\n",
    "    def save_pretrained(self, *args, **kwargs) -> None:\n",
    "        self.transformer.save_pretrained(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235a764a",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "In Poutyne the loss function receives the output of the model and the targets.\n",
    "When using default models, neither of both has to be used to obtain the loss, since we can extract the internal loss from the model's output.\n",
    "In our case we have to implement a function that computes the loss on our own.\n",
    "Since we do not have access to the model or the tokenizer, we have to create a loss function that stores the id of the current target token. For that, we opt for creating a class that holds this id as an attribute and computes the loss via its `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230b5289",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class PoutyneSequenceOrderingLoss:\n",
    "    def __init__(self, target_token_id):\n",
    "        self.target_token_id = target_token_id\n",
    "\n",
    "    def __call__(self, outputs, targets) -> float:\n",
    "        batch_labels = targets[\"labels\"]\n",
    "        batch_logits = outputs[\"logits\"]\n",
    "        batch_input_ids = targets[\"input_ids\"]\n",
    "\n",
    "        # Since we have varying number of labels per instance, we need to compute the loss manually for each one.\n",
    "        loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "        batch_loss = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n",
    "        for labels, logits, input_ids in zip(\n",
    "            batch_labels, batch_logits, batch_input_ids\n",
    "        ):\n",
    "            # Firstly, we need to convert the sentence indices to regression targets.\n",
    "            # To avoid exploding gradients, we norm them to be in range 0 <-> 1\n",
    "            # Also we need to remove the padding entries (-100)\n",
    "            true_labels = labels[labels != -100].reshape(-1)\n",
    "            targets = true_labels.float()\n",
    "\n",
    "            # Secondly, we need to get the logits from each target token in the input sequence\n",
    "            target_logits = logits[input_ids == self.target_token_id].reshape(-1)\n",
    "\n",
    "            # Sometimes we will have less target_logits than targets due to trunction of the input\n",
    "            # In this case, we just consider as many targets as we have logits\n",
    "            if target_logits.size(0) < targets.size(0):\n",
    "                targets = targets[: target_logits.size(0)]\n",
    "\n",
    "            # Finally we compute the loss for the current instance and add it to the batch loss\n",
    "            batch_loss = batch_loss + loss_fn(targets, target_logits)\n",
    "\n",
    "        # The final loss is obtained by averaging over the number of instances per batch\n",
    "        loss = batch_loss / batch_logits.size(0)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e4e24",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Unlike the Huggingface `Trainer`, which expects all external metrics a single function to compute them all at once, in Poutyne, the `Model` or `Experiment` classes are equipped with multiple single functions for each metric.\n",
    "Like the loss, functions that compute other performance metrics receive the model's output alongside the targets (extracted by collation function).\n",
    "Because `transformer` models return not only the logits or predictions of a model but also other things, it is not possible to use Poutynes built-in metrics out of the box.\n",
    "They expect the output to be a single tensor containing the logits of the model, so we create a wrapper for metric functions that extracts them from the output and passes them to the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72969b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "class MetricWrapper:\n",
    "    def __init__(self, metric: Callable, pred_key: str = \"logits\", y_key: str = None):\n",
    "        self.metric = metric\n",
    "        self.pred_key = pred_key\n",
    "        self.y_key = y_key\n",
    "        self._set_metric_name(metric)\n",
    "\n",
    "    def _set_metric_name(self, metric):\n",
    "        self.__name__ = metric.__name__\n",
    "\n",
    "    def __call__(self, outputs: Dict[str, Any], y_true: Any):\n",
    "        y_pred = outputs[self.pred_key]\n",
    "        if self.y_key is not None:\n",
    "            y_true = outputs[self.y_key]\n",
    "        return self.metric(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cea636",
   "metadata": {},
   "source": [
    "Since the logging components of Poutyne infer the name of the metric by assessing the class name of their functions, we need to set the `__name__`-attribute of our wrapper instance with the name of the contained metric.\n",
    "\n",
    "To implement our sentence ordering metrics, we adopt our existing code to return a function for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec09e7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'kendalls_tau', 'mean_logits', 'std_logits']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def make_compute_metrics_functions(target_token_id) -> Callable:\n",
    "    def compute_ranking_func(\n",
    "        outputs: Dict, targets: Any, metric_key: str\n",
    "    ) -> Dict[str, float]:\n",
    "        batch_sent_idx = targets[\"labels\"].detach().cpu().numpy()\n",
    "        batch_input_ids = targets[\"input_ids\"].detach().cpu().numpy()\n",
    "        batch_logits = outputs.detach().cpu().numpy()\n",
    "\n",
    "        metrics = defaultdict(list)\n",
    "        for sent_idx, input_ids, logits in zip(\n",
    "            batch_sent_idx, batch_input_ids, batch_logits\n",
    "        ):\n",
    "            sent_idx = sent_idx.reshape(-1)\n",
    "            input_ids = input_ids.reshape(-1)\n",
    "            logits = logits.reshape(-1)\n",
    "\n",
    "            sent_idx = sent_idx[sent_idx != 100]\n",
    "            target_logits = logits[input_ids == target_token_id]\n",
    "            if sent_idx.shape[0] > target_logits.shape[0]:\n",
    "                sent_idx = sent_idx[: target_logits.shape[0]]\n",
    "            # Calling argsort twice on the logits gives us their ranking in ascending order\n",
    "            predicted_idx = np.argsort(np.argsort(target_logits))\n",
    "            tau, pvalue = kendalltau(sent_idx, predicted_idx)\n",
    "            acc = accuracy_score(sent_idx, predicted_idx)\n",
    "            metrics[\"kendalls_tau\"].append(tau)\n",
    "            metrics[\"acc\"].append(acc)\n",
    "            metrics[\"mean_logits\"].append(logits.mean())\n",
    "            metrics[\"std_logits\"].append(logits.std())\n",
    "        metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
    "        return metrics[metric_key]\n",
    "\n",
    "    metrics = []\n",
    "    for metric in (\"acc\", \"kendalls_tau\", \"mean_logits\", \"std_logits\"):\n",
    "        metric_func = partial(compute_ranking_func, metric_key=metric)\n",
    "        metric_func.__name__ = metric\n",
    "        metrics.append(metric_func)\n",
    "    return metrics\n",
    "\n",
    "metrics = [\n",
    "        MetricWrapper(func)\n",
    "        for func in make_compute_metrics_functions(0)\n",
    "    ]\n",
    "print([metric.__name__ for metric in metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae4566",
   "metadata": {},
   "source": [
    "Additionally, we add two functions to track the mean and standard deviation of the logits to monitor whether the regression can fit the desired indices or only learns their average, which lies around `2.5`.\n",
    "### Complete code\n",
    "\n",
    "Once again, we factor out our adaptions into an external module and implement the rest of the experiment.\n",
    "Due to Poutyne's lack of tooling for creating a command-line interface, this experiment is only configurable via hard-coding the parameters into the source.\n",
    "The rest of the code is mainly similar to the other two frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5390629b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from poutyne.framework import experiment\n",
    "from torch.optim import AdamW\n",
    "from poutyne import (\n",
    "    set_seeds,\n",
    "    TensorBoardLogger,\n",
    "    TensorBoardGradientTracker,\n",
    "    Experiment,\n",
    ")\n",
    "from poutyne_transformers import ModelWrapper, MetricWrapper, TransformerCollator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import load_from_disk\n",
    "from poutyne_modules import (\n",
    "    make_tokenization_func,\n",
    "    PoutyneSequenceOrderingLoss,\n",
    "    make_compute_metrics_functions,\n",
    "    so_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seeds(42)\n",
    "\n",
    "    MODEL_NAME_OR_PATH = \"bert-base-cased\"\n",
    "    LEARNING_RATE = 3e-5\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VAL_BATCH_SIZE = 16\n",
    "    DEVICE = 0\n",
    "    N_EPOCHS = 3\n",
    "    SAVE_DIR = \"experiments/rocstories/bert\"\n",
    "\n",
    "    print(\"Loading model & tokenizer.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "    transformer = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME_OR_PATH, return_dict=True, num_labels=1\n",
    "    )\n",
    "\n",
    "    print(\"Loading & preparing data.\")\n",
    "    dataset = load_from_disk(\"../data/rocstories/\")\n",
    "\n",
    "    if tokenizer.cls_token != \"[CLS]\":\n",
    "        print(\n",
    "            f\"Model does not a have a [CLS] token. Updating the data with token {tokenizer.cls_token} ...\"\n",
    "        )\n",
    "\n",
    "        def replace_cls_token(entry):\n",
    "            texts = entry[\"text\"]\n",
    "            replaced_texts = []\n",
    "            for text in texts:\n",
    "                replaced_texts.append(text.replace(\"[CLS]\", tokenizer.cls_token))\n",
    "            entry[\"text\"] = replaced_texts\n",
    "            return entry\n",
    "\n",
    "        dataset = dataset.map(replace_cls_token, batched=True)\n",
    "\n",
    "    tokenization_func = make_tokenization_func(\n",
    "        tokenizer=tokenizer,\n",
    "        text_column=\"text\",\n",
    "        add_special_tokens=False,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    dataset = dataset.map(tokenization_func, batched=True)\n",
    "\n",
    "    dataset = dataset.rename_column(\"so_targets\", \"labels\")\n",
    "\n",
    "    dataset = dataset.remove_columns(\n",
    "        [\"text\", \"storyid\", \"storytitle\"] + [f\"sentence{i}\" for i in range(1, 6)]\n",
    "    )\n",
    "    dataset.set_format(\"torch\")\n",
    "\n",
    "    collate_fn = TransformerCollator(\n",
    "        y_keys=[\"labels\", \"input_ids\"],\n",
    "        custom_collator=so_data_collator,\n",
    "        remove_labels=True,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset[\"train\"], batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        dataset[\"val\"], batch_size=VAL_BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset[\"test\"], batch_size=VAL_BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    print(\"Preparing training.\")\n",
    "    wrapped_transformer = ModelWrapper(transformer)\n",
    "    optimizer = AdamW(wrapped_transformer.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = PoutyneSequenceOrderingLoss(target_token_id=tokenizer.cls_token_id)\n",
    "\n",
    "    metrics = [\n",
    "        MetricWrapper(func)\n",
    "        for func in make_compute_metrics_functions(tokenizer.cls_token_id)\n",
    "    ]\n",
    "\n",
    "    writer = SummaryWriter(\"runs/roberta/1\")\n",
    "    tensorboard_logger = TensorBoardLogger(writer)\n",
    "    gradient_logger = TensorBoardGradientTracker(writer)\n",
    "\n",
    "    experiment = Experiment(\n",
    "        directory=SAVE_DIR,\n",
    "        network=wrapped_transformer,\n",
    "        device=DEVICE,\n",
    "        logging=True,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_fn,\n",
    "        batch_metrics=metrics,\n",
    "    )\n",
    "\n",
    "    experiment.train(\n",
    "        train_generator=train_dataloader,\n",
    "        valid_generator=val_dataloader,\n",
    "        epochs=N_EPOCHS,\n",
    "        save_every_epoch=True,\n",
    "    )\n",
    "\n",
    "    test_results = experiment.test(test_generator=test_dataloader)\n",
    "    with open(f\"test_results_{MODEL_NAME_OR_PATH}.json\", \"w\") as f:\n",
    "        json.dump(test_results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b37181",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Poutyne provides a well thought and, most of all easy to understand framework to train neural networks.\n",
    "Like its conceptual role model Keras, this simplicity is achieved by strict design decisions, like the `X, y` format for data.\n",
    "While this strictness is helpful for beginners because they only have to learn one way of doing things, it comes at the cost of being hard to adapt to other frameworks or unintended tasks.\n",
    "Luckily, the necessary steps to adapt it to `transformers` and our task are simple and can be reused for most other cases.\n",
    "Since Poutynes mimics the Keras-API, its additional features are much more limited than the other frameworks. Even basic techniques like gradient accumulation are not supported.\n",
    "Depending on the use case, this limited scope might be a deal-breaker for experienced users or complex tasks, but on the other hand, it makes getting started with the framework much more manageable.\n",
    "This accessibility is underlined by the documentation's quality, which covers all aspects of the framework in concise and easily understandable manners without losing itself in the depths of technical details.\n",
    "Yet, there is also potential for further improvements.\n",
    "The lack of support for creating-command line interfaces could force users to migrate to another framework as soon as they need to retrain a model regularly.\n",
    "Currently, the scope of the framework is heavily skewed towards sequence classification tasks. For example, all built-in metrics measure the quality of a classification model.\n",
    "Widening the range of tasks that could be implemented without further extensions would help beginners get into deep learning.\n",
    "A possible improvement that falls more into the category of wishful thinking would be that Poutyne would mimic not only the training parts of the Keras API.\n",
    "If Poutyne would also introduce the ease of building neural networks without manually adjusting each layer's dimensionality, it would significantly contribute to the community."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "source_map": [
   14,
   23,
   33,
   62,
   126,
   162,
   168,
   176,
   184,
   204,
   213,
   251,
   260,
   278,
   284,
   334,
   343,
   468
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}