{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576bb8ac",
   "metadata": {},
   "source": [
    "# Huggingface Trainer\n",
    "\n",
    "Since Huggingface proclaimed goal is to provide an environment to develop and train all sorts of language models, they also ship a solution for training models.\n",
    "It is called the `Trainer`, and is integrated into the `transformers` library itself.\n",
    "Of course, it is profoundly integrated into the Huggingface-ecosystem and can train most `transformers` models out of the box.\n",
    "\n",
    "## Classes\n",
    "\n",
    "### Trainer\n",
    "\n",
    "Design-wise, the `Trainer` is one single class that handles the training end-to-end.\n",
    "Its configuration is outsourced to a `TrainingArguments` class that stores all relevant parameters for training.\n",
    "These arguments are passed to the `Trainer`  alongside a model and a dataset during initialization.\n",
    "Since Huggingface models compute the loss internally, the `Trainer` passes the input data to the model, extracts the loss from the output, and does the backward step.\n",
    "It also handles additional steps to monitor the training process, like saving checkpoints of the model or logging the loss and other validation metrics.\n",
    "A significant advantage of using the `Trainer` is its ability to do multi-device training without requiring the user to care about dispatching the models and data to multiple accelerators.\n",
    "Also, it comes with an extension that allows more sophisticated tweaks, like training with 16bit-precision.\n",
    "\n",
    "#### Extending the `Trainer`\n",
    "\n",
    "There are two different options to customize certain aspects of the behavior of the `Trainer`.\n",
    "Additional read-only operations can be implemented with the callback API.\n",
    "Callbacks are executed at specific events during the training (e.g., at the end of an epoch).\n",
    "They have access to many different things like the model or the current state of the `Trainer`.\n",
    "However, since they can not manipulate their environment, their scope is limited to logging, saving certain parts of a model, or stopping the training if a specific condition is met.\n",
    "\n",
    "If further changes to the `Trainer` are required, the recommended way is to subclass it and create a custom via inheritance.\n",
    "Internally, the `Trainer` structures the training into different sub-steps and exposes them via a method for each of them.\n",
    "By overwriting these methods, it is possible to change certain parts of the logic without rewriting the rest of the code that would not be changed anyway.\n",
    "The most important methods to modify the `train-test-val`-loop itself are the `<train/test/val>-step` methods and the `compute_loss` method.\n",
    "These methods implement the essential individual training steps and are called within methods that implement higher-order operations like the `.train`-method, which handles the complete training loop.\n",
    "\n",
    "\n",
    "#### Logging\n",
    "\n",
    "If a `logdir`-argument is specified in the `TrainingArguments`-object, logging is enabled automatically.\n",
    "By default, the `Trainer` outputs the logs in two formats: Stdout and disk, using a Tensorboard-compliant format.\n",
    "Additional logging can be implemented by either overwriting the `.log`-method of Trainer or by using callbacks.\n",
    "There are already some pre-built callbacks available. For example, to log the progress to Weights and Biases or a CSV table.\n",
    "\n",
    "#### Custom metrics\n",
    "\n",
    "Since the `Trainer` is agnostic towards the task it is used with; it only logs the loss by default.\n",
    "Additionally, metrics can be added by equipping the `Trainer` with a function that computes them during initialization.\n",
    "This function receives an `EvalPrediction` object.\n",
    "This object holds all predictions of the model and the valid labels.\n",
    "The output of the custom metric function ought to be a dictionary containing the name of the metric as key and the score as value.\n",
    "\n",
    "### Training Arguments\n",
    "\n",
    "As stated above, a `TrainingArguments` object stores all hyperparameters of the training.\n",
    "Storing all parameters in a single object is helpful to ensuring reproducibility since this object can easily be serialized and saved to disk as JSON using its `.to_json_string`-method.\n",
    "Also, the `TrainingArguments` class works seamlessly with the built-in CLI-parser class of `transformers`, which helps make the configuration of an experiment available through a command-line interface.\n",
    "\n",
    "### HfArgumentParser\n",
    "\n",
    "Most experiments are repeated several times with different parameters. These parameters have to be changed directly in the source code by default, which is not ideal for several reasons.\n",
    "Most importantly, it can harm reproducibility since tracking changes in the source code requires either version control and a strict commit regime or keeping several versions of the same file with different parameters. Also, it can be tedious the search for the location of all parameters across the code manually.\n",
    "Making the hyperparameters adjustable via a command-line interface decouples their configuration from the rest of the code, alleviating this issue.\n",
    "While there are arguably a lot of different solutions to this problem with many strategies that are more sophisticated than a command-line interface, it is an excellent first step.\n",
    "Moreover, it has the advantage of being platform-independent without depending on additional dependencies.\n",
    "\n",
    "Huggingface provides a built-in solution for building these interfaces called `HfArgumentParser`.\n",
    "It is an extended version of Pythons `argsparse` parser and creates command-line interfaces by parsing the fields of `dataclasses` and exposing them as command-line arguments.\n",
    "Since most configuration classes of the `transformers` library are `dataclasses,` the `HfArgumentParser` can flexibly control nearly every aspect of the training.\n",
    "Further extending the arguments can be easily done by creating custom `dataclasses` that hold additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "483614a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch_size BATCH_SIZE]\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Number of batched for training. (default: 8)\n",
      "(TrainArgs(batch_size=4),)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "@dataclass\n",
    "class TrainArgs:\n",
    "    batch_size: int = field(\n",
    "        default = 8,\n",
    "        metadata = {\"help\": \"Number of batched for training.\"}\n",
    "    )\n",
    "\n",
    "parser = HfArgumentParser(TrainArgs)\n",
    "parser.print_help()\n",
    "train_args = parser.parse_args_into_dataclasses([\"--batch_size\", \"4\"])\n",
    "print(train_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbf48b",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "### Loss function\n",
    "\n",
    "For the sentence ordering task, we employ a language model with a standard token-classification head.\n",
    "However, since the task requires a custom loss function, we have to discard the loss of the model and use our custom loss function.\n",
    "To do so, we follow the guidelines and create our custom version of the `Trainer` with a custom `.compute_loss` function.\n",
    "The implementation <!--of the `.compute_loss` method --> is straightforward.\n",
    "The `.compute_loss` method receives a reference to the model and the input data as inputs, which is especially helpful in cases like ours where we need to check the `input_ids` to compute the loss.\n",
    "In addition, to our custom loss function, we also add another attribute to the `Trainer`, which holds the id of the target sentence token in order to find the correct tokens in the input sequence.\n",
    "We leave the rest of the `Trainer` untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b6efd",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class SentenceOrderingTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.target_token_id = kwargs.pop(\"target_token_id\")\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "\n",
    "        # Get sentence indices\n",
    "        batch_labels = inputs.pop(\"labels\")\n",
    "\n",
    "        # Get logits from model\n",
    "        outputs = model(**inputs)\n",
    "        batch_logits = outputs[\"logits\"]\n",
    "\n",
    "        # Get logits for all cls tokens\n",
    "        batch_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "        batch_loss = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n",
    "        \n",
    "        for labels, logits, input_ids in zip(\n",
    "            batch_labels, batch_logits, batch_input_ids\n",
    "        ):\n",
    "\n",
    "            true_labels = labels[labels != -100].reshape(-1)\n",
    "            targets = true_labels.float()\n",
    "\n",
    "            target_logits = logits[input_ids == self.target_token_id].reshape(-1)\n",
    "\n",
    "            if target_logits.size(0) < targets.size(0):\n",
    "                targets = targets[: target_logits.size(0)]\n",
    "\n",
    "            batch_loss = batch_loss + loss_fn(targets, target_logits)\n",
    "\n",
    "        loss = batch_loss / batch_logits.size(0)\n",
    "\n",
    "        outputs[\"loss\"] = loss\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18564505",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "To compute custom metrics during validation, we need to create a function.\n",
    "The function computes all metrics at once.\n",
    "In contrast to the `.compute_loss`-method, which receives the input and the model, it receives an `EvalPrediction` object as input.\n",
    "An `EvalPrediction` contains the model's outputs and the labels from the dataset.\n",
    "However, similar to the loss function, computing the metrics requires access to the input data to retrieve the indices of the target tokens.\n",
    "To control the content of an `EvalPrediction` object, we can use the `label_names` parameter of the `TrainingArguments`.\n",
    "With this argument, we can specify additional fields that are copied from the input batches to the `EvalPrediction` objects.\n",
    "This way, we can incorporate the labels and the `input_ids` of tokens in the `EvalPrediction` object.\n",
    "\n",
    "A minor but valuable trait of the `EvalPrediction` objects is that their content gets converted from `torch.tensors` to `np.arrays`.\n",
    "Because most validation metrics from other libraries use NumPy, we do not need to convert the data manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0ffb07",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    ...,\n",
    "    label_names=[\"labels\", \"input_ids\"],\n",
    "    ...,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f061b82",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "def make_compute_metrics_func(target_token_id) -> Callable:\n",
    "    def compute_ranking_func(eval_prediction: EvalPrediction) -> Dict[str, float]:\n",
    "        batch_sent_idx, batch_input_ids = eval_prediction.label_ids\n",
    "        batch_logits = eval_prediction.predictions.squeeze(2)\n",
    "\n",
    "        metrics = defaultdict(list)\n",
    "        for sent_idx, input_ids, logits in zip(\n",
    "            batch_sent_idx, batch_input_ids, batch_logits\n",
    "        ):\n",
    "            sent_idx = sent_idx.reshape(-1)\n",
    "            input_ids = input_ids.reshape(-1)\n",
    "            logits = logits.reshape(-1)\n",
    "\n",
    "            sent_idx = sent_idx[sent_idx != 100]\n",
    "            target_logits = logits[input_ids == target_token_id]\n",
    "            if sent_idx.shape[0] > target_logits.shape[0]:\n",
    "                sent_idx = sent_idx[: target_logits.shape[0]]\n",
    "            predicted_idx = np.argsort(np.argsort(target_logits))\n",
    "            tau, pvalue = kendalltau(sent_idx, predicted_idx)\n",
    "            metrics[\"kendalls_tau\"].append(tau)\n",
    "            metrics[\"acc\"].append(accuracy_score(sent_idx, predicted_idx))\n",
    "            metrics[\"mean_logits\"].append(logits.mean())\n",
    "            metrics[\"std_logits\"].append(logits.std())\n",
    "        metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
    "        return metrics\n",
    "\n",
    "    return compute_ranking_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207cf7a1",
   "metadata": {},
   "source": [
    "### Custom CLI arguments\n",
    "\n",
    "We use the `HfArgumentParser` to make the parameters of our experiment adjustable via the command line.\n",
    "In addition to the `TrainingsArguments`, we also want to control the type of the model.\n",
    "Custom parameters can easily be added by creating a custom `dataclass`. \n",
    "We create a `ModelArgs` class that has two fields. One to specify the name or path to the model and a second parameter to specify the path where the final model is saved after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40cbacf",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"bert-base-cased\",\n",
    "        metadata={\n",
    "            \"help\": \"Path to pretrained model or model or its name to load it from Huggingface Hub.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    final_checkpoint_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to save the final model.\"}\n",
    "    )\n",
    "\n",
    "...\n",
    "\n",
    "args_parser = HfArgumentParser((ModelArgs, TrainingArguments))\n",
    "model_args, training_args = args_parser.parse_args_into_dataclasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a081981",
   "metadata": {},
   "source": [
    "## Complete code\n",
    "\n",
    "After moving our custom code for the `Trainer` and the metric function to an external module, the rest of the code to implement the experiment looks like Listing (TODO).\n",
    "There are only two steps left to complete the script.\n",
    "Firstly, we must ensure that our data always contains the correct special tokens for ordering the sentences.\n",
    "Since we prepared the data beforehand by adding BERTs special `[SEP]`-token as a prefix to each sentence, we have to ensure that these tokens are replaced if necessary using the  `replace_cls_token` function.\n",
    "\n",
    "Lastly, we want to control the randomness in our experiment to make it consistently reproducible.\n",
    "The `transformers` library comes with a helpful function called `set_seed`, which controls the state of all random number generators of Python itself, NumPy, and PyTorch at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31c399a",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer\n",
    "from transformers import set_seed\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from model import (\n",
    "    SentenceOrderingTrainer,\n",
    "    so_data_collator,\n",
    "    make_compute_metrics_func,\n",
    "    ModelArgs,\n",
    "    make_tokenization_func,\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args_parser = HfArgumentParser((ModelArgs, TrainingArguments))\n",
    "    model_args, training_args = args_parser.parse_args_into_dataclasses()\n",
    "\n",
    "    # Add fixed args\n",
    "    training_args.label_names = [\"labels\", \"input_ids\"]\n",
    "\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    dataset = load_from_disk(\n",
    "        \"/home/keller/Uni/trf_training_tut/scripts/data/rocstories\"\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path)\n",
    "\n",
    "    if tokenizer.cls_token != \"[CLS]\":\n",
    "        print(\n",
    "            f\"Model does not a have a [CLS] token. Updating the data with token {tokenizer.cls_token} ...\"\n",
    "        )\n",
    "\n",
    "        def replace_cls_token(entry):\n",
    "            texts = entry[\"text\"]\n",
    "            replaced_texts = []\n",
    "            for text in texts:\n",
    "                replaced_texts.append(text.replace(\"[CLS]\", tokenizer.cls_token))\n",
    "            entry[\"text\"] = replaced_texts\n",
    "            return entry\n",
    "\n",
    "        dataset = dataset.map(replace_cls_token, batched=True)\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(\n",
    "        model_args.model_name_or_path, num_labels=1\n",
    "    )\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_args.model_name_or_path, config=model_config\n",
    "    )\n",
    "\n",
    "    tokenization = make_tokenization_func(\n",
    "        tokenizer=tokenizer,\n",
    "        text_column=\"text\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    dataset = dataset.map(tokenization, batched=True)\n",
    "\n",
    "    dataset = dataset.rename_column(\"so_targets\", \"labels\")\n",
    "\n",
    "    dataset.set_format(\"torch\")\n",
    "\n",
    "    metrics_func = make_compute_metrics_func(tokenizer.cls_token_id)\n",
    "\n",
    "    trainer = SentenceOrderingTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"val\"],\n",
    "        target_token_id=tokenizer.cls_token_id,\n",
    "        data_collator=so_data_collator,\n",
    "        compute_metrics=metrics_func,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(model_args.final_checkpoint_path)\n",
    "\n",
    "    test_results = trainer.evaluate(eval_dataset=dataset[\"test\"])\n",
    "    with open(f\"test_results_{model_args.model_name_or_path}.json\", \"w\") as f:\n",
    "        json.dump(test_results, f)\n",
    "\n",
    "    print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217783c6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Huggingface `Trainer` is a perfect choice when training models on standard tasks that are well supported.\n",
    "In these cases, it enables to train models effortlessly without requiring to write much code.\n",
    "In the best case, when the dataset is already available as Huggingface `Dataset`, it comes down to a few lines of code to train the model without having to dive deep into any internals along the way.\n",
    "\n",
    "Also, it has many useful out-of-the-box features, like gradient clipping, half-precision training, support of distributed training, or logging to Tensorboard, which make it feasible for training large models on large datasets.\n",
    "\n",
    "Nonetheless, there are a few issues if one wants to leave the carved-out paths.\n",
    "Like the rest of Huggingface's software, the `transformers` library is relatively new and evolves at great speed.\n",
    "Huggingface's self-proclaimed goal is to provide an easy-to-use all-in-one infrastructure for NLP with language models and incorporate new models, architectures, and developments as quickly as possible.\n",
    "On this path, sacrifices have to be made.\n",
    "\n",
    "One area that seems to suffer from the speedy development is documentation.\n",
    "It is sufficient and provides all essential information, but it can sometimes be very sparse in detail.\n",
    "Often, there are multiple options to choose from when customizing something.\n",
    "For example, the default optimizer can be exchanged during the initialization of the `Trainer`, by simply passing another one to it. Or by overwriting the `.create_optimizer`-method.\n",
    "In cases like this one, the documentation lacks hints to decide which way to go.\n",
    "\n",
    "Other times the documentation does not paint the whole picture of the behavior of the described object.\n",
    "In these cases, it might become necessary to take a look into the source code itself.\n",
    "\n",
    "By looking into the source code of `Trainer`, it becomes evident that it could use some refactoring.\n",
    "Especially, its high-level methods, like the `.train`-method, are very complex since they do much heavy lifting, for example, dispatching the training to multiple devices.\n",
    "While the preferred way to customize the training is to subclass the `Trainer` and overwrite methods, this is only feasible for the low-level methods that define single steps. Even tiny adjustments to the high-level methods can require copying code or rewriting certain parts."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "source_map": [
   12,
   81,
   96,
   111,
   151,
   169,
   178,
   207,
   216,
   238,
   250,
   339
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}