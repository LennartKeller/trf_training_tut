
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>PyTorch Lightning &#8212; A comparing guide to train language models with Python.</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Poutyne" href="Poutyne.html" />
    <link rel="prev" title="Huggingface Trainer" href="HuggingFaceTrainer.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">A comparing guide to train language models with Python.</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Home.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="HuggingfaceEcosystem.html">
   The Huggingface ecosystem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Experiment.html">
   Experimental Design
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Prerequisites.html">
   Prerequisites
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="HuggingFaceTrainer.html">
   Huggingface Trainer
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   PyTorch Lightning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Poutyne.html">
   Poutyne
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ExperimentalResults.html">
   Experimental Results
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Conclusion.html">
   Conclusion
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/PyTorchLightning.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/PyTorchLightning.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/LennartKeller/trf_training_tut"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/LennartKeller/trf_training_tut/issues/new?title=Issue%20on%20page%20%2FPyTorchLightning.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/LennartKeller/trf_training_tut/master?urlpath=tree/PyTorchLightning.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#classes">
   Classes
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lightningmodule">
     <code class="docutils literal notranslate">
      <span class="pre">
       LightningModule
      </span>
     </code>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-testing-validation">
       Training, Testing, Validation
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#model-hyperparameters-and-checkpoints">
       Model Hyperparameters and checkpoints
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#logging">
       Logging
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lighningdatamodule">
     <code class="docutils literal notranslate">
      <span class="pre">
       LighningDataModule
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#trainer">
     <code class="docutils literal notranslate">
      <span class="pre">
       Trainer
      </span>
     </code>
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#extending-the-trainer">
       Extending the
       <code class="docutils literal notranslate">
        <span class="pre">
         Trainer
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Logging
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cli-interface">
     CLI Interface
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#implementation">
   Implementation
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model">
     Model
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data">
     Data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#complete-code">
     Complete code
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="pytorch-lightning">
<h1>PyTorch Lightning<a class="headerlink" href="#pytorch-lightning" title="Permalink to this headline">¶</a></h1>
<p>In contrast to the Huggingface <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>, which handles the complete training itself, PyTorch Lightning (<span id="id1">Falcon [<a class="reference internal" href="Bibliography.html#id3">2019</a>]</span>) takes a different approach.
It not only aims at handling the training but also at structuring the creation of a model too.
Its main goal is not to hide complexity from the user but to provide a well-structured API for building neural networks of all kinds.
The most striking aspect of this is that in PyTorch Lightning’s philosophy, a model and its inference-, training and prediction logic are not separate things that can be exchanged independently.
Instead, it binds all these parts directly to the model itself.
In doing so, PyTorch-Lightning does not make any assumptions on the nature of the model or the training itself. Thus it allows covering many tasks and domains with maximum flexibility.</p>
<p>However, this approach comes at the cost that the user again must implement many things manually.
Naturally, this approach is keener to researchers who implement and test custom models, while practitioners who only want to employ pre-built models must deal with some implementational overhead.
PyTorch Lightning’s steep learning curve compounds this issue.
However, there is exhaustive documentation with many tutorials (as texts and videos), best practices, and user guides on building various models across different domains.
Also, if an experiment is implemented in PyTorch Lightning, there are a lot of helpful tweaks and techniques to improve or speed up the training.
So that it can be worthwhile even when using pre-built models.
These facilitation features include tweaks like training with half-precision, automatic tuning of the learning rate, and integrations into hyperparameter tuning frameworks or creating command-line interfaces to control the parameters.
In addition to that, there is support for different computational backends that help to dispatch the training on multiple accelerators like GPUs and TPUs.
If these features are not enough, there is a growing ecosystem of third-party extensions, widening the scope and functionality of the framework.</p>
<div class="section" id="classes">
<h2>Classes<a class="headerlink" href="#classes" title="Permalink to this headline">¶</a></h2>
<p>From a technical point of view, Pytorch Lightning provides an API composed of three different main classes, dividing the training process into a sequence of single atomic steps.
These classes implement the model, the logic for storing and processing the training data, and the training process itself.</p>
<div class="section" id="lightningmodule">
<h3><code class="docutils literal notranslate"><span class="pre">LightningModule</span></code><a class="headerlink" href="#lightningmodule" title="Permalink to this headline">¶</a></h3>
<p>A subclass of a <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> implements the model.
A <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> is an extended version of PyTorch’s <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class.
<code class="docutils literal notranslate"><span class="pre">nn.Modules</span></code> are the basic building blocks of neural networks in PyTorch. In essence, they store a set of parameters, for example, weights of a single layer alongside with <code class="docutils literal notranslate"><span class="pre">.forward</span></code>-method that defines the computational logic when data flows through the module. They are designed to work recursively. One module can be composed of several submodules so that each building block of a neural network, starting from single layers up to a complete network, can be implemented in this one class.
The following listing shows an exemplary implementation of a simple linear layer as <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.
By chaining multiple instances of the dense layer in a <code class="docutils literal notranslate"><span class="pre">nn.</span> <span class="pre">Sequential</span></code> class, it is possible to create a simple feed-forward network.
This network is again a subclass of the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">DenseLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fully connected linear layer.&quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">in_shape</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">DenseLayer</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">DenseLayer</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">DenseLayer</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>  <span class="c1"># Batchsize 8</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">network</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">issubclass</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>torch.Size([8, 2])
True
</pre></div>
</div>
</div>
</div>
<p>A <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> is intended to replace the outmost <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> instance of a model, which holds the complete network.
It extends the <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class with new methods, designed to structure not only the logic of a single forward pass but also other steps like a complete train-, test- or validation-steps.
With this extension, it becomes possible to define a single forward step through the network and how the models should be trained and tested as well.
In essence, it provides a way to incorporate the training loop into the model itself.
This strategy has one massive advantage over the standard PyTorch practice of writing an external function that implements the training loop.
It helps to make the model self-contained, meaning that it holds all necessary logic itself. This property alleviates sharing models since only one class carries all information to train and test the model.</p>
<div class="section" id="training-testing-validation">
<h4>Training, Testing, Validation<a class="headerlink" href="#training-testing-validation" title="Permalink to this headline">¶</a></h4>
<p>The methods for training, testing and prediction are called <code class="docutils literal notranslate"><span class="pre">.train_step</span></code>,  <code class="docutils literal notranslate"><span class="pre">.validation_step</span></code> and <code class="docutils literal notranslate"><span class="pre">.test_step</span></code> respectively.
They all define how a single batch of data should be handled for these steps.
Typically the <code class="docutils literal notranslate"><span class="pre">.train_step</span></code>-method computes the loss score, and the other two methods compute other validation metrics.
Design-wise, only the <code class="docutils literal notranslate"><span class="pre">.train_step</span></code>-method is required to return the loss averaged loss score for the current batch.
The test- and validation methods are not required to return anything. Instead, they can use the built-in logging capabilities of the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>.
Similar to the <code class="docutils literal notranslate"><span class="pre">.train_step</span></code>-method, they should return their scores averaged over the complete batch too.</p>
</div>
<div class="section" id="model-hyperparameters-and-checkpoints">
<h4>Model Hyperparameters and checkpoints<a class="headerlink" href="#model-hyperparameters-and-checkpoints" title="Permalink to this headline">¶</a></h4>
<p>Much effort has been put into organizing the hyperparameters of an experiment.
Like the train and test routines, PyTorch Lightning binds all hyperparameters that control the model directly to the object.
This strategy ensures that saved models also contain the combination of parameters used for training.
There are two ways to define the hyperparameters of a model.
By default, each argument in the signature of the model’s constructor is regarded as a hyperparameter.
By calling a <code class="docutils literal notranslate"><span class="pre">.save_hyperparameters</span></code>-method in the constructor, these arguments get serialized into a <code class="docutils literal notranslate"><span class="pre">.hparams</span></code>-attribute.
The <code class="docutils literal notranslate"><span class="pre">.hparams</span></code>-attribute is saved as a <code class="docutils literal notranslate"><span class="pre">YAML</span></code>-file for each checkpoint of the model, making it easy to see which parameters were used without loading the whole model.
If the constructor contains non-hyperparameters arguments, these can be excluded from serialization using the saving method’s <code class="docutils literal notranslate"><span class="pre">ignore</span></code> flag.</p>
<p>Another more explicit way of defining the parameters of a model is to store them all in a dictionary into the constructor and to pass this dictionary to the <code class="docutils literal notranslate"><span class="pre">-save_hyperparameters</span></code>-method.
This strategy is suitable in cases where many arguments of the constructor are non-hyperparameters.</p>
</div>
<div class="section" id="logging">
<h4>Logging<a class="headerlink" href="#logging" title="Permalink to this headline">¶</a></h4>
<p>Logging in PyTorch-Lightning is a two-stage procedure.
Inside the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>, various metrics can be logged at different steps while training using the <code class="docutils literal notranslate"><span class="pre">.log</span></code>- or <code class="docutils literal notranslate"><span class="pre">log_dict</span></code>-methods.
The <code class="docutils literal notranslate"><span class="pre">.log</span></code>-methods can log a single score, while the <code class="docutils literal notranslate"><span class="pre">log-dict</span></code>-method can log multiple scores stored in a dictionary (with names as keys and the score as values).
These logs are extracted by <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> and written out in various formats (see Trainer section for further details.)
One benefit of using an autonomous logging function is that it gives flexibility to the user in deciding when to log which metrics, for example, making it possible to log something only when a condition applies.</p>
</div>
</div>
<div class="section" id="lighningdatamodule">
<h3><code class="docutils literal notranslate"><span class="pre">LighningDataModule</span></code><a class="headerlink" href="#lighningdatamodule" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">PyTorchLightning</span></code> also comes with a custom solution to bundle data-related operations into a single object.
It is called <code class="docutils literal notranslate"><span class="pre">LighningDataModule</span></code> and should contain the code to load and prepare the data for training and testing.
A class derived from <code class="docutils literal notranslate"><span class="pre">LighntningDataModule</span></code> must implement four required methods.
The <code class="docutils literal notranslate"><span class="pre">.prepare_data</span></code>-method should implement all steps required to load the data and convert it into a correct representation for the model.
To return the splits for training, testing and evaluation, there are <code class="docutils literal notranslate"><span class="pre">.train|.test|.val_loader</span></code>-methods. Each of them has to return a <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> object.
Like the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>, its data counterpart has the advantage of holding all code to load and prepare the data, which alleviates distribution and publication.
Another key feature of the <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> is its ability to adapt to distributed environments.
While the <code class="docutils literal notranslate"><span class="pre">.prepare_data</span></code>-method is called once at the beginning of the training, there are also additional <code class="docutils literal notranslate"><span class="pre">.setup</span></code>- and <code class="docutils literal notranslate"><span class="pre">teardown</span></code>-methods.
These methods can define operations pre- or post-training data-preparation steps that must be performed independently on each accelerator.</p>
</div>
<div class="section" id="trainer">
<h3><code class="docutils literal notranslate"><span class="pre">Trainer</span></code><a class="headerlink" href="#trainer" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> object handles the actual training.
It receives the model and data (wrapped in Lightning modules) alongside all training-specific hyperparameters, like the number of epochs, the devices to train on, or a list of loggers to log the progress.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> exposes four high-level methods to the user. Each of them triggers either the training, the validation, the prediction of unseen instances, and the hyperparameter-tuning.
Like the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code>, an instance of the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> is initialized with all hyperparameters relevant to the training, like the batch size or a number of epochs.</p>
<p>The different stages of the training (training and testing)</p>
<div class="section" id="extending-the-trainer">
<h4>Extending the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code><a class="headerlink" href="#extending-the-trainer" title="Permalink to this headline">¶</a></h4>
<p>In contrast to the <code class="docutils literal notranslate"><span class="pre">LightningModule</span></code> and <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> itself is not intended to be customized in any way. Since their respective objects contain all model or data-related code, the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> is better kept untouched.
Instead, if necessary, the functions of the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> can be extended with callbacks and plugins. Both of them can add custom operations to different stages of the training.
Callbacks implement steps that are not strictly necessary for training. Instead, they can be used to define things like logging or applying non-essential operations to the model (i.e., weigh pruning after each epoch) that add new functions but are not required to perform training.
On the other hand, plugins are meant to extend the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> with new functionalities like adding support for new accelerators or computational backends. So by their scope, they are meant to be used by experienced users who need to extend the Trainer.
However, since their API is still in beta and subject to changes in the future, it should be used with caution.</p>
<p>Also, it contains a handful of tweaks to improve the results, like gradient accumulation or gradient .
In addition to that, the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> also supports tuning the learning rate and batch size out of the box. Both tuning features must be enabled while initializing the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> and can be invoked by calling the <code class="docutils literal notranslate"><span class="pre">.tune</span></code>-method.</p>
</div>
<div class="section" id="id2">
<h4>Logging<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>While the model defines what measures are logged, the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> is responsible for writing out these logs.
By default, it logs the standard output.
In addition to that, it can be extended with additional loggers.
PyTorch Lightning provides built-in loggers that log the progress to Tensorboard or other services like Weights and Biases.
Further loggers, can be implemented using the Logger base-class.
Multiple loggers are passed to the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> during initialization as a list.</p>
</div>
</div>
<div class="section" id="cli-interface">
<h3>CLI Interface<a class="headerlink" href="#cli-interface" title="Permalink to this headline">¶</a></h3>
<p>PyTorch Lightning supports the creation of command-line interfaces through the <code class="docutils literal notranslate"><span class="pre">LightninArgumentParser</span></code> class.
This class is an extended version of the parser from the <code class="docutils literal notranslate"><span class="pre">jsonargparse</span></code> module, and it can parse the arguments of Lightning classes and other classes out-of-the-box.
This feature enables adding parameters of different modules to the parser effortlessly.</p>
<p>If more flexibility is needed, for example, when only some parameters of an object should be added to the parser, the best practice is to add a method to the object, which adds these arguments to the parser.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parser</span><span class="o">.</span><span class="n">add_lightning_class_args</span><span class="p">(</span><span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_class_arguments</span><span class="p">(</span><span class="n">TensorBoardLogger</span><span class="p">,</span> <span class="n">nested_key</span><span class="o">=</span><span class="s2">&quot;tensorboard&quot;</span><span class="p">)</span>
<span class="n">parser</span><span class="o">.</span><span class="n">add_lightning_class_args</span><span class="p">(</span><span class="n">Trainer</span><span class="p">,</span> <span class="s2">&quot;trainer&quot;</span><span class="p">)</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">PlLanguageModelForSequenceOrdering</span><span class="o">.</span><span class="n">add_model_specific_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="implementation">
<h2>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="model">
<h3>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h3>
<p>Since we do not build our model from scratch, we need to load the pretrained transformer in the LightningModules’s constructor.
To be able to load different models, we introduce the name of the model as hyperparameters.
Since the model is pretrained, we only have to specify two other hyperparameters, namely the learning rate and the id of the target token.
Because Huggingface models are also subclasses of the  <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> class, loading the transformer model works flawlessly, and the language model is recognized as a submodule of the <code class="docutils literal notranslate"><span class="pre">PlLanguageModelForSequenceOrdering</span></code> class.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PlLanguageModelForSequenceOrdering</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hparams</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;model_name_or_path&quot;</span><span class="p">],</span>
            <span class="n">return_dict</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">output_hidden_states</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">num_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we define a single forward step. Again, the logic is pretty simple since we only need to exclude the labels from the inputs for the language model and pass the rest of the input data to the language model to obtain the outputs.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
        <span class="c1"># We do not want to compute token classification loss, so we remove the labels temporarily</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;labels&quot;</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># And reattach them later on ...</span>
        <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>
</div>
</div>
</div>
<p>Because we want to compute the loss while training and validating the model, we factor out the loss function into a separate method.
Implementation-wise, the loss function is only slightly variated from the original implementation. The only changes are that we retrieve the target token id from the hyperparameters of the model.
Also, we draw inspiration from the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> API and add a custom version of the forward method. This method computes both the forward step and the loss. The loss is then attached to the output of the model.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">_compute_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_labels</span><span class="p">,</span> <span class="n">batch_logits</span><span class="p">,</span> <span class="n">batch_input_ids</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="c1"># Since we have varying number of labels per instance, </span>
        <span class="c1"># we need to compute the loss manually for each one.</span>
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
        <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">labels</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">input_ids</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">batch_labels</span><span class="p">,</span> <span class="n">batch_logits</span><span class="p">,</span> <span class="n">batch_input_ids</span>
        <span class="p">):</span>

            <span class="c1"># Firstly, we need to convert the sentence indices to regression targets.</span>
            <span class="c1"># To avoid exploding gradients, we norm them to be in range 0 &lt;-&gt; 1.</span>
            <span class="c1"># labels = labels / labels.max()</span>
            <span class="c1"># Also we need to remove the padding entries (-100).</span>
            <span class="n">true_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">labels</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">100</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">targets</span> <span class="o">=</span> <span class="n">true_labels</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

            <span class="c1"># Secondly, we need to get the logits </span>
            <span class="c1"># from each target token in the input sequence</span>
            <span class="n">target_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span>
                <span class="n">input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;target_token_id&quot;</span><span class="p">]</span>
            <span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Sometimes we will have less target_logits </span>
            <span class="c1"># than targets due to truncation of the input.</span>
            <span class="c1"># In this case, we just consider as many targets as we have logit.</span>
            <span class="k">if</span> <span class="n">target_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">targets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
                <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="p">[:</span> <span class="n">target_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]</span>

            <span class="c1"># Finally we compute the loss for the current instance </span>
            <span class="c1"># and add it to the batch loss.</span>
            <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">batch_loss</span> <span class="o">+</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">target_logits</span><span class="p">)</span>

        <span class="c1"># The final loss is obtained by averaging </span>
        <span class="c1"># over the number of instances per batch.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">batch_loss</span> <span class="o">/</span> <span class="n">batch_logits</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">_forward_with_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># Get sentence indices</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span>
        <span class="c1"># Get logits from model</span>
        <span class="n">batch_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="c1"># Get logits for all cls tokens</span>
        <span class="n">batch_input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_loss</span><span class="p">(</span>
            <span class="n">batch_labels</span><span class="o">=</span><span class="n">batch_labels</span><span class="p">,</span>
            <span class="n">batch_logits</span><span class="o">=</span><span class="n">batch_logits</span><span class="p">,</span>
            <span class="n">batch_input_ids</span><span class="o">=</span><span class="n">batch_input_ids</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>

        <span class="k">return</span> <span class="n">outputs</span>
</pre></div>
</div>
</div>
</div>
<p>Using the <code class="docutils literal notranslate"><span class="pre">_foward_with_loss</span></code>-method implementing the <code class="docutils literal notranslate"><span class="pre">training_step</span></code>-method becomes relatively simple.
The only thing left to do inside this method is to log the training loss in order to be able to monitor the progress during training.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">batch_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_with_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<p>Like the <code class="docutils literal notranslate"><span class="pre">_compute_loss</span></code>-method, we only need to slightly adapt the validation metrics’ computation to use the model’s hyperparameters.
Since we want to compute the identical scores for testing and validation, we can also use the <code class="docutils literal notranslate"><span class="pre">validation_step</span></code>-method for testing.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forward_with_loss</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

        <span class="c1"># Detach all torch.tensors and convert them to np.arrays.</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">outputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">outputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">inputs</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

        <span class="c1"># Get sentence indices</span>
        <span class="n">batch_labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span>
        <span class="c1"># Get logits from model</span>
        <span class="n">batch_logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;logits&quot;</span><span class="p">]</span>
        <span class="c1"># Get logits for all cls tokens</span>
        <span class="n">batch_input_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>

        <span class="n">metrics</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">sent_idx</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">logits</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span>
            <span class="n">batch_labels</span><span class="p">,</span> <span class="n">batch_input_ids</span><span class="p">,</span> <span class="n">batch_logits</span>
        <span class="p">):</span>
            <span class="n">sent_idx</span> <span class="o">=</span> <span class="n">sent_idx</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">input_ids</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">sent_idx</span> <span class="o">=</span> <span class="n">sent_idx</span><span class="p">[</span><span class="n">sent_idx</span> <span class="o">!=</span> <span class="mi">100</span><span class="p">]</span>
            <span class="n">target_logits</span> <span class="o">=</span> <span class="n">logits</span><span class="p">[</span><span class="n">input_ids</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;target_token_id&quot;</span><span class="p">]]</span>
            <span class="k">if</span> <span class="n">sent_idx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">target_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">sent_idx</span> <span class="o">=</span> <span class="n">sent_idx</span><span class="p">[:</span> <span class="n">target_logits</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>

            <span class="c1"># Calling argsort twice on the logits </span>
            <span class="c1"># gives us their ranking in ascending order.</span>
            <span class="n">predicted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">target_logits</span><span class="p">))</span>
            <span class="n">tau</span><span class="p">,</span> <span class="n">pvalue</span> <span class="o">=</span> <span class="n">kendalltau</span><span class="p">(</span><span class="n">sent_idx</span><span class="p">,</span> <span class="n">predicted_idx</span><span class="p">)</span>
            <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">sent_idx</span><span class="p">,</span> <span class="n">predicted_idx</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;kendalls_tau&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;acc&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;mean_logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;std_logits&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">logits</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Add val prefix to each metric name and compute mean over the batch.</span>
        <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sa">f</span><span class="s2">&quot;val_</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">metric</span><span class="p">,</span> <span class="n">scores</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">metrics</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Lastly, we need to implement the <code class="docutils literal notranslate"><span class="pre">configure_optimizers</span></code>-method and add the model’s hyperparameter to the parser via the <code class="docutils literal notranslate"><span class="pre">add_model_specific_args</span></code>-method.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hparams</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">add_model_specific_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">):</span>
        <span class="n">parser</span> <span class="o">=</span> <span class="n">parent_parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">(</span>
            <span class="s2">&quot;PlLanguageModelForSequenceOrdering&quot;</span>
            <span class="p">)</span>
        <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
            <span class="s2">&quot;--model.model_name_or_path&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="s2">&quot;bert-base-cased&quot;</span>
        <span class="p">)</span>
        <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--model.lr&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">)</span>
        <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--model.target_token_id&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">parent_parser</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="data">
<h3>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h3>
<p>In contrast to the model class, we design our version of the <code class="docutils literal notranslate"><span class="pre">LightningDataModule</span></code> to work with any Huggingface <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>.
Most of the work is done by the <code class="docutils literal notranslate"><span class="pre">.prepare_data</span></code>-method, which implements the processing pipeline for the contained dataset
Firstly, it applies all functions to prepare the data via the <code class="docutils literal notranslate"><span class="pre">.map</span></code>-method of the <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> class.
Afterward, the text data will be tokenized using the passed instance of the tokenizer.
Lastly, it is ensured that the dataset’s column containing the target is named <code class="docutils literal notranslate"><span class="pre">labels</span></code> to be compliant with standard <code class="docutils literal notranslate"><span class="pre">transformers</span></code> models.
Additionally, we implement a method to use the map functionalities of the contained dataset directly.
This method allows the manipulation of the data manually since the <code class="docutils literal notranslate"><span class="pre">.prepare_data</span></code>-method is automatically executed by the <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>.
The datasets wrapped in this class should already contain train-/ test- and validation-splits.
To create batches of the data, we use the default collation function of the transformers library but allow passing a custom collation function.</p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HuggingfaceDatasetWrapper</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">text_column</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">target_column</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PreTrainedTokenizerBase</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
        <span class="n">eval_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span>
        <span class="n">mapping_funcs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Callable</span> <span class="o">=</span> <span class="n">default_data_collator</span><span class="p">,</span>
        <span class="n">train_split_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;train&quot;</span><span class="p">,</span>
        <span class="n">eval_split_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;val&quot;</span><span class="p">,</span>
        <span class="n">test_split_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;test&quot;</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_column</span> <span class="o">=</span> <span class="n">text_column</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_column</span> <span class="o">=</span> <span class="n">target_column</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span> <span class="o">=</span> <span class="n">train_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_batch_size</span> <span class="o">=</span> <span class="n">eval_batch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mapping_funcs</span> <span class="o">=</span> <span class="n">mapping_funcs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">collate_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_split_name</span> <span class="o">=</span> <span class="n">train_split_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_split_name</span> <span class="o">=</span> <span class="n">eval_split_name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_split_name</span> <span class="o">=</span> <span class="n">test_split_name</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 1. Apply user defined preparation functions</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping_funcs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">mapping_func</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">mapping_funcs</span><span class="p">:</span>
                <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">mapping_func</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># 2. Tokenize the text</span>
        <span class="k">if</span> <span class="n">tokenizer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tokenizer_kwargs</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;truncation&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="s2">&quot;padding&quot;</span><span class="p">:</span> <span class="s2">&quot;max_length&quot;</span><span class="p">,</span>
                <span class="s2">&quot;add_special_tokens&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span>
            <span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">e</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">e</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">text_column</span><span class="p">],</span> <span class="o">**</span><span class="n">tokenizer_kwargs</span><span class="p">),</span>
            <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># 3. Set format of important columns to torch</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
            <span class="s2">&quot;torch&quot;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">,</span> <span class="s2">&quot;attention_mask&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_column</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="c1"># 4. If the target columns is not named &#39;labels&#39; rename it</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">rename_column</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">target_column</span><span class="p">,</span> <span class="s2">&quot;labels&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
            <span class="c1"># target column should already have correct name</span>
            <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">train_split_name</span><span class="p">],</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">train_batch_size</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_split_name</span><span class="p">],</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">test_split_name</span><span class="p">],</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_batch_size</span><span class="p">,</span>
            <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="complete-code">
<h3>Complete code<a class="headerlink" href="#complete-code" title="Permalink to this headline">¶</a></h3>
<p>Once again, after factoring out the custom modules, the actual experiment can be implemented in relatively few lines of code.
To control the experiment via the command line, we use the <code class="docutils literal notranslate"><span class="pre">LightningArgumentParser</span></code>.
We initialize the parser with all arguments from the <code class="docutils literal notranslate"><span class="pre">Trainer,</span></code> <code class="docutils literal notranslate"><span class="pre">PlLanguageModelForSequenceOrdering</span></code>, and <code class="docutils literal notranslate"><span class="pre">HuggingfaceDatasetWrapper</span></code>.
Additionally, we add more parameters to give each run a name and control the batch sizes for training and testing.
Similar to implementing the experiment with the Huggingface <code class="docutils literal notranslate"><span class="pre">Trainer</span></code>, we need to ensure that the sentences contain the correct special tokens.
Replacing these tokens if necessary can be done using the <code class="docutils literal notranslate"><span class="pre">.map</span></code>-method of the <code class="docutils literal notranslate"><span class="pre">HuggingfaceDatasetWrapper</span></code></p>
<div class="cell tag_skip-execution docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">os.path</span> <span class="kn">import</span> <span class="n">basename</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_from_disk</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning</span> <span class="kn">import</span> <span class="n">Trainer</span><span class="p">,</span> <span class="n">seed_everything</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.loggers.tensorboard</span> <span class="kn">import</span> <span class="n">TensorBoardLogger</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.callbacks</span> <span class="kn">import</span> <span class="n">ModelCheckpoint</span>
<span class="kn">from</span> <span class="nn">pytorch_lightning.utilities.cli</span> <span class="kn">import</span> <span class="n">LightningArgumentParser</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="kn">from</span> <span class="nn">pl_modules</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">HuggingfaceDatasetWrapper</span><span class="p">,</span>
    <span class="n">PlLanguageModelForSequenceOrdering</span><span class="p">,</span>
    <span class="n">so_data_collator</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">model_args</span><span class="p">,</span> <span class="n">trainer_args</span><span class="p">,</span> <span class="n">checkpoint_args</span><span class="p">,</span> <span class="n">tensorboard_args</span><span class="p">,</span> <span class="n">run_args</span><span class="p">):</span>

    <span class="n">seed_everything</span><span class="p">(</span><span class="n">run_args</span><span class="p">[</span><span class="s2">&quot;seed&quot;</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading tokenizer.&quot;</span><span class="p">)</span>
    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_args</span><span class="p">[</span><span class="s2">&quot;model_name_or_path&quot;</span><span class="p">])</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading datasets.&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">load_from_disk</span><span class="p">(</span><span class="s2">&quot;../data/rocstories&quot;</span><span class="p">)</span>

    <span class="c1"># Downsampling for debugging...</span>
    <span class="c1"># data = data.filter(lambda _, index: index &lt; 10000, with_indices=True)</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="n">HuggingfaceDatasetWrapper</span><span class="p">(</span>
        <span class="n">data</span><span class="p">,</span>
        <span class="n">text_column</span><span class="o">=</span><span class="s2">&quot;text&quot;</span><span class="p">,</span>
        <span class="n">target_column</span><span class="o">=</span><span class="s2">&quot;so_targets&quot;</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">mapping_funcs</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">so_data_collator</span><span class="p">,</span>
        <span class="n">train_batch_size</span><span class="o">=</span><span class="n">run_args</span><span class="p">[</span><span class="s2">&quot;train_batch_size&quot;</span><span class="p">],</span>
        <span class="n">eval_batch_size</span><span class="o">=</span><span class="n">run_args</span><span class="p">[</span><span class="s2">&quot;val_batch_size&quot;</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">!=</span> <span class="s2">&quot;[CLS]&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Model does not a have a [CLS] token. Updating the data with token </span><span class="si">{</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token</span><span class="si">}</span><span class="s2"> ...&quot;</span>
        <span class="p">)</span>

        <span class="k">def</span> <span class="nf">replace_cls_token</span><span class="p">(</span><span class="n">entry</span><span class="p">):</span>
            <span class="n">texts</span> <span class="o">=</span> <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
            <span class="n">replaced_texts</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
                <span class="n">replaced_texts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;[CLS]&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token</span><span class="p">))</span>
            <span class="n">entry</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">replaced_texts</span>
            <span class="k">return</span> <span class="n">entry</span>

        <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">replace_cls_token</span><span class="p">,</span> <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">model_args</span><span class="p">[</span><span class="s2">&quot;target_token_id&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">cls_token_id</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loading model.&quot;</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">PlLanguageModelForSequenceOrdering</span><span class="p">(</span><span class="n">hparams</span><span class="o">=</span><span class="n">model_args</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Initializing trainer.&quot;</span><span class="p">)</span>
    <span class="c1"># Init logger</span>
    <span class="n">tensorboard_logger</span> <span class="o">=</span> <span class="n">TensorBoardLogger</span><span class="p">(</span><span class="o">**</span><span class="n">tensorboard_args</span><span class="p">)</span>

    <span class="c1"># Init callbacks</span>
    <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">checkpoint_callback</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="o">**</span><span class="n">checkpoint_args</span><span class="p">)</span>
    <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">checkpoint_callback</span><span class="p">)</span>

    <span class="c1"># Remove default args</span>
    <span class="n">trainer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;logger&quot;</span><span class="p">)</span>
    <span class="n">trainer_args</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;callbacks&quot;</span><span class="p">)</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">logger</span><span class="o">=</span><span class="n">tensorboard_logger</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span> <span class="o">**</span><span class="n">trainer_args</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start training.&quot;</span><span class="p">)</span>
    <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">dataset</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Start testing.&quot;</span><span class="p">)</span>
    <span class="n">test_results</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">ckpt_path</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;test_results_</span><span class="si">{</span><span class="n">model_args</span><span class="p">[</span><span class="s1">&#39;model_name_or_path&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">test_results</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">LightningArgumentParser</span><span class="p">()</span>
    <span class="n">group</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument_group</span><span class="p">()</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--run.run_name&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="n">basename</span><span class="p">(</span><span class="vm">__file__</span><span class="p">))</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--run.seed&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--run.train_batch_size&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">group</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--run.val_batch_size&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_lightning_class_args</span><span class="p">(</span><span class="n">ModelCheckpoint</span><span class="p">,</span> <span class="s2">&quot;checkpoint&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_class_arguments</span><span class="p">(</span><span class="n">TensorBoardLogger</span><span class="p">,</span> <span class="n">nested_key</span><span class="o">=</span><span class="s2">&quot;tensorboard&quot;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_lightning_class_args</span><span class="p">(</span><span class="n">Trainer</span><span class="p">,</span> <span class="s2">&quot;trainer&quot;</span><span class="p">)</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">PlLanguageModelForSequenceOrdering</span><span class="o">.</span><span class="n">add_model_specific_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>

    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>

    <span class="n">model_args</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;model&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">trainer_args</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;trainer&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">checkpoint_args</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;checkpoint&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">tensorboard_args</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;tensorboard&quot;</span><span class="p">,</span> <span class="p">{})</span>
    <span class="n">run_args</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;run&quot;</span><span class="p">,</span> <span class="p">{})</span>

    <span class="n">main</span><span class="p">(</span><span class="n">model_args</span><span class="p">,</span> <span class="n">trainer_args</span><span class="p">,</span> <span class="n">checkpoint_args</span><span class="p">,</span> <span class="n">tensorboard_args</span><span class="p">,</span> <span class="n">run_args</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">¶</a></h2>
<p>Pytorch Lightning’s goal is not to hide complexity from the user. Instead, it provides an API that helps to structure the complexity into a sequence of single steps.
This approach is constructive when designing custom models from scratch or implementing new training regimes that differ from the standard training loop.
This flexibility comes at the cost of friendliness to beginners. People who have little experience with PyTorch itself will quickly be overwhelmed by PyTorch Lightning API with vast possibilities to customize steps manually.
Even though the documentation is extensive and covers nearly all aspects of the library in great detail, it can be frustrating sometimes that there are multiple ways to achieve the same behavior, and there is little to no guidance in choosing between the different parts.
Like most modern deep learning frameworks, PyTorch Lightning is rapidly evolving, and thus many parts of it are either in beta and subject to significant changes in the future or deprecated. Unfortunately, this is also noticeable when searching the web for further advice since many tips or tutorials quickly become outdated.
Nevertheless, despite these limitations for beginners, experienced users can benefit from using PyTorch Lightning. Not only because of the additional features like built-in logging, tuning, or other tweaks but mainly because the well-thought API enforces them to write self-contained models that contain all the logic for experimenting with them.
This approach effortlessly enables sharing of models and also alleviates maintainability.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="HuggingFaceTrainer.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Huggingface Trainer</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Poutyne.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Poutyne</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Lennart Keller<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>