{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a83123",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import set_caching_enabled\n",
    "set_caching_enabled(False)\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(depth=6, compact=True)\n",
    "print = pp.pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f5c66",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from poutyne import Model\n",
    "\n",
    "...\n",
    "\n",
    "network = make_network()\n",
    "X_train, y_train = load_data(subset=\"train\")\n",
    "X_val, y_val = load_data(subset=\"validation\")\n",
    "X_test, y_test = load_data(subset=\"test\")\n",
    "\n",
    "model = Model(\n",
    "    network,\n",
    "    \"sgd\",\n",
    "    \"cross_entropy\",\n",
    "    batch_metrics=[\"accuracy\"],\n",
    "    epoch_metrics=[\"f1\"],\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "results = model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2449ae",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from transformers import default_data_collator\n",
    "\n",
    "\n",
    "class TransformerCollator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        y_keys: Union[str, List[str]] = None,\n",
    "        custom_collator: Callable = None,\n",
    "        remove_labels: bool = False,\n",
    "    ):\n",
    "        self.y_keys = y_keys\n",
    "        self.custom_collator = (\n",
    "            custom_collator if custom_collator is not None else default_data_collator\n",
    "        )\n",
    "        self.remove_labels = remove_labels\n",
    "\n",
    "    def __call__(self, inputs: Tuple[Dict]) -> Tuple[Dict, Any]:\n",
    "        batch_size = len(inputs)\n",
    "        batch = self.custom_collator(inputs)\n",
    "        if self.y_keys is None:\n",
    "            y = torch.tensor(float(\"nan\")).repeat(batch_size)\n",
    "        elif isinstance(self.y_keys, list):\n",
    "            y = {\n",
    "                key: batch.pop(key)\n",
    "                if \"labels\" in key and self.remove_labels\n",
    "                else batch.get(key)\n",
    "                for key in self.y_keys\n",
    "            }\n",
    "        else:\n",
    "            y = batch.pop(self.y_keys) if self.remove_labels else batch.get(self.y_keys)\n",
    "        return batch, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f5f5518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['last_hidden_state', 'pooler_output'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "inputs = tokenizer(\"Poutyne is inspired by Keras\", return_tensors=\"pt\")\n",
    "print(model(**inputs).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5d9a9c",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from torch import nn\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "class ModelWrapper(nn.Module):\n",
    "    def __init__(self, transformer: PreTrainedModel):\n",
    "        super().__init__()\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}({repr(self.transformer)})\"\n",
    "\n",
    "    def forward(self, inputs) -> Dict[str, Any]:\n",
    "        return self.transformer(**inputs)\n",
    "\n",
    "    def save_pretrained(self, *args, **kwargs) -> None:\n",
    "        self.transformer.save_pretrained(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100da176",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class PoutyneSequenceOrderingLoss:\n",
    "    def __init__(self, target_token_id):\n",
    "        self.target_token_id = target_token_id\n",
    "\n",
    "    def __call__(self, outputs, targets) -> float:\n",
    "        batch_labels = targets[\"labels\"]\n",
    "        batch_logits = outputs[\"logits\"]\n",
    "        batch_input_ids = targets[\"input_ids\"]\n",
    "\n",
    "        # Since we have varying number of labels per instance, we need to compute the loss manually for each one.\n",
    "        loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "        batch_loss = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n",
    "        for labels, logits, input_ids in zip(\n",
    "            batch_labels, batch_logits, batch_input_ids\n",
    "        ):\n",
    "            # Firstly, we need to convert the sentence indices to regression targets.\n",
    "            # To avoid exploding gradients, we norm them to be in range 0 <-> 1\n",
    "            # Also we need to remove the padding entries (-100)\n",
    "            true_labels = labels[labels != -100].reshape(-1)\n",
    "            targets = true_labels.float()\n",
    "\n",
    "            # Secondly, we need to get the logits from each target token in the input sequence\n",
    "            target_logits = logits[input_ids == self.target_token_id].reshape(-1)\n",
    "\n",
    "            # Sometimes we will have less target_logits than targets due to trunction of the input\n",
    "            # In this case, we just consider as many targets as we have logits\n",
    "            if target_logits.size(0) < targets.size(0):\n",
    "                targets = targets[: target_logits.size(0)]\n",
    "\n",
    "            # Finally we compute the loss for the current instance and add it to the batch loss\n",
    "            batch_loss = batch_loss + loss_fn(targets, target_logits)\n",
    "\n",
    "        # The final loss is obtained by averaging over the number of instances per batch\n",
    "        loss = batch_loss / batch_logits.size(0)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29ecd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict\n",
    "\n",
    "class MetricWrapper:\n",
    "    def __init__(self, metric: Callable, pred_key: str = \"logits\", y_key: str = None):\n",
    "        self.metric = metric\n",
    "        self.pred_key = pred_key\n",
    "        self.y_key = y_key\n",
    "        self._set_metric_name(metric)\n",
    "\n",
    "    def _set_metric_name(self, metric):\n",
    "        self.__name__ = metric.__name__\n",
    "\n",
    "    def __call__(self, outputs: Dict[str, Any], y_true: Any):\n",
    "        y_pred = outputs[self.pred_key]\n",
    "        if self.y_key is not None:\n",
    "            y_true = outputs[self.y_key]\n",
    "        return self.metric(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92e4f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acc', 'kendalls_tau', 'mean_logits', 'std_logits']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import kendalltau\n",
    "\n",
    "def make_compute_metrics_functions(target_token_id) -> Callable:\n",
    "    def compute_ranking_func(\n",
    "        outputs: Dict, targets: Any, metric_key: str\n",
    "    ) -> Dict[str, float]:\n",
    "        batch_sent_idx = targets[\"labels\"].detach().cpu().numpy()\n",
    "        batch_input_ids = targets[\"input_ids\"].detach().cpu().numpy()\n",
    "        batch_logits = outputs.detach().cpu().numpy()\n",
    "\n",
    "        metrics = defaultdict(list)\n",
    "        for sent_idx, input_ids, logits in zip(\n",
    "            batch_sent_idx, batch_input_ids, batch_logits\n",
    "        ):\n",
    "            sent_idx = sent_idx.reshape(-1)\n",
    "            input_ids = input_ids.reshape(-1)\n",
    "            logits = logits.reshape(-1)\n",
    "\n",
    "            sent_idx = sent_idx[sent_idx != 100]\n",
    "            target_logits = logits[input_ids == target_token_id]\n",
    "            if sent_idx.shape[0] > target_logits.shape[0]:\n",
    "                sent_idx = sent_idx[: target_logits.shape[0]]\n",
    "            # Calling argsort twice on the logits gives us their ranking in ascending order\n",
    "            predicted_idx = np.argsort(np.argsort(target_logits))\n",
    "            tau, pvalue = kendalltau(sent_idx, predicted_idx)\n",
    "            acc = accuracy_score(sent_idx, predicted_idx)\n",
    "            metrics[\"kendalls_tau\"].append(tau)\n",
    "            metrics[\"acc\"].append(acc)\n",
    "            metrics[\"mean_logits\"].append(logits.mean())\n",
    "            metrics[\"std_logits\"].append(logits.std())\n",
    "        metrics = {metric: np.mean(scores) for metric, scores in metrics.items()}\n",
    "        return metrics[metric_key]\n",
    "\n",
    "    metrics = []\n",
    "    for metric in (\"acc\", \"kendalls_tau\", \"mean_logits\", \"std_logits\"):\n",
    "        metric_func = partial(compute_ranking_func, metric_key=metric)\n",
    "        metric_func.__name__ = metric\n",
    "        metrics.append(metric_func)\n",
    "    return metrics\n",
    "\n",
    "metrics = [\n",
    "        MetricWrapper(func)\n",
    "        for func in make_compute_metrics_functions(0)\n",
    "    ]\n",
    "print([metric.__name__ for metric in metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2304d33",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from poutyne.framework import experiment\n",
    "from torch.optim import AdamW\n",
    "from poutyne import (\n",
    "    set_seeds,\n",
    "    TensorBoardLogger,\n",
    "    TensorBoardGradientTracker,\n",
    "    Experiment,\n",
    ")\n",
    "from poutyne_transformers import ModelWrapper, MetricWrapper, TransformerCollator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import load_from_disk\n",
    "from poutyne_modules import (\n",
    "    make_tokenization_func,\n",
    "    PoutyneSequenceOrderingLoss,\n",
    "    make_compute_metrics_functions,\n",
    "    so_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    set_seeds(42)\n",
    "\n",
    "    MODEL_NAME_OR_PATH = \"bert-base-cased\"\n",
    "    LEARNING_RATE = 3e-5\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VAL_BATCH_SIZE = 16\n",
    "    DEVICE = 0\n",
    "    N_EPOCHS = 3\n",
    "    SAVE_DIR = \"experiments/rocstories/bert\"\n",
    "\n",
    "    print(\"Loading model & tokenizer.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "    transformer = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME_OR_PATH, return_dict=True, num_labels=1\n",
    "    )\n",
    "\n",
    "    print(\"Loading & preparing data.\")\n",
    "    dataset = load_from_disk(\"../data/rocstories/\")\n",
    "\n",
    "    if tokenizer.cls_token != \"[CLS]\":\n",
    "        print(\n",
    "            f\"Model does not a have a [CLS] token. Updating the data with token {tokenizer.cls_token} ...\"\n",
    "        )\n",
    "\n",
    "        def replace_cls_token(entry):\n",
    "            texts = entry[\"text\"]\n",
    "            replaced_texts = []\n",
    "            for text in texts:\n",
    "                replaced_texts.append(text.replace(\"[CLS]\", tokenizer.cls_token))\n",
    "            entry[\"text\"] = replaced_texts\n",
    "            return entry\n",
    "\n",
    "        dataset = dataset.map(replace_cls_token, batched=True)\n",
    "\n",
    "    tokenization_func = make_tokenization_func(\n",
    "        tokenizer=tokenizer,\n",
    "        text_column=\"text\",\n",
    "        add_special_tokens=False,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    dataset = dataset.map(tokenization_func, batched=True)\n",
    "\n",
    "    dataset = dataset.rename_column(\"so_targets\", \"labels\")\n",
    "\n",
    "    dataset = dataset.remove_columns(\n",
    "        [\"text\", \"storyid\", \"storytitle\"] + [f\"sentence{i}\" for i in range(1, 6)]\n",
    "    )\n",
    "    dataset.set_format(\"torch\")\n",
    "\n",
    "    collate_fn = TransformerCollator(\n",
    "        y_keys=[\"labels\", \"input_ids\"],\n",
    "        custom_collator=so_data_collator,\n",
    "        remove_labels=True,\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        dataset[\"train\"], batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        dataset[\"val\"], batch_size=VAL_BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset[\"test\"], batch_size=VAL_BATCH_SIZE, collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    print(\"Preparing training.\")\n",
    "    wrapped_transformer = ModelWrapper(transformer)\n",
    "    optimizer = AdamW(wrapped_transformer.parameters(), lr=LEARNING_RATE)\n",
    "    loss_fn = PoutyneSequenceOrderingLoss(target_token_id=tokenizer.cls_token_id)\n",
    "\n",
    "    metrics = [\n",
    "        MetricWrapper(func)\n",
    "        for func in make_compute_metrics_functions(tokenizer.cls_token_id)\n",
    "    ]\n",
    "\n",
    "    writer = SummaryWriter(\"runs/roberta/1\")\n",
    "    tensorboard_logger = TensorBoardLogger(writer)\n",
    "    gradient_logger = TensorBoardGradientTracker(writer)\n",
    "\n",
    "    experiment = Experiment(\n",
    "        directory=SAVE_DIR,\n",
    "        network=wrapped_transformer,\n",
    "        device=DEVICE,\n",
    "        logging=True,\n",
    "        optimizer=optimizer,\n",
    "        loss_function=loss_fn,\n",
    "        batch_metrics=metrics,\n",
    "    )\n",
    "\n",
    "    experiment.train(\n",
    "        train_generator=train_dataloader,\n",
    "        valid_generator=val_dataloader,\n",
    "        epochs=N_EPOCHS,\n",
    "        save_every_epoch=True,\n",
    "    )\n",
    "\n",
    "    test_results = experiment.test(test_generator=test_dataloader)\n",
    "    with open(f\"test_results_{MODEL_NAME_OR_PATH}.json\", \"w\") as f:\n",
    "        json.dump(test_results, f)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "source_map": [
   14,
   23,
   33,
   62,
   126,
   162,
   168,
   176,
   184,
   204,
   213,
   251,
   260,
   278,
   284,
   334,
   343,
   468
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}