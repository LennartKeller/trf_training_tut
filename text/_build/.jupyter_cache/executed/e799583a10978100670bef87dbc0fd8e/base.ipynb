{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9b4c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "class DenseLayer(nn.Module):\n",
    "    \"\"\"Fully connected linear layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(in_shape, out_shape), requires_grad=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return torch.matmul(inputs, self.weights)\n",
    "\n",
    "network = nn.Sequential(\n",
    "    DenseLayer(512, 16),\n",
    "    nn.ReLU(),\n",
    "    DenseLayer(16, 8),\n",
    "    nn.ReLU(),\n",
    "    DenseLayer(8, 2)\n",
    ")\n",
    "\n",
    "inputs = torch.randn(8, 512)  # Batchsize 8\n",
    "outputs = network(inputs)\n",
    "print(outputs.size())\n",
    "print(issubclass(nn.Sequential, nn.Module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefe79d",
   "metadata": {
    "name": "lightning-parser-listing",
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "parser.add_lightning_class_args(ModelCheckpoint, \"checkpoint\")\n",
    "parser.add_class_arguments(TensorBoardLogger, nested_key=\"tensorboard\")\n",
    "parser.add_lightning_class_args(Trainer, \"trainer\")\n",
    "parser = PlLanguageModelForSequenceOrdering.add_model_specific_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d90395b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class PlLanguageModelForSequenceOrdering(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(hparams)\n",
    "        self.base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "            self.hparams[\"model_name_or_path\"],\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "            num_labels=1,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25628695",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def forward(self, inputs: Dict[Any, Any]) -> Dict[Any, Any]:\n",
    "        # We do not want to compute token classification loss, so we remove the labels temporarily\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = self.base_model(**inputs)\n",
    "\n",
    "        # And reattach them later on ...\n",
    "        inputs[\"labels\"] = labels\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6674e8c5",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def _compute_loss(self, batch_labels, batch_logits, batch_input_ids) -> float:\n",
    "        # Since we have varying number of labels per instance, \n",
    "        # we need to compute the loss manually for each one.\n",
    "        loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "        batch_loss = torch.tensor(0.0, dtype=torch.float64, requires_grad=True)\n",
    "        for labels, logits, input_ids in zip(\n",
    "            batch_labels, batch_logits, batch_input_ids\n",
    "        ):\n",
    "\n",
    "            # Firstly, we need to convert the sentence indices to regression targets.\n",
    "            # To avoid exploding gradients, we norm them to be in range 0 <-> 1.\n",
    "            # labels = labels / labels.max()\n",
    "            # Also we need to remove the padding entries (-100).\n",
    "            true_labels = labels[labels != -100].reshape(-1)\n",
    "            targets = true_labels.float()\n",
    "\n",
    "            # Secondly, we need to get the logits \n",
    "            # from each target token in the input sequence\n",
    "            target_logits = logits[\n",
    "                input_ids == self.hparams[\"target_token_id\"]\n",
    "            ].reshape(-1)\n",
    "\n",
    "            # Sometimes we will have less target_logits \n",
    "            # than targets due to truncation of the input.\n",
    "            # In this case, we just consider as many targets as we have logit.\n",
    "            if target_logits.size(0) < targets.size(0):\n",
    "                targets = targets[: target_logits.size(0)]\n",
    "\n",
    "            # Finally we compute the loss for the current instance \n",
    "            # and add it to the batch loss.\n",
    "            batch_loss = batch_loss + loss_fn(targets, target_logits)\n",
    "\n",
    "        # The final loss is obtained by averaging \n",
    "        # over the number of instances per batch.\n",
    "        loss = batch_loss / batch_logits.size(0)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _forward_with_loss(self, inputs):\n",
    "        outputs = self(inputs)\n",
    "\n",
    "        # Get sentence indices\n",
    "        batch_labels = inputs[\"labels\"]\n",
    "        # Get logits from model\n",
    "        batch_logits = outputs[\"logits\"]\n",
    "        # Get logits for all cls tokens\n",
    "        batch_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        loss = self._compute_loss(\n",
    "            batch_labels=batch_labels,\n",
    "            batch_logits=batch_logits,\n",
    "            batch_input_ids=batch_input_ids,\n",
    "        )\n",
    "        outputs[\"loss\"] = loss\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec0793",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def training_step(self, inputs: Dict[Any, Any], batch_idx: int) -> float:\n",
    "        outputs = self._forward_with_loss(inputs)\n",
    "        loss = outputs[\"loss\"]\n",
    "        self.log(\"loss\", loss, logger=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356845b9",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def validation_step(self, inputs, batch_idx):\n",
    "        outputs = self._forward_with_loss(inputs)\n",
    "\n",
    "        # Detach all torch.tensors and convert them to np.arrays.\n",
    "        for key, value in outputs.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                outputs[key] = value.detach().cpu().numpy()\n",
    "        for key, value in inputs.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                inputs[key] = value.detach().cpu().numpy()\n",
    "\n",
    "        # Get sentence indices\n",
    "        batch_labels = inputs[\"labels\"]\n",
    "        # Get logits from model\n",
    "        batch_logits = outputs[\"logits\"]\n",
    "        # Get logits for all cls tokens\n",
    "        batch_input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        metrics = defaultdict(list)\n",
    "        for sent_idx, input_ids, logits in zip(\n",
    "            batch_labels, batch_input_ids, batch_logits\n",
    "        ):\n",
    "            sent_idx = sent_idx.reshape(-1)\n",
    "            input_ids = input_ids.reshape(-1)\n",
    "            logits = logits.reshape(-1)\n",
    "\n",
    "            sent_idx = sent_idx[sent_idx != 100]\n",
    "            target_logits = logits[input_ids == self.hparams[\"target_token_id\"]]\n",
    "            if sent_idx.shape[0] > target_logits.shape[0]:\n",
    "                sent_idx = sent_idx[: target_logits.shape[0]]\n",
    "\n",
    "            # Calling argsort twice on the logits \n",
    "            # gives us their ranking in ascending order.\n",
    "            predicted_idx = np.argsort(np.argsort(target_logits))\n",
    "            tau, pvalue = kendalltau(sent_idx, predicted_idx)\n",
    "            acc = accuracy_score(sent_idx, predicted_idx)\n",
    "            metrics[\"kendalls_tau\"].append(tau)\n",
    "            metrics[\"acc\"].append(acc)\n",
    "            metrics[\"mean_logits\"].append(logits.mean().item())\n",
    "            metrics[\"std_logits\"].append(logits.std().item())\n",
    "\n",
    "        metrics[\"loss\"] = outputs[\"loss\"].item()\n",
    "\n",
    "        # Add val prefix to each metric name and compute mean over the batch.\n",
    "        metrics = {\n",
    "            f\"val_{metric}\": np.mean(scores).item()\n",
    "            for metric, scores in metrics.items()\n",
    "        }\n",
    "        self.log_dict(metrics, prog_bar=True, logger=True, on_epoch=True, on_step=True)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, inputs, batch_idx):\n",
    "        return self.validation_step(inputs, batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9505d5b",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(params=self.parameters(), lr=self.hparams[\"lr\"])\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = parent_parser.add_argument_group(\n",
    "            \"PlLanguageModelForSequenceOrdering\"\n",
    "            )\n",
    "        parser.add_argument(\n",
    "            \"--model.model_name_or_path\", type=str, default=\"bert-base-cased\"\n",
    "        )\n",
    "        parser.add_argument(\"--model.lr\", type=float, default=3e-5)\n",
    "        parser.add_argument(\"--model.target_token_id\", type=int, default=101)\n",
    "        return parent_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1010e254",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "class HuggingfaceDatasetWrapper(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        text_column: str,\n",
    "        target_column: str,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        train_batch_size: int = 8,\n",
    "        eval_batch_size: int = 16,\n",
    "        mapping_funcs: List[Callable] = None,\n",
    "        collate_fn: Callable = default_data_collator,\n",
    "        train_split_name: str = \"train\",\n",
    "        eval_split_name: str = \"val\",\n",
    "        test_split_name: str = \"test\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.text_column = text_column\n",
    "        self.target_column = target_column\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.eval_batch_size = eval_batch_size\n",
    "        self.mapping_funcs = mapping_funcs\n",
    "        self.collate_fn = collate_fn\n",
    "        self.train_split_name = train_split_name\n",
    "        self.eval_split_name = eval_split_name\n",
    "        self.test_split_name = test_split_name\n",
    "\n",
    "    def prepare_data(self, tokenizer_kwargs: Dict[str, str] = None):\n",
    "        # 1. Apply user defined preparation functions\n",
    "        if self.mapping_funcs:\n",
    "            for mapping_func in self.mapping_funcs:\n",
    "                dataset = dataset.map(mapping_func, batched=True)\n",
    "\n",
    "        # 2. Tokenize the text\n",
    "        if tokenizer_kwargs is None:\n",
    "            tokenizer_kwargs = {\n",
    "                \"truncation\": True,\n",
    "                \"padding\": \"max_length\",\n",
    "                \"add_special_tokens\": False,\n",
    "            }\n",
    "        self.dataset = self.dataset.map(\n",
    "            lambda e: self.tokenizer(e[self.text_column], **tokenizer_kwargs),\n",
    "            batched=True,\n",
    "        )\n",
    "        # 3. Set format of important columns to torch\n",
    "        self.dataset.set_format(\n",
    "            \"torch\", columns=[\"input_ids\", \"attention_mask\", self.target_column]\n",
    "        )\n",
    "        # 4. If the target columns is not named 'labels' rename it\n",
    "        try:\n",
    "            self.dataset = self.dataset.rename_column(self.target_column, \"labels\")\n",
    "        except ValueError:\n",
    "            # target column should already have correct name\n",
    "            pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset[self.train_split_name],\n",
    "            batch_size=self.train_batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset[self.eval_split_name],\n",
    "            batch_size=self.eval_batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset[self.test_split_name],\n",
    "            batch_size=self.eval_batch_size,\n",
    "            collate_fn=self.collate_fn,\n",
    "        )\n",
    "\n",
    "    def map(self, *args, **kwargs):\n",
    "        self.dataset = self.dataset.map(*args, **kwargs)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d1fa6",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import basename\n",
    "from datasets import load_from_disk\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.cli import LightningArgumentParser\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from pl_modules import (\n",
    "    HuggingfaceDatasetWrapper,\n",
    "    PlLanguageModelForSequenceOrdering,\n",
    "    so_data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "def main(model_args, trainer_args, checkpoint_args, tensorboard_args, run_args):\n",
    "\n",
    "    seed_everything(run_args[\"seed\"])\n",
    "\n",
    "    print(\"Loading tokenizer.\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_args[\"model_name_or_path\"])\n",
    "\n",
    "    print(\"Loading datasets.\")\n",
    "    data = load_from_disk(\"../data/rocstories\")\n",
    "\n",
    "    # Downsampling for debugging...\n",
    "    # data = data.filter(lambda _, index: index < 10000, with_indices=True)\n",
    "\n",
    "    dataset = HuggingfaceDatasetWrapper(\n",
    "        data,\n",
    "        text_column=\"text\",\n",
    "        target_column=\"so_targets\",\n",
    "        tokenizer=tokenizer,\n",
    "        mapping_funcs=[],\n",
    "        collate_fn=so_data_collator,\n",
    "        train_batch_size=run_args[\"train_batch_size\"],\n",
    "        eval_batch_size=run_args[\"val_batch_size\"],\n",
    "    )\n",
    "\n",
    "    if tokenizer.cls_token != \"[CLS]\":\n",
    "        print(\n",
    "            f\"Model does not a have a [CLS] token. Updating the data with token {tokenizer.cls_token} ...\"\n",
    "        )\n",
    "\n",
    "        def replace_cls_token(entry):\n",
    "            texts = entry[\"text\"]\n",
    "            replaced_texts = []\n",
    "            for text in texts:\n",
    "                replaced_texts.append(text.replace(\"[CLS]\", tokenizer.cls_token))\n",
    "            entry[\"text\"] = replaced_texts\n",
    "            return entry\n",
    "\n",
    "        dataset = dataset.map(replace_cls_token, batched=True)\n",
    "        model_args[\"target_token_id\"] = tokenizer.cls_token_id\n",
    "\n",
    "    print(\"Loading model.\")\n",
    "    model = PlLanguageModelForSequenceOrdering(hparams=model_args)\n",
    "\n",
    "    print(\"Initializing trainer.\")\n",
    "    # Init logger\n",
    "    tensorboard_logger = TensorBoardLogger(**tensorboard_args)\n",
    "\n",
    "    # Init callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(**checkpoint_args)\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    # Remove default args\n",
    "    trainer_args.pop(\"logger\")\n",
    "    trainer_args.pop(\"callbacks\")\n",
    "    trainer = Trainer(logger=tensorboard_logger, callbacks=callbacks, **trainer_args)\n",
    "\n",
    "    print(\"Start training.\")\n",
    "    trainer.fit(model=model, datamodule=dataset)\n",
    "\n",
    "    print(\"Start testing.\")\n",
    "    test_results = trainer.test(model=model, datamodule=dataset, ckpt_path=None)\n",
    "    with open(f\"test_results_{model_args['model_name_or_path']}.json\", \"w\") as f:\n",
    "        json.dump(test_results, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = LightningArgumentParser()\n",
    "    group = parser.add_argument_group()\n",
    "    group.add_argument(\"--run.run_name\", type=str, default=basename(__file__))\n",
    "    group.add_argument(\"--run.seed\", type=int, default=0)\n",
    "    group.add_argument(\"--run.train_batch_size\", type=int, default=8)\n",
    "    group.add_argument(\"--run.val_batch_size\", type=int, default=16)\n",
    "\n",
    "    parser.add_lightning_class_args(ModelCheckpoint, \"checkpoint\")\n",
    "    parser.add_class_arguments(TensorBoardLogger, nested_key=\"tensorboard\")\n",
    "    parser.add_lightning_class_args(Trainer, \"trainer\")\n",
    "    parser = PlLanguageModelForSequenceOrdering.add_model_specific_args(parser)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    model_args = args.get(\"model\", {})\n",
    "    trainer_args = args.get(\"trainer\", {})\n",
    "    checkpoint_args = args.get(\"checkpoint\", {})\n",
    "    tensorboard_args = args.get(\"tensorboard\", {})\n",
    "    run_args = args.get(\"run\", {})\n",
    "\n",
    "    main(model_args, trainer_args, checkpoint_args, tensorboard_args, run_args)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.10.3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "source_map": [
   12,
   48,
   74,
   165,
   172,
   183,
   195,
   199,
   209,
   215,
   273,
   278,
   285,
   290,
   345,
   349,
   365,
   379,
   462,
   474,
   580
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}