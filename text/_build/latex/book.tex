%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]


\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{A comparing guide to train language models with Python.}
\date{Nov 29, 2021}
\release{}
\author{Lennart Keller}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{Home::doc}}


\sphinxAtStartPar
For the sake of readability, the listings in this text are shortened if necessary.
To take a look at the whole code, please visit the associated \sphinxhref{https://github.com/LennartKeller/trf\_training\_tut}{GitHub\sphinxhyphen{}Repository}.
Note that the dataset has to be downloaded manually after registration at \sphinxurl{https://cs.rochester.edu/nlp/rocstories/}.

\sphinxAtStartPar
Just place the CSV\sphinxhyphen{}File \sphinxcode{\sphinxupquote{ROCStories\_winter2017 \sphinxhyphen{} ROCStories\_winter2017.csv}} in the \sphinxcode{\sphinxupquote{scripts/data}} directory and run the \sphinxcode{\sphinxupquote{BuildRocStoriesDataset.ipynb}} notebook to create the dataset.

\sphinxAtStartPar
To run the actual experiments, execute the bash scripts from within their directories.

\sphinxAtStartPar
To run all experiments at once, just run the \sphinxcode{\sphinxupquote{run\_all.sh}} script from within its location

\begin{DUlineblock}{0em}
\item[] \sphinxstylestrong{\Large Execution Summary}
\end{DUlineblock}


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Document
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Modified
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Method
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Run Time (s)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Status
\\
\hline
\sphinxAtStartPar
HuggingFaceTrainer
&
\sphinxAtStartPar
2021\sphinxhyphen{}11\sphinxhyphen{}29 17:06
&
\sphinxAtStartPar
cache
&
\sphinxAtStartPar
4.03
&
\sphinxAtStartPar
✅
\\
\hline
\sphinxAtStartPar
HuggingfaceEcosystem
&
\sphinxAtStartPar
2021\sphinxhyphen{}11\sphinxhyphen{}29 17:06
&
\sphinxAtStartPar
cache
&
\sphinxAtStartPar
12.06
&
\sphinxAtStartPar
✅
\\
\hline
\sphinxAtStartPar
Poutyne
&
\sphinxAtStartPar
2021\sphinxhyphen{}11\sphinxhyphen{}29 17:06
&
\sphinxAtStartPar
cache
&
\sphinxAtStartPar
11.06
&
\sphinxAtStartPar
✅
\\
\hline
\sphinxAtStartPar
Prerequisites
&
\sphinxAtStartPar
2021\sphinxhyphen{}11\sphinxhyphen{}29 17:06
&
\sphinxAtStartPar
cache
&
\sphinxAtStartPar
\sphinxhyphen{}
&
\sphinxAtStartPar
❌
\\
\hline
\sphinxAtStartPar
PyTorchLightning
&
\sphinxAtStartPar
2021\sphinxhyphen{}11\sphinxhyphen{}29 17:06
&
\sphinxAtStartPar
cache
&
\sphinxAtStartPar
1.13
&
\sphinxAtStartPar
✅
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\chapter{Introduction}
\label{\detokenize{Introduction:introduction}}\label{\detokenize{Introduction::doc}}
\sphinxAtStartPar
Transformer\sphinxhyphen{}based neural language models have revolutionized the world of NLP.
Their ability to process long\sphinxhyphen{}range dependencies within texts and gain language processing abilities via self\sphinxhyphen{}supervised pretraining allowed them to set\sphinxhyphen{}state\sphinxhyphen{}of the art results across many different tasks.
These successes made them a technique with great interest across many disciplines of academia and the industry alike.

\sphinxAtStartPar
But training a transformer can be challenging.
One reason for this is the state of the software landscape.
The high rate of innovations in this field drives the development of new models and architectures.
Software libraries trying to keep up with this rapid pace have to regularly adapt to new models and techniques, complicating building robust and stable software.
The other reason lies in the complexity of the models.
Like any other neural network, training a transformer bases mode is a complicated process that requires a lot of technical and domain knowledge.
Also, a robust understanding of how neural networks work and what pitfalls must be avoided is needed.
While no software can replace domain knowledge and general understanding, numerous frameworks aim at lowering the barrier of entry by mitigating the technical complexities.
Depending on the scale of the network, these technical hurdles may include things like multi\sphinxhyphen{}GPU training, data loading, hyperparameter search, progress tracking, early stopping, and many more.
The scope of this work is two\sphinxhyphen{}fold: Firstly, it compares three of these frameworks by using them to train a language model on predicting the correct order of a sequence of shuffled sentences.
Furthermore, it also acts as a tutorial that guides through the basic features of each framework and shows how to adapt them to work with language models.
As the underlying source of pretrained language models, the \sphinxcode{\sphinxupquote{transformers}} library (Wolf \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id10}{2019}{]}) from Huggingface, which has become the de\sphinxhyphen{}facto standard for these models, is used.

\sphinxAtStartPar
The following frameworks are chosen: The built\sphinxhyphen{}in \sphinxcode{\sphinxupquote{Trainer}} of the \sphinxcode{\sphinxupquote{transformers}} library itself, PyTorch Lightning, a framework, which offers a general framework to train neural networks of all kinds; and Poutyne, a library which tries to bring the ease of Tensorflow’s Keras API to PyTorch.
This selection is intended to segment the broader set of available frameworks into different levels of complexity. Du its origin, the \sphinxcode{\sphinxupquote{Trainer}} is highly optimized for working with \sphinxcode{\sphinxupquote{transformers}}. Thus it can be seen as the middle ground between the pro\sphinxhyphen{}level solution PyTorch Lightning and the more beginner\sphinxhyphen{}friendly Poutyne framework.

\sphinxAtStartPar
The rest of the works is structured as follows:
Firstly, a brief introduction to the Huggingface software stack, which serves as the foundation for all experiments, is given.
Next, the experimental design is presented. Since the frameworks only control the actual training of the models, many parts of the experiment will be mostly the same.
These steps are laid out in the next chapter to avoid redundancies in the following chapters, where each library is presented, and the implementational details are discussed.
Then, the results of all experiments are analyzed to check if the choice of a framework affects the model’s performance. And, of course, to see if the sentence ordering works in general.
Finally, we end with a concluding comparison of the frameworks carving out their strengths and weaknesses and discussing their different potential for specific use cases.


\chapter{The Huggingface ecosystem}
\label{\detokenize{HuggingfaceEcosystem:the-huggingface-ecosystem}}\label{\detokenize{HuggingfaceEcosystem::doc}}

\section{\sphinxstyleliteralintitle{\sphinxupquote{tranformers}}}
\label{\detokenize{HuggingfaceEcosystem:tranformers}}
\sphinxAtStartPar
In 2018 on the same day that Google published its research implementation of BERT, developed in Tensorflow, Thomas Wolf, a researcher at the NLP startup Huggingface, created a Github repository called “PyTorch\sphinxhyphen{}transformers.”
The initial goal of this project was to load the weights of the Bert model, published alongside the paper in Tensorflow, with PyTorch.

\sphinxAtStartPar
From here on, this repository quickly evolved into the Transformers library, which sits at the heart of the Huggingface NLP infrastructure. The goal of the transformers library is to provide the majority of transformer\sphinxhyphen{}based neural language models alongside all of the extra tooling required to use them.

\sphinxAtStartPar
Originating as a pure PyTorch library, Huggingface widened its scope over the last two years and integrated other deep learning frameworks such as Tensorflow or the newly created Flax library.
But these additions are relatively unstable and subject to frequent significant changes so that this work will only focus on the much more stable PyTorch branch of the Transformers library.


\section{\sphinxstyleliteralintitle{\sphinxupquote{tokenizers}}}
\label{\detokenize{HuggingfaceEcosystem:tokenizers}}
\sphinxAtStartPar
A notable characteristic of modern language models is that nearly all ship with a custom, fitted tokenizer.
These tokenizers operate on a subword level and are trained to represent texts with a fixed\sphinxhyphen{}sized vocabulary.
Huggingface provides the \sphinxcode{\sphinxupquote{tokenizers}} library that offers implementations of the most common tokenizer models. These tokenizers come in two versions, a fast one written in Rust and a slower python implementation.
For the sake of efficiency, the Rust version is the best choice most of the time.


\section{\sphinxstyleliteralintitle{\sphinxupquote{datasets}}}
\label{\detokenize{HuggingfaceEcosystem:datasets}}
\sphinxAtStartPar
Lastly, to complete the NLP pipeline, Huggingface also develops a library for Dataset management, called \sphinxcode{\sphinxupquote{datasets}}.
This library aims to streamline the process of data preparation and provide a consistent interface to create, store, and process large datasets too large to fit into the memory in a whole.

\sphinxAtStartPar
With these three libraries, it is possible to cover the overwhelming majority of possible tasks.


\section{Interoperability}
\label{\detokenize{HuggingfaceEcosystem:interoperability}}
\sphinxAtStartPar
To make all libraries as interoperable as possible, they use dictionaries or dictionary\sphinxhyphen{}like objects as a standard data exchange format. These dictionaries contain all argument names of the function or method that is supposedly called next as keys and the data as values.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoTokenizer}\PYG{p}{,} \PYG{n}{AutoModel}
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{Dataset}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{AutoModel}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{add\PYGZus{}pooling\PYGZus{}layer}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{Dataset}\PYG{o}{.}\PYG{n}{from\PYGZus{}dict}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Dictionaries? Everywhere!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{dataset}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}

\PYG{n}{inputs} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{pt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
\PYG{n}{outputs} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{inputs}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{outputs}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{last\PYGZus{}hidden\PYGZus{}state}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZob{}\PYGZsq{}text\PYGZsq{}: \PYGZsq{}Dictionaries? Everywhere!\PYGZsq{}\PYGZcb{}
\PYGZob{}\PYGZsq{}input\PYGZus{}ids\PYGZsq{}: tensor([[  101, 12120,  5796,  5927,   136,  4081, 15839,   106,   102]]), \PYGZsq{}token\PYGZus{}type\PYGZus{}ids\PYGZsq{}: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), \PYGZsq{}attention\PYGZus{}mask\PYGZsq{}: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\PYGZcb{}
BaseModelOutputWithPoolingAndCrossAttentions(last\PYGZus{}hidden\PYGZus{}state=tensor([[[ 0.4575,  0.0958, \PYGZhy{}0.0544,  ..., \PYGZhy{}0.1948,  0.3481, \PYGZhy{}0.1724],
         [\PYGZhy{}0.5101, \PYGZhy{}0.1217,  0.8169,  ...,  0.5608,  0.3777, \PYGZhy{}0.0601],
         [\PYGZhy{}0.4231,  0.8276, \PYGZhy{}0.2315,  ..., \PYGZhy{}0.4748, \PYGZhy{}0.1374,  0.2291],
         ...,
         [\PYGZhy{}0.1184, \PYGZhy{}0.2610, \PYGZhy{}0.1218,  ..., \PYGZhy{}0.1848,  0.1142, \PYGZhy{}0.5246],
         [ 0.5114, \PYGZhy{}0.0423,  0.2668,  ...,  0.3748,  0.2570, \PYGZhy{}0.0132],
         [ 0.6647,  0.5592, \PYGZhy{}0.1306,  ..., \PYGZhy{}0.3199,  0.4948, \PYGZhy{}1.2197]]],
       grad\PYGZus{}fn=\PYGZlt{}NativeLayerNormBackward\PYGZgt{}), pooler\PYGZus{}output=None, hidden\PYGZus{}states=None, past\PYGZus{}key\PYGZus{}values=None, attentions=None, cross\PYGZus{}attentions=None)
tensor([[[ 0.4575,  0.0958, \PYGZhy{}0.0544,  ..., \PYGZhy{}0.1948,  0.3481, \PYGZhy{}0.1724],
         [\PYGZhy{}0.5101, \PYGZhy{}0.1217,  0.8169,  ...,  0.5608,  0.3777, \PYGZhy{}0.0601],
         [\PYGZhy{}0.4231,  0.8276, \PYGZhy{}0.2315,  ..., \PYGZhy{}0.4748, \PYGZhy{}0.1374,  0.2291],
         ...,
         [\PYGZhy{}0.1184, \PYGZhy{}0.2610, \PYGZhy{}0.1218,  ..., \PYGZhy{}0.1848,  0.1142, \PYGZhy{}0.5246],
         [ 0.5114, \PYGZhy{}0.0423,  0.2668,  ...,  0.3748,  0.2570, \PYGZhy{}0.0132],
         [ 0.6647,  0.5592, \PYGZhy{}0.1306,  ..., \PYGZhy{}0.3199,  0.4948, \PYGZhy{}1.2197]]],
       grad\PYGZus{}fn=\PYGZlt{}NativeLayerNormBackward\PYGZgt{})
\end{sphinxVerbatim}
\end{sphinxVerbatimOutput}


\section{\sphinxstyleliteralintitle{\sphinxupquote{PyTorch}}\sphinxhyphen{}Backend}
\label{\detokenize{HuggingfaceEcosystem:pytorch-backend}}
\sphinxAtStartPar
Relying on PyTorch as the underlying deep learning framework comes with one caveat: Unlike Tensorflow, which has integrated Keras as a high\sphinxhyphen{}level API for training neural networks, PyTorch does not provide any tools to facilitate the training process.
Instead, PyTorch’s research\sphinxhyphen{}orientated nature makes it entirely up to the users to implement the training loop. While this is no problem when researching and experimenting with new techniques, it is often time\sphinxhyphen{}consuming in the practitioner’s case.
When applying standard models to tasks like text classification, implementing the training loop is an obstacle that only increases development time. Also, it introduces a new space for making errors.

\sphinxAtStartPar
In most application\sphinxhyphen{}oriented scenarios, the training loop roughly looks like this:
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{create\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{DEVICE}\PYG{p}{)}
\PYG{n}{train\PYGZus{}data}\PYG{p}{,} \PYG{n}{val\PYGZus{}data} \PYG{o}{=} \PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{SGD}\PYG{p}{(}\PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{5e\PYGZhy{}5}\PYG{p}{,} \PYG{n}{params}\PYG{o}{=}\PYG{n}{model}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{train\PYGZus{}step}\PYG{p}{,} \PYG{n}{batch} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{train\PYGZus{}data}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{input\PYGZus{}data}\PYG{p}{,} \PYG{n}{targets} \PYG{o}{=} \PYG{n}{batch}
    \PYG{n}{input\PYGZus{}data} \PYG{o}{=} \PYG{n}{input\PYGZus{}data}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{DEVICE}\PYG{p}{)}
    \PYG{n}{targets} \PYG{o}{=} \PYG{n}{targets}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{DEVICE}\PYG{p}{)}
    \PYG{n}{outputs} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{n}{input\PYGZus{}data}\PYG{p}{)}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n}{loss\PYGZus{}function}\PYG{p}{(}\PYG{n}{outputs}\PYG{p}{,} \PYG{n}{targets}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Compute gradients w.r.t the input data}
    \PYG{n}{loss}\PYG{o}{.}\PYG{n}{backward}\PYG{p}{(}\PYG{p}{)} 
    \PYG{c+c1}{\PYGZsh{} Update the parameters of the model}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{p}{)} 
    \PYG{c+c1}{\PYGZsh{} Clear the gradients before next step}
    \PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{zero\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}

    \PYG{n}{train\PYGZus{}log}\PYG{p}{(}\PYG{n}{train\PYGZus{}step}\PYG{p}{,} \PYG{n}{loss}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Validate the performance of the model every 100 train steps}
    \PYG{k}{if} \PYG{n}{train\PYGZus{}step} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{100} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{eval}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{val\PYGZus{}step}\PYG{p}{,} \PYG{n}{batch} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{val\PYGZus{}data}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{input\PYGZus{}data}\PYG{p}{,} \PYG{n}{targets} \PYG{o}{=} \PYG{n}{batch}
                \PYG{n}{input\PYGZus{}data} \PYG{o}{=} \PYG{n}{input\PYGZus{}data}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{DEVICE}\PYG{p}{)}
                \PYG{n}{targets} \PYG{o}{=} \PYG{n}{targets}\PYG{o}{.}\PYG{n}{to}\PYG{p}{(}\PYG{n}{DEVICE}\PYG{p}{)}
            \PYG{k}{with} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{no\PYGZus{}grad}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{outputs} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{n}{input\PYGZus{}data}\PYG{p}{)}
                \PYG{n}{val\PYGZus{}loss} \PYG{o}{=} \PYG{n}{loss\PYGZus{}function}\PYG{p}{(}\PYG{n}{outputs}\PYG{p}{,} \PYG{n}{targets}\PYG{p}{)}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} Compute other val metrics (i.e. accuracy)}
                \PYG{n}{val\PYGZus{}score} \PYG{o}{=} \PYG{n}{other\PYGZus{}metric}\PYG{p}{(}\PYG{n}{outputs}\PYG{p}{,} \PYG{n}{targets}\PYG{p}{)}
                
                \PYG{n}{val\PYGZus{}log}\PYG{p}{(}\PYG{n}{val\PYGZus{}step}\PYG{p}{,} \PYG{n}{val\PYGZus{}loss}\PYG{p}{,} \PYG{n}{val\PYGZus{}loss}\PYG{p}{)}
\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
But not only can it become quite tedious to write this loop (or variations of it) repeatedly, but more gravely, it sets a barrier of entry for beginners or non\sphinxhyphen{}experts because it adds another layer of complexity when tinkering around with deep learning.

\sphinxAtStartPar
Another implication of outsourcing this process to the users hits when the models grow in size. Modern language models may require a massive amount of memory even when trained with tiny batch sizes. There are strategies to overcome these limitations, like gradient accumulation. But all these tricks again have to be implemented by the user.
While one can argue that most of these tweaks are pretty easy to implement, and there is a vast number of educational material available, the downside comes very clear when working with models that do not even fit on a single GPU. These models have to be trained in a distributed manner across multiple devices. When doing so, the training loop itself gets much more complex and challenging to implement.


\chapter{Experimental Design}
\label{\detokenize{Experiment:experimental-design}}\label{\detokenize{Experiment::doc}}

\section{Task}
\label{\detokenize{Experiment:task}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{GraphicsTrfTut}.png}
\caption{Visualization of the sentence ordering task.}\label{\detokenize{Experiment:fig-task-desc}}\end{figure}

\sphinxAtStartPar
To compare the frameworks, we will implement the same experiment with each of them.
The task of the experiment is a critical choice since training a model on a standard objective like text\sphinxhyphen{} or token classification would not require much customization. Also, it would put the Huggingface Trainer into an advantageous position because it supports such tasks out of the box.
To ensure a fair comparison, we chose another quite exotic objective: Sentence Ordering.
Our goal is to train a model to predict the correct order of a sequence of shuffled sentences.
This task seemed right for two reasons.
Firstly, it can be implemented with a standard Huggingface model but requires a custom loss function.
Secondly, the task falls into the category of self\sphinxhyphen{}supervised learning. So it is possible to generate training and test data from unstructured text data in an effortless manner.
Besides these practical implications, the objective is interesting because it can measure the causal coherence of texts. For example, to assess whether the coherence of actions varies between different text types or genres.

\sphinxAtStartPar
But how do we achieve this task?
There are various methods proposed ranging from relatively simple approaches like applying ranking\sphinxhyphen{}loss functions (Zhu \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id7}{2021}{]}) to rather complex ones that learn a graph representation of the sentences and then use topological sorting to extract the correct order of the sentences (Yin \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id6}{2019}{]}).
Because we do not care much about achieving state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art results, we opt for one of the most straightforward approaches and frame it as a regression task.
\hyperref[\detokenize{Experiment:fig-task-desc}]{Fig.\@ \ref{\detokenize{Experiment:fig-task-desc}}} visualizes this approach.
The model should output a regression score for each sentence in the input, indicating its position in the original text.
Therefore, a special token is added as a prefix to each sentence. These tokens act as our targets while training, and they should output a value near to the original index of the sentence in the correct ordered text.
The loss is measured using the Mean\sphinxhyphen{}Squared\sphinxhyphen{}Error objective
The target value for each sentence token is not normalized and ranges from 0 to \(N\), where \(N\) is the number of sentences in the input sequence.
Another even more straightforward approach would be to add a final layer to the network with a fixed size of neurons (one for each sentence), but this would mean we had to know the number of sentences in the input beforehand, which would harm the usability of the model.

\sphinxAtStartPar
Using regression for sentence ordering is not a novel approach.
It was first proposed by McClure \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id8}{2018}{]}, who used it with CNNs and LSTMs and was later on employed as a baseline by Kumar \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id9}{2020}{]} with BERT.
However, in both cases, the authors used the neural network to encode all sentences independently and then fed the sentence embeddings into a regression component.
But we will feed the whole shuffled text into the network. This strategy allows the model to attend to all sentences and their tokens simultaneously, giving it more context to decide on the correct order.
Also, both other authors normalized the values of the regression target to be in the range of \(-1,1\) or \(0,1\), respectively. However, after some brief experiments, we dropped the normalization because it did not yield any benefits. But, in general, dropping the normalization is only feasible with a dataset with a fixed number of sentences because otherwise, it could skew the loss in favor of short texts.

\sphinxAtStartPar
Since the position of the target tokens in the input sequences differs, we need our language model to output one logit for each token. Two Huggingface model variants return a suitable output \sphinxcode{\sphinxupquote{<...ModelType...>ForTokenClassification}} or \sphinxcode{\sphinxupquote{<...ModelType...>ForQuestionAnswering}}. We chose the first one, but all the code in the following section should also run when employing a model with a question\sphinxhyphen{}answering head.


\section{Metrics}
\label{\detokenize{Experiment:metrics}}
\sphinxAtStartPar
To measure the performance of our model, we use two metrics.

\sphinxAtStartPar
\sphinxstylestrong{Accuracy}

\sphinxAtStartPar
Accuracy measures how many sentences per instance are indexed correctly.
Accuracy gives a rough estimate of how well the model performs, but it can paint a misleading picture since it does not fully account for our task’s ranking aspect.
For example, suppose that the model would correctly predict that sentence \(B\) follows sentence \(A\) and expects them to be at position \(0\) and \(1\) in the total ordering.
But in reality, they are the last sentences of the text. So, in this case, the accuracy would be \(0\) (assuming that all other predictions were also wrong).

\sphinxAtStartPar
\sphinxstylestrong{Kendalls Tau}

\sphinxAtStartPar
In contrast to accuracy, Kendall Tau is a ranking correlation coefficient that accounts for partially correct parts of a ranking.
It measures the difference between pairs of sentences correctly predicted as following and all other wrongly predicted pairs.
This value is further corrected for the chance of randomly predicting correct pairs of sentences by dividing it by the total number of unique ways to pick two sentences from the sequence.
\begin{equation*}
\begin{split}
\tau_{\textrm{Kendall}} = \frac{\#\textrm{Correctly predicted pairs of sentences} - \#\textrm{Wrongly predicted pairs of sentences}}{\binom{N}{2}}
\end{split}
\end{equation*}

\section{Dataset}
\label{\detokenize{Experiment:dataset}}
\sphinxAtStartPar
We use the 2017 version of the ROCStories dataset by Mostafazadeh \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id2}{2016}{]}. It contains 52.665 short stories with a fixed length of five sentences. This dataset is commonly used in the literature because its stories mainly depict concrete actions with a clear causal order, making them relatively simple to understand without leaving much space for ambiguities. This property makes it a good fit for testing the general capabilities of language models on this task.

\begin{sphinxadmonition}{warning}{Warning:}
\sphinxAtStartPar
Even though the ROCStories dataset is freely available to the public, anyone who uses it must submit contact data. So the dataset itself is not included in the Github\sphinxhyphen{}Repository and must be downloaded independently from \sphinxurl{https://cs.rochester.edu/nlp/rocstories/}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
In addition, we tested the same experimental setup on a dataset of sentences sampled from german short novels (Novellen) without much success. An insufficient sampling of subparts of the texts is the most likely reason for this failure.
Applying this task to all kinds of different textual domains can be a fruitful question itself but lies outside the scope of this work.
\end{sphinxadmonition}


\chapter{Prerequisites}
\label{\detokenize{Prerequisites:prerequisites}}\label{\detokenize{Prerequisites::doc}}
\sphinxAtStartPar
The following experiments share the same general logic, but the concrete implementation will differ in minor details since each framework has another structural approach.
So before we start, we will take a brief look at the general logic for the data loading parts of the experiment and the computation of the loss function.


\section{Dataset\sphinxhyphen{}preparation}
\label{\detokenize{Prerequisites:dataset-preparation}}
\sphinxAtStartPar
To load the stories, shuffle the sentences, and further prepare, we use Huggingface’s Datasets library, which provides various useful functions for manipulating text data.
Because Huggingface Datasets are fully compatible with PyTorch’s class for data\sphinxhyphen{}loading, they can also be used by all non\sphinxhyphen{}Huggingface libraries without further adjustments.

\sphinxAtStartPar
The preparation itself is simple:
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{Dataset}\PYG{p}{,} \PYG{n}{DatasetDict}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
At first, we load the dataset in its original format.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{Dataset}\PYG{o}{.}\PYG{n}{from\PYGZus{}csv}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../scripts/data/ROCStories\PYGZus{}winter2017 \PYGZhy{} ROCStories\PYGZus{}winter2017.csv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dataset}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
We got 52.665 stories. Each one has a length of five sentences. Additionally, each text has a short title, but we discard them.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Next, we create the training data by shuffling the sentences and creating labels indicating the original order. Also, we add special tokens to each sentence.

\sphinxAtStartPar
We implement the shuffling process using the \sphinxcode{\sphinxupquote{.map}}\sphinxhyphen{}method of the \sphinxcode{\sphinxupquote{Dataset}}\sphinxhyphen{}class.
Following the library’s out\sphinxhyphen{}of\sphinxhyphen{}place policy, the \sphinxcode{\sphinxupquote{.map}}\sphinxhyphen{}method returns a new dataset containing the changes instead of changing the dataset it was called on.

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{.map}}\sphinxhyphen{}method has two modes: batch\sphinxhyphen{}mode or single entry mode. Either way, it receives a dictionary as input where each key represents a column of the dataset.
In single entry mode, the values of the input dictionary hold one entry in the dataset.
In batch mode, the values are lists containing more than one entry.
The following function only works in both modes since it converts both input formats to the same intermediate form, but in general, the batch mode should be preferred to save time.
The output of the function has to be a dictionary in the same format as the input.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{shuffle}
\PYG{k+kn}{from} \PYG{n+nn}{random} \PYG{k+kn}{import} \PYG{n}{seed} \PYG{k}{as} \PYG{n}{set\PYGZus{}seed}

\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}shuffle\PYGZus{}func}\PYG{p}{(}\PYG{n}{sep\PYGZus{}token}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{shuffle\PYGZus{}stories}\PYG{p}{(}\PYG{n}{entries}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{set\PYGZus{}seed}\PYG{p}{(}\PYG{n}{seed}\PYG{p}{)}
        \PYG{n}{entries\PYGZus{}as\PYGZus{}dicts} \PYG{o}{=} \PYG{p}{[}
            \PYG{n+nb}{dict}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{entries}\PYG{p}{,} \PYG{n}{values}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{for} \PYG{n}{values} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{o}{*}\PYG{n}{entries}\PYG{o}{.}\PYG{n}{values}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
        \PYG{p}{]}
        \PYG{n}{converted\PYGZus{}entries} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{entry} \PYG{o+ow}{in} \PYG{n}{entries\PYGZus{}as\PYGZus{}dicts}\PYG{p}{:}
            \PYG{n}{sents} \PYG{o}{=} \PYG{p}{[}
                \PYG{n}{entry}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}
                \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n+nb}{sorted}\PYG{p}{(}
                    \PYG{p}{[}\PYG{n}{key} \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n}{entry}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)} \PYG{k}{if} \PYG{n}{key}\PYG{o}{.}\PYG{n}{startswith}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sentence}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
                    \PYG{p}{]}\PYG{p}{,} \PYG{n}{key}\PYG{o}{=}\PYG{k}{lambda} \PYG{n}{x}\PYG{p}{:} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{x}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
                \PYG{p}{)}
            \PYG{p}{]}
            \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{sents}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{sents\PYGZus{}with\PYGZus{}idx} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{sents}\PYG{p}{,} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{sents\PYGZus{}with\PYGZus{}idx}\PYG{p}{)}
            \PYG{n}{text} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sep\PYGZus{}token}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sep\PYGZus{}token}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ }\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}
                \PYG{p}{[}\PYG{n}{s}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{sents\PYGZus{}with\PYGZus{}idx}\PYG{p}{]}
            \PYG{p}{)} 
            \PYG{n}{so\PYGZus{}targets} \PYG{o}{=} \PYG{p}{[}\PYG{n}{s}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n}{sents\PYGZus{}with\PYGZus{}idx}\PYG{p}{]}
            \PYG{n}{shuffled\PYGZus{}entry} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{text}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{text}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{so\PYGZus{}targets}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{so\PYGZus{}targets}\PYG{p}{\PYGZcb{}}
            \PYG{n}{converted\PYGZus{}entries}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{shuffled\PYGZus{}entry}\PYG{p}{)}
        \PYG{n}{new\PYGZus{}entry} \PYG{o}{=} \PYG{p}{\PYGZob{}}
            \PYG{n}{key}\PYG{p}{:} \PYG{p}{[}\PYG{n}{entry}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{k}{for} \PYG{n}{entry} \PYG{o+ow}{in} \PYG{n}{converted\PYGZus{}entries}\PYG{p}{]}
            \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n}{converted\PYGZus{}entries}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{p}{\PYGZcb{}}
        \PYG{k}{return} \PYG{n}{new\PYGZus{}entry}
    \PYG{k}{return} \PYG{n}{shuffle\PYGZus{}stories}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{{[}CLS{]}}} is one of the special tokens of models directly descending from BERT. During the pretraining stage, it learns a representation of the whole input sequence and thus only occurs once in each input.
Since we do not need a representation of the input as a whole, we use it as the special sentence token.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{map\PYGZus{}func} \PYG{o}{=} \PYG{n}{make\PYGZus{}shuffle\PYGZus{}func}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{[CLS]}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{map\PYGZus{}func}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
After applying the shuffle function, the dataset has two additional columns. The \sphinxcode{\sphinxupquote{text}} column contains the shuffled and concatenated sentences, and the \sphinxcode{\sphinxupquote{so\_targets}} column contains the indices of the sentences in the original order. For example, in the first text in the dataset, the first sentence in the shuffled text is 4th place in the original order.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Lastly, we want to split our dataset into three subsets.
The train\sphinxhyphen{}set is used for training.
The validation set can be used to validate the performance during training or hyperparameter optimization.
The test set will be used for the final evaluation of the final model.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{train\PYGZus{}test} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{n}{test\PYGZus{}validation} \PYG{o}{=} \PYG{n}{train\PYGZus{}test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{DatasetDict}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{train\PYGZus{}test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{test\PYGZus{}validation}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{val}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{test\PYGZus{}validation}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\PYG{n}{dataset}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Finally, we save the dataset.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{save\PYGZus{}to\PYGZus{}disk}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rocstories}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\section{Data loading}
\label{\detokenize{Prerequisites:data-loading}}
\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics{{DataFlow}.png}
\caption{High\sphinxhyphen{}level visualization of data\sphinxhyphen{}flow from the original dataset to the model.}\label{\detokenize{Prerequisites:fig-dataflow}}\end{figure}

\sphinxAtStartPar
From a high\sphinxhyphen{}level view, a Huggingface \sphinxcode{\sphinxupquote{Dataset}} can be seen as a table with columns that correspond to attributes (called features) and rows representing one dataset entry.
From a more concrete technical perspective, the \sphinxcode{\sphinxupquote{Dataset}}\sphinxhyphen{}instance provides an iterable that yields a dictionary for each entry in the dataset. Each dictionary contains attribute\sphinxhyphen{}value pairs.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}

\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{../scripts/data/rocstories}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{features}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
We can’t feed the model with raw texts, so we have to tokenize them beforehand.

\sphinxAtStartPar
As stated before, each model comes with a custom tokenizer, so we have to load it, just like the model itself.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoTokenizer}

\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}dict}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{tokenized\PYGZus{}text} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Jimmy went down the road.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tokenized\PYGZus{}text}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
The tokenizer takes a text or a collection of texts and converts it to a tokenized sequence. Also, it creates additional inputs for the model, such as the attention mask.

\sphinxAtStartPar
To tokenize the whole dataset, we can once again use the map function.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}tokenization\PYGZus{}func}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{p}{,} \PYG{n}{text\PYGZus{}column}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{tokenization}\PYG{p}{(}\PYG{n}{entry}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{entry}\PYG{p}{[}\PYG{n}{text\PYGZus{}column}\PYG{p}{]}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{tokenization}

\PYG{n}{tokenization} \PYG{o}{=} \PYG{n}{make\PYGZus{}tokenization\PYGZus{}func}\PYG{p}{(}
    \PYG{n}{tokenizer}\PYG{o}{=}\PYG{n}{tokenizer}\PYG{p}{,}
    \PYG{n}{text\PYGZus{}column}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{padding}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}length}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{truncation}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
    \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
    \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{np}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{)}

\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{tokenization}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
To feed the data to the neural network, we have to split it up into batches of a fixed size.
To do so, PyTorch provides a general class, called \sphinxcode{\sphinxupquote{torch.utils.data.DataLoader}}, that takes in iterable and returns batches just in time while training.

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{DataLoader}} class is agnostic towards the data it receives. To create batches that are compatible with the Huggingface model, we have to pass it a function that takes in multiple entries from our dataset and converts them into the correct format.

\sphinxAtStartPar
This function is called \sphinxcode{\sphinxupquote{collate\_fn}} and can be specified while initiating the \sphinxcode{\sphinxupquote{DataLoader}} object.
Using a simple identity function, we see that the \sphinxcode{\sphinxupquote{collate\_fn}} receives a list with \(B\) entries where \(B\) is the batch size.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{DataLoader}

\PYG{k}{def} \PYG{n+nf}{identity}\PYG{p}{(}\PYG{n}{batch}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{batch}

\PYG{n}{data\PYGZus{}loader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n}{identity}\PYG{p}{)}
\PYG{n}{batch} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n+nb}{iter}\PYG{p}{(}\PYG{n}{data\PYGZus{}loader}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{batch}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{type}\PYG{p}{(}\PYG{n}{batch}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{batch}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Huggingface provides a collate function that can convert tokenized data into batches in a suitable format.
The Huggingface collation function only works with numeric data such as scalars or arrays. So we have to drop all texts before we pass the dataset into the \sphinxcode{\sphinxupquote{Dataloader}} object.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{remove\PYGZus{}columns}\PYG{p}{(}
    \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{storyid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{storytitle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sentence}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{]}
\PYG{p}{)}
\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{set\PYGZus{}format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{torch}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{features}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
After only numeric data is left, we have to face the last problem in the collation problem.
The Huggingface collation function only handles arrays of the same shape when collating them into one batch. In theory (e.g., with other datasets), we could have a varying number of labels if we wanted to work shuffled texts with a variable number of sentences. We tackle this problem by introducing a custom collation function to make our preparation pipeline as flexible as possible.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{default\PYGZus{}data\PYGZus{}collator}
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{nn}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{rnn} \PYG{k+kn}{import} \PYG{n}{pad\PYGZus{}sequence}

\PYG{k}{def} \PYG{n+nf}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{(}\PYG{n}{batch\PYGZus{}entries}\PYG{p}{,} \PYG{n}{label\PYGZus{}key}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{so\PYGZus{}targets}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Custom dataloader to apply padding to the labels.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{label\PYGZus{}dicts} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

    \PYG{c+c1}{\PYGZsh{} We split the labels from the rest to process them independently}
    \PYG{k}{for} \PYG{n}{entry} \PYG{o+ow}{in} \PYG{n}{batch\PYGZus{}entries}\PYG{p}{:}
        \PYG{n}{label\PYGZus{}dict} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
        \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{entry}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{if} \PYG{n}{label\PYGZus{}key} \PYG{o+ow}{in} \PYG{n}{key}\PYG{p}{:}
                \PYG{n}{label\PYGZus{}dict}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{=} \PYG{n}{entry}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}
        \PYG{n}{label\PYGZus{}dicts}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{label\PYGZus{}dict}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Everything except our labels can easily be handled by the \PYGZdq{}default collator\PYGZdq{}}
    \PYG{n}{batch} \PYG{o}{=} \PYG{n}{default\PYGZus{}data\PYGZus{}collator}\PYG{p}{(}\PYG{n}{batch\PYGZus{}entries}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} We need to pad the labels \PYGZsq{}manually\PYGZsq{}}
    \PYG{k}{for} \PYG{n}{label} \PYG{o+ow}{in} \PYG{n}{label\PYGZus{}dicts}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{:}
        \PYG{n}{labels} \PYG{o}{=} \PYG{n}{pad\PYGZus{}sequence}\PYG{p}{(}
            \PYG{p}{[}\PYG{n}{label\PYGZus{}dict}\PYG{p}{[}\PYG{n}{label}\PYG{p}{]} \PYG{k}{for} \PYG{n}{label\PYGZus{}dict} \PYG{o+ow}{in} \PYG{n}{label\PYGZus{}dicts}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{batch\PYGZus{}first}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
            \PYG{n}{padding\PYGZus{}value}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{,}
        \PYG{p}{)}

        \PYG{n}{batch}\PYG{p}{[}\PYG{n}{label}\PYG{p}{]} \PYG{o}{=} \PYG{n}{labels}
    \PYG{k}{return} \PYG{n}{batch}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
This function used the Huggingface default collation function to handle everything except the labels. The labels are padded with a batch\sphinxhyphen{}wise max length strategy and added to the batch.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{data\PYGZus{}loader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{)}
\PYG{n}{batch} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n+nb}{iter}\PYG{p}{(}\PYG{n}{data\PYGZus{}loader}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{batch}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Now the data is in the correct format for training.


\section{Loss function}
\label{\detokenize{Prerequisites:loss-function}}
\sphinxAtStartPar
As stated in the experimental design, we use a plain Mean\sphinxhyphen{}Squared\sphinxhyphen{}Error regression loss. Since we only want to consider the special tokens, we must select them before the actual computation. Therefore, we need the \sphinxcode{\sphinxupquote{input\_ids}} to figure out their position in the sequence.

\sphinxAtStartPar
To compute the loss for one single batch, we add the loss scores of all sentences of one text and take the average of all batch entries.
Due to computational constraints, transformer\sphinxhyphen{}based language models typically have a limit on the input size. So our inputs might have to be truncated to fit into the model. In this case, we discard the labels for the sentences left out and only consider the data that fits into the model.

\sphinxAtStartPar
The following listing contains a general implementation of the loss function:
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{torch} \PYG{k+kn}{import} \PYG{n}{nn}

\PYG{k}{def} \PYG{n+nf}{sentence\PYGZus{}ordering\PYGZus{}loss}\PYG{p}{(}\PYG{n}{batch\PYGZus{}logits}\PYG{p}{,} \PYG{n}{batch\PYGZus{}targets}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Since we have varying number of labels per instance, we need to compute the loss manually for each one.}
    \PYG{n}{loss\PYGZus{}fn} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{MSELoss}\PYG{p}{(}\PYG{n}{reduction}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sum}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{labels}\PYG{p}{,} \PYG{n}{logits}\PYG{p}{,} \PYG{n}{input\PYGZus{}ids} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}
        \PYG{n}{batch\PYGZus{}labels}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}
    \PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Firstly, we need to convert the sentence indices to regression targets.}
        \PYG{c+c1}{\PYGZsh{} Also we need to remove the padding entries (\PYGZhy{}100)}
        \PYG{n}{true\PYGZus{}labels} \PYG{o}{=} \PYG{n}{labels}\PYG{p}{[}\PYG{n}{labels} \PYG{o}{!=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{n}{targets} \PYG{o}{=} \PYG{n}{true\PYGZus{}labels}\PYG{o}{.}\PYG{n}{float}\PYG{p}{(}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Secondly, we need to get the logits from each target token in the input sequence}
        \PYG{n}{target\PYGZus{}logits} \PYG{o}{=} \PYG{n}{logits}\PYG{p}{[}\PYG{n}{input\PYGZus{}ids} \PYG{o}{==} \PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Sometimes, we will have less target\PYGZus{}logits than targets due to trunction of the input.}
        \PYG{c+c1}{\PYGZsh{} In this case, we just consider as many targets as we have logits}
        \PYG{k}{if} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{targets}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{targets} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{p}{:} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{} Finally we compute the loss for the current instance and add it to the batch loss}
        \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{+} \PYG{n}{loss\PYGZus{}fn}\PYG{p}{(}\PYG{n}{targets}\PYG{p}{,} \PYG{n}{target\PYGZus{}logits}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} The final loss is obtained by averaging over the number of instances per batch}
    \PYG{n}{loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{/} \PYG{n}{batch\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{loss}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\chapter{Huggingface Trainer}
\label{\detokenize{HuggingFaceTrainer:huggingface-trainer}}\label{\detokenize{HuggingFaceTrainer::doc}}
\sphinxAtStartPar
Since Huggingface proclaimed goal is to provide an environment to develop and train all sorts of language models, they also ship a solution for training models.
It is called the \sphinxcode{\sphinxupquote{Trainer}}, and is integrated into the \sphinxcode{\sphinxupquote{transformers}} library itself.
Of course, it is profoundly integrated into the Huggingface\sphinxhyphen{}ecosystem and can train most \sphinxcode{\sphinxupquote{transformers}} models out of the box.


\section{Classes}
\label{\detokenize{HuggingFaceTrainer:classes}}

\subsection{Trainer}
\label{\detokenize{HuggingFaceTrainer:trainer}}
\sphinxAtStartPar
Design\sphinxhyphen{}wise, the \sphinxcode{\sphinxupquote{Trainer}} is one single class that handles the training end\sphinxhyphen{}to\sphinxhyphen{}end.
Its configuration is outsourced to a \sphinxcode{\sphinxupquote{TrainingArguments}} class that stores all relevant parameters for training.
These arguments are passed to the \sphinxcode{\sphinxupquote{Trainer}}  alongside a model and a dataset during initialization.
Since Huggingface models compute the loss internally, the \sphinxcode{\sphinxupquote{Trainer}} passes the input data to the model, extracts the loss from the output, and does the backward step.
It also handles additional steps to monitor the training process, like saving checkpoints of the model or logging the loss and other validation metrics.
A significant advantage of using the \sphinxcode{\sphinxupquote{Trainer}} is its ability to do multi\sphinxhyphen{}device training without requiring the user to care about dispatching the models and data to multiple accelerators.
Also, it comes with an extension that allows more sophisticated tweaks, like training with 16bit\sphinxhyphen{}precision.


\subsubsection{Extending the \sphinxstyleliteralintitle{\sphinxupquote{Trainer}}}
\label{\detokenize{HuggingFaceTrainer:extending-the-trainer}}
\sphinxAtStartPar
There are two different options to customize certain aspects of the behavior of the \sphinxcode{\sphinxupquote{Trainer}}.
Additional read\sphinxhyphen{}only operations can be implemented with the callback API.
Callbacks are executed at specific events during the training (e.g., at the end of an epoch).
They have access to many different things like the model or the current state of the \sphinxcode{\sphinxupquote{Trainer}}.
However, since they can not manipulate their environment, their scope is limited to logging, saving certain parts of a model, or stopping the training if a specific condition is met.

\sphinxAtStartPar
If further changes to the \sphinxcode{\sphinxupquote{Trainer}} are required, the recommended way is to subclass it and create a custom via inheritance.
Internally, the \sphinxcode{\sphinxupquote{Trainer}} structures the training into different sub\sphinxhyphen{}steps and exposes them via a method for each of them.
By overwriting these methods, it is possible to change certain parts of the logic without rewriting the rest of the code that would not be changed anyway.
The most important methods to modify the \sphinxcode{\sphinxupquote{train\sphinxhyphen{}test\sphinxhyphen{}val}}\sphinxhyphen{}loop itself are the \sphinxcode{\sphinxupquote{<train/test/val>\sphinxhyphen{}step}} methods and the \sphinxcode{\sphinxupquote{compute\_loss}} method.
These methods implement the essential individual training steps and are called within methods that implement higher\sphinxhyphen{}order operations like the \sphinxcode{\sphinxupquote{.train}}\sphinxhyphen{}method, which handles the complete training loop.


\subsubsection{Logging}
\label{\detokenize{HuggingFaceTrainer:logging}}
\sphinxAtStartPar
If a \sphinxcode{\sphinxupquote{logdir}}\sphinxhyphen{}argument is specified in the \sphinxcode{\sphinxupquote{TrainingArguments}}\sphinxhyphen{}object, logging is enabled automatically.
By default, the \sphinxcode{\sphinxupquote{Trainer}} outputs the logs in two formats: Stdout and disk, using a Tensorboard\sphinxhyphen{}compliant format.
Additional logging can be implemented by either overwriting the \sphinxcode{\sphinxupquote{.log}}\sphinxhyphen{}method of Trainer or by using callbacks.
There are already some pre\sphinxhyphen{}built callbacks available. For example, to log the progress to Weights and Biases or a CSV table.


\subsubsection{Custom metrics}
\label{\detokenize{HuggingFaceTrainer:custom-metrics}}
\sphinxAtStartPar
Since the \sphinxcode{\sphinxupquote{Trainer}} is agnostic towards the task it is used with; it only logs the loss by default.
Additionally, metrics can be added by equipping the \sphinxcode{\sphinxupquote{Trainer}} with a function that computes them during initialization.
This function receives an \sphinxcode{\sphinxupquote{EvalPrediction}} object.
This object holds all predictions of the model and the valid labels.
The output of the custom metric function ought to be a dictionary containing the name of the metric as key and the score as value.


\subsection{Training Arguments}
\label{\detokenize{HuggingFaceTrainer:training-arguments}}
\sphinxAtStartPar
As stated above, a \sphinxcode{\sphinxupquote{TrainingArguments}} object stores all hyperparameters of the training.
Storing all parameters in a single object is helpful to ensuring reproducibility since this object can easily be serialized and saved to disk as JSON using its \sphinxcode{\sphinxupquote{.to\_json\_string}}\sphinxhyphen{}method.
Also, the \sphinxcode{\sphinxupquote{TrainingArguments}} class works seamlessly with the built\sphinxhyphen{}in CLI\sphinxhyphen{}parser class of \sphinxcode{\sphinxupquote{transformers}}, which helps make the configuration of an experiment available through a command\sphinxhyphen{}line interface.


\subsection{HfArgumentParser}
\label{\detokenize{HuggingFaceTrainer:hfargumentparser}}
\sphinxAtStartPar
Most experiments are repeated several times with different parameters. These parameters have to be changed directly in the source code by default, which is not ideal for several reasons.
Most importantly, it can harm reproducibility since tracking changes in the source code requires either version control and a strict commit regime or keeping several versions of the same file with different parameters. Also, it can be tedious the search for the location of all parameters across the code manually.
Making the hyperparameters adjustable via a command\sphinxhyphen{}line interface decouples their configuration from the rest of the code, alleviating this issue.
While there are arguably a lot of different solutions to this problem with many strategies that are more sophisticated than a command\sphinxhyphen{}line interface, it is an excellent first step.
Moreover, it has the advantage of being platform\sphinxhyphen{}independent without depending on additional dependencies.

\sphinxAtStartPar
Huggingface provides a built\sphinxhyphen{}in solution for building these interfaces called \sphinxcode{\sphinxupquote{HfArgumentParser}}.
It is an extended version of Pythons \sphinxcode{\sphinxupquote{argsparse}} parser and creates command\sphinxhyphen{}line interfaces by parsing the fields of \sphinxcode{\sphinxupquote{dataclasses}} and exposing them as command\sphinxhyphen{}line arguments.
Since most configuration classes of the \sphinxcode{\sphinxupquote{transformers}} library are \sphinxcode{\sphinxupquote{dataclasses,}} the \sphinxcode{\sphinxupquote{HfArgumentParser}} can flexibly control nearly every aspect of the training.
Further extending the arguments can be easily done by creating custom \sphinxcode{\sphinxupquote{dataclasses}} that hold additional parameters.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{dataclasses} \PYG{k+kn}{import} \PYG{n}{dataclass}\PYG{p}{,} \PYG{n}{field}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{HfArgumentParser}

\PYG{n+nd}{@dataclass}
\PYG{k}{class} \PYG{n+nc}{TrainArgs}\PYG{p}{:}
    \PYG{n}{batch\PYGZus{}size}\PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{n}{field}\PYG{p}{(}
        \PYG{n}{default} \PYG{o}{=} \PYG{l+m+mi}{8}\PYG{p}{,}
        \PYG{n}{metadata} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{help}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Number of batched for training.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}

\PYG{n}{parser} \PYG{o}{=} \PYG{n}{HfArgumentParser}\PYG{p}{(}\PYG{n}{TrainArgs}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{print\PYGZus{}help}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{train\PYGZus{}args} \PYG{o}{=} \PYG{n}{parser}\PYG{o}{.}\PYG{n}{parse\PYGZus{}args\PYGZus{}into\PYGZus{}dataclasses}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}batch\PYGZus{}size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{4}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{train\PYGZus{}args}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
usage: ipykernel\PYGZus{}launcher.py [\PYGZhy{}h] [\PYGZhy{}\PYGZhy{}batch\PYGZus{}size BATCH\PYGZus{}SIZE]

optional arguments:
  \PYGZhy{}h, \PYGZhy{}\PYGZhy{}help            show this help message and exit
  \PYGZhy{}\PYGZhy{}batch\PYGZus{}size BATCH\PYGZus{}SIZE
                        Number of batched for training. (default: 8)
(TrainArgs(batch\PYGZus{}size=4),)
\end{sphinxVerbatim}
\end{sphinxVerbatimOutput}


\section{Implementation}
\label{\detokenize{HuggingFaceTrainer:implementation}}

\subsection{Loss function}
\label{\detokenize{HuggingFaceTrainer:loss-function}}
\sphinxAtStartPar
For the sentence ordering task, we employ a language model with a standard token\sphinxhyphen{}classification head.
However, since the task requires a custom loss function, we have to discard the loss of the model and use our custom loss function.
To do so, we follow the guidelines and create our custom version of the \sphinxcode{\sphinxupquote{Trainer}} with a custom \sphinxcode{\sphinxupquote{.compute\_loss}} function.
The implementation  is straightforward.
The \sphinxcode{\sphinxupquote{.compute\_loss}} method receives a reference to the model and the input data as inputs, which is especially helpful in cases like ours where we need to check the \sphinxcode{\sphinxupquote{input\_ids}} to compute the loss.
In addition, to our custom loss function, we also add another attribute to the \sphinxcode{\sphinxupquote{Trainer}}, which holds the id of the target sentence token in order to find the correct tokens in the input sequence.
We leave the rest of the \sphinxcode{\sphinxupquote{Trainer}} untouched.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{SentenceOrderingTrainer}\PYG{p}{(}\PYG{n}{Trainer}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}token\PYGZus{}id} \PYG{o}{=} \PYG{n}{kwargs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target\PYGZus{}token\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{model}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{,} \PYG{n}{return\PYGZus{}outputs}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}

        \PYG{c+c1}{\PYGZsh{} Get sentence indices}
        \PYG{n}{batch\PYGZus{}labels} \PYG{o}{=} \PYG{n}{inputs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Get logits from model}
        \PYG{n}{outputs} \PYG{o}{=} \PYG{n}{model}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{inputs}\PYG{p}{)}
        \PYG{n}{batch\PYGZus{}logits} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{} Get logits for all cls tokens}
        \PYG{n}{batch\PYGZus{}input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

        \PYG{n}{loss\PYGZus{}fn} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{MSELoss}\PYG{p}{(}\PYG{n}{reduction}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sum}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        
        \PYG{k}{for} \PYG{n}{labels}\PYG{p}{,} \PYG{n}{logits}\PYG{p}{,} \PYG{n}{input\PYGZus{}ids} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}
            \PYG{n}{batch\PYGZus{}labels}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}
        \PYG{p}{)}\PYG{p}{:}

            \PYG{n}{true\PYGZus{}labels} \PYG{o}{=} \PYG{n}{labels}\PYG{p}{[}\PYG{n}{labels} \PYG{o}{!=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{targets} \PYG{o}{=} \PYG{n}{true\PYGZus{}labels}\PYG{o}{.}\PYG{n}{float}\PYG{p}{(}\PYG{p}{)}

            \PYG{n}{target\PYGZus{}logits} \PYG{o}{=} \PYG{n}{logits}\PYG{p}{[}\PYG{n}{input\PYGZus{}ids} \PYG{o}{==} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

            \PYG{k}{if} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{targets}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{targets} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{p}{:} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}

            \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{+} \PYG{n}{loss\PYGZus{}fn}\PYG{p}{(}\PYG{n}{targets}\PYG{p}{,} \PYG{n}{target\PYGZus{}logits}\PYG{p}{)}

        \PYG{n}{loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{/} \PYG{n}{batch\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

        \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loss}
        \PYG{k}{return} \PYG{p}{(}\PYG{n}{loss}\PYG{p}{,} \PYG{n}{outputs}\PYG{p}{)} \PYG{k}{if} \PYG{n}{return\PYGZus{}outputs} \PYG{k}{else} \PYG{n}{loss}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\subsection{Metrics}
\label{\detokenize{HuggingFaceTrainer:metrics}}
\sphinxAtStartPar
To compute custom metrics during validation, we need to create a function.
The function computes all metrics at once.
In contrast to the \sphinxcode{\sphinxupquote{.compute\_loss}}\sphinxhyphen{}method, which receives the input and the model, it receives an \sphinxcode{\sphinxupquote{EvalPrediction}} object as input.
An \sphinxcode{\sphinxupquote{EvalPrediction}} contains the model’s outputs and the labels from the dataset.
However, similar to the loss function, computing the metrics requires access to the input data to retrieve the indices of the target tokens.
To control the content of an \sphinxcode{\sphinxupquote{EvalPrediction}} object, we can use the \sphinxcode{\sphinxupquote{label\_names}} parameter of the \sphinxcode{\sphinxupquote{TrainingArguments}}.
With this argument, we can specify additional fields that are copied from the input batches to the \sphinxcode{\sphinxupquote{EvalPrediction}} objects.
This way, we can incorporate the labels and the \sphinxcode{\sphinxupquote{input\_ids}} of tokens in the \sphinxcode{\sphinxupquote{EvalPrediction}} object.

\sphinxAtStartPar
A minor but valuable trait of the \sphinxcode{\sphinxupquote{EvalPrediction}} objects is that their content gets converted from \sphinxcode{\sphinxupquote{torch.tensors}} to \sphinxcode{\sphinxupquote{np.arrays}}.
Because most validation metrics from other libraries use NumPy, we do not need to convert the data manually.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{training\PYGZus{}args} \PYG{o}{=} \PYG{n}{TrainingArguments}\PYG{p}{(}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}
    \PYG{n}{label\PYGZus{}names}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{,}
\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}compute\PYGZus{}metrics\PYGZus{}func}\PYG{p}{(}\PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Callable}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}ranking\PYGZus{}func}\PYG{p}{(}\PYG{n}{eval\PYGZus{}prediction}\PYG{p}{:} \PYG{n}{EvalPrediction}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Dict}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n+nb}{float}\PYG{p}{]}\PYG{p}{:}
        \PYG{n}{batch\PYGZus{}sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{eval\PYGZus{}prediction}\PYG{o}{.}\PYG{n}{label\PYGZus{}ids}
        \PYG{n}{batch\PYGZus{}logits} \PYG{o}{=} \PYG{n}{eval\PYGZus{}prediction}\PYG{o}{.}\PYG{n}{predictions}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}

        \PYG{n}{metrics} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{logits} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}
            \PYG{n}{batch\PYGZus{}sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}
        \PYG{p}{)}\PYG{p}{:}
            \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{input\PYGZus{}ids}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{logits} \PYG{o}{=} \PYG{n}{logits}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

            \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{[}\PYG{n}{sent\PYGZus{}idx} \PYG{o}{!=} \PYG{l+m+mi}{100}\PYG{p}{]}
            \PYG{n}{target\PYGZus{}logits} \PYG{o}{=} \PYG{n}{logits}\PYG{p}{[}\PYG{n}{input\PYGZus{}ids} \PYG{o}{==} \PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{]}
            \PYG{k}{if} \PYG{n}{sent\PYGZus{}idx}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{:}
                \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{[}\PYG{p}{:} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}
            \PYG{n}{predicted\PYGZus{}idx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{target\PYGZus{}logits}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{tau}\PYG{p}{,} \PYG{n}{pvalue} \PYG{o}{=} \PYG{n}{kendalltau}\PYG{p}{(}\PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}idx}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{kendalls\PYGZus{}tau}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{tau}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{acc}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}idx}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{std\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{metric}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)} \PYG{k}{for} \PYG{n}{metric}\PYG{p}{,} \PYG{n}{scores} \PYG{o+ow}{in} \PYG{n}{metrics}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
        \PYG{k}{return} \PYG{n}{metrics}

    \PYG{k}{return} \PYG{n}{compute\PYGZus{}ranking\PYGZus{}func}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\subsection{Custom CLI arguments}
\label{\detokenize{HuggingFaceTrainer:custom-cli-arguments}}
\sphinxAtStartPar
We use the \sphinxcode{\sphinxupquote{HfArgumentParser}} to make the parameters of our experiment adjustable via the command line.
In addition to the \sphinxcode{\sphinxupquote{TrainingsArguments}}, we also want to control the type of the model.
Custom parameters can easily be added by creating a custom \sphinxcode{\sphinxupquote{dataclass}}.
We create a \sphinxcode{\sphinxupquote{ModelArgs}} class that has two fields. One to specify the name or path to the model and a second parameter to specify the path where the final model is saved after training.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{dataclasses} \PYG{k+kn}{import} \PYG{n}{dataclass}\PYG{p}{,} \PYG{n}{field}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{TrainingArguments}\PYG{p}{,} \PYG{n}{HfArgumentParser}

\PYG{n+nd}{@dataclass}
\PYG{k}{class} \PYG{n+nc}{ModelArgs}\PYG{p}{:}
    \PYG{n}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{n}{field}\PYG{p}{(}
        \PYG{n}{default}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{metadata}\PYG{o}{=}\PYG{p}{\PYGZob{}}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{help}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Path to pretrained model or model or its name to load it from Huggingface Hub.}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{\PYGZcb{}}\PYG{p}{,}
    \PYG{p}{)}

    \PYG{n}{final\PYGZus{}checkpoint\PYGZus{}path}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{n}{field}\PYG{p}{(}
        \PYG{n}{default}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{metadata}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{help}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Path to save the final model.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{\PYGZcb{}}
    \PYG{p}{)}

\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{n}{args\PYGZus{}parser} \PYG{o}{=} \PYG{n}{HfArgumentParser}\PYG{p}{(}\PYG{p}{(}\PYG{n}{ModelArgs}\PYG{p}{,} \PYG{n}{TrainingArguments}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{model\PYGZus{}args}\PYG{p}{,} \PYG{n}{training\PYGZus{}args} \PYG{o}{=} \PYG{n}{args\PYGZus{}parser}\PYG{o}{.}\PYG{n}{parse\PYGZus{}args\PYGZus{}into\PYGZus{}dataclasses}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\section{Complete code}
\label{\detokenize{HuggingFaceTrainer:complete-code}}
\sphinxAtStartPar
After moving our custom code for the \sphinxcode{\sphinxupquote{Trainer}} and the metric function to an external module, the rest of the code to implement the experiment looks like Listing (TODO).
There are only two steps left to complete the script.
Firstly, we must ensure that our data always contains the correct special tokens for ordering the sentences.
Since we prepared the data beforehand by adding BERTs special \sphinxcode{\sphinxupquote{{[}SEP{]}}}\sphinxhyphen{}token as a prefix to each sentence, we have to ensure that these tokens are replaced if necessary using the  \sphinxcode{\sphinxupquote{replace\_cls\_token}} function.

\sphinxAtStartPar
Lastly, we want to control the randomness in our experiment to make it consistently reproducible.
The \sphinxcode{\sphinxupquote{transformers}} library comes with a helpful function called \sphinxcode{\sphinxupquote{set\_seed}}, which controls the state of all random number generators of Python itself, NumPy, and PyTorch at once.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{TrainingArguments}\PYG{p}{,} \PYG{n}{HfArgumentParser}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoModelForTokenClassification}\PYG{p}{,} \PYG{n}{AutoConfig}\PYG{p}{,} \PYG{n}{AutoTokenizer}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{set\PYGZus{}seed}
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}

\PYG{k+kn}{from} \PYG{n+nn}{model} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{SentenceOrderingTrainer}\PYG{p}{,}
    \PYG{n}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{,}
    \PYG{n}{make\PYGZus{}compute\PYGZus{}metrics\PYGZus{}func}\PYG{p}{,}
    \PYG{n}{ModelArgs}\PYG{p}{,}
    \PYG{n}{make\PYGZus{}tokenization\PYGZus{}func}\PYG{p}{,}
\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}

    \PYG{n}{args\PYGZus{}parser} \PYG{o}{=} \PYG{n}{HfArgumentParser}\PYG{p}{(}\PYG{p}{(}\PYG{n}{ModelArgs}\PYG{p}{,} \PYG{n}{TrainingArguments}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{model\PYGZus{}args}\PYG{p}{,} \PYG{n}{training\PYGZus{}args} \PYG{o}{=} \PYG{n}{args\PYGZus{}parser}\PYG{o}{.}\PYG{n}{parse\PYGZus{}args\PYGZus{}into\PYGZus{}dataclasses}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Add fixed args}
    \PYG{n}{training\PYGZus{}args}\PYG{o}{.}\PYG{n}{label\PYGZus{}names} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

    \PYG{n}{set\PYGZus{}seed}\PYG{p}{(}\PYG{n}{training\PYGZus{}args}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{)}

    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}\PYG{p}{(}
        \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{/home/keller/Uni/trf\PYGZus{}training\PYGZus{}tut/scripts/data/rocstories}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{p}{)}

    \PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{model\PYGZus{}args}\PYG{o}{.}\PYG{n}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{p}{)}

    \PYG{k}{if} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token} \PYG{o}{!=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[CLS]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}
            \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Model does not a have a [CLS] token. Updating the data with token }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ ...}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{replace\PYGZus{}cls\PYGZus{}token}\PYG{p}{(}\PYG{n}{entry}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{texts} \PYG{o}{=} \PYG{n}{entry}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
            \PYG{n}{replaced\PYGZus{}texts} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
            \PYG{k}{for} \PYG{n}{text} \PYG{o+ow}{in} \PYG{n}{texts}\PYG{p}{:}
                \PYG{n}{replaced\PYGZus{}texts}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{text}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[CLS]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{entry}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{replaced\PYGZus{}texts}
            \PYG{k}{return} \PYG{n}{entry}

        \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{replace\PYGZus{}cls\PYGZus{}token}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{n}{model\PYGZus{}config} \PYG{o}{=} \PYG{n}{AutoConfig}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}
        \PYG{n}{model\PYGZus{}args}\PYG{o}{.}\PYG{n}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{p}{,} \PYG{n}{num\PYGZus{}labels}\PYG{o}{=}\PYG{l+m+mi}{1}
    \PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{AutoModelForTokenClassification}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}
        \PYG{n}{model\PYGZus{}args}\PYG{o}{.}\PYG{n}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{p}{,} \PYG{n}{config}\PYG{o}{=}\PYG{n}{model\PYGZus{}config}
    \PYG{p}{)}

    \PYG{n}{tokenization} \PYG{o}{=} \PYG{n}{make\PYGZus{}tokenization\PYGZus{}func}\PYG{p}{(}
        \PYG{n}{tokenizer}\PYG{o}{=}\PYG{n}{tokenizer}\PYG{p}{,}
        \PYG{n}{text\PYGZus{}column}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{padding}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}length}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{truncation}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
        \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
    \PYG{p}{)}
    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{tokenization}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{rename\PYGZus{}column}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{so\PYGZus{}targets}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{set\PYGZus{}format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{torch}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{metrics\PYGZus{}func} \PYG{o}{=} \PYG{n}{make\PYGZus{}compute\PYGZus{}metrics\PYGZus{}func}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token\PYGZus{}id}\PYG{p}{)}

    \PYG{n}{trainer} \PYG{o}{=} \PYG{n}{SentenceOrderingTrainer}\PYG{p}{(}
        \PYG{n}{model}\PYG{o}{=}\PYG{n}{model}\PYG{p}{,}
        \PYG{n}{args}\PYG{o}{=}\PYG{n}{training\PYGZus{}args}\PYG{p}{,}
        \PYG{n}{train\PYGZus{}dataset}\PYG{o}{=}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{eval\PYGZus{}dataset}\PYG{o}{=}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{o}{=}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token\PYGZus{}id}\PYG{p}{,}
        \PYG{n}{data\PYGZus{}collator}\PYG{o}{=}\PYG{n}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{,}
        \PYG{n}{compute\PYGZus{}metrics}\PYG{o}{=}\PYG{n}{metrics\PYGZus{}func}\PYG{p}{,}
    \PYG{p}{)}

    \PYG{n}{trainer}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{p}{)}

    \PYG{n}{trainer}\PYG{o}{.}\PYG{n}{save\PYGZus{}model}\PYG{p}{(}\PYG{n}{model\PYGZus{}args}\PYG{o}{.}\PYG{n}{final\PYGZus{}checkpoint\PYGZus{}path}\PYG{p}{)}

    \PYG{n}{test\PYGZus{}results} \PYG{o}{=} \PYG{n}{trainer}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{eval\PYGZus{}dataset}\PYG{o}{=}\PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test\PYGZus{}results\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model\PYGZus{}args}\PYG{o}{.}\PYG{n}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{w}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
        \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{test\PYGZus{}results}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{test\PYGZus{}results}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\section{Conclusion}
\label{\detokenize{HuggingFaceTrainer:conclusion}}
\sphinxAtStartPar
The Huggingface \sphinxcode{\sphinxupquote{Trainer}} is a perfect choice when training models on standard tasks that are well supported.
In these cases, it enables to train models effortlessly without requiring to write much code.
In the best case, when the dataset is already available as Huggingface \sphinxcode{\sphinxupquote{Dataset}}, it comes down to a few lines of code to train the model without having to dive deep into any internals along the way.

\sphinxAtStartPar
Also, it has many useful out\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}box features, like gradient clipping, half\sphinxhyphen{}precision training, support of distributed training, or logging to Tensorboard, which make it feasible for training large models on large datasets.

\sphinxAtStartPar
Nonetheless, there are a few issues if one wants to leave the carved\sphinxhyphen{}out paths.
Like the rest of Huggingface’s software, the \sphinxcode{\sphinxupquote{transformers}} library is relatively new and evolves at great speed.
Huggingface’s self\sphinxhyphen{}proclaimed goal is to provide an easy\sphinxhyphen{}to\sphinxhyphen{}use all\sphinxhyphen{}in\sphinxhyphen{}one infrastructure for NLP with language models and incorporate new models, architectures, and developments as quickly as possible.
On this path, sacrifices have to be made.

\sphinxAtStartPar
One area that seems to suffer from the speedy development is documentation.
It is sufficient and provides all essential information, but it can sometimes be very sparse in detail.
Often, there are multiple options to choose from when customizing something.
For example, the default optimizer can be exchanged during the initialization of the \sphinxcode{\sphinxupquote{Trainer}}, by simply passing another one to it. Or by overwriting the \sphinxcode{\sphinxupquote{.create\_optimizer}}\sphinxhyphen{}method.
In cases like this one, the documentation lacks hints to decide which way to go.

\sphinxAtStartPar
Other times the documentation does not paint the whole picture of the behavior of the described object.
In these cases, it might become necessary to take a look into the source code itself.

\sphinxAtStartPar
By looking into the source code of \sphinxcode{\sphinxupquote{Trainer}}, it becomes evident that it could use some refactoring.
Especially, its high\sphinxhyphen{}level methods, like the \sphinxcode{\sphinxupquote{.train}}\sphinxhyphen{}method, are very complex since they do much heavy lifting, for example, dispatching the training to multiple devices.
While the preferred way to customize the training is to subclass the \sphinxcode{\sphinxupquote{Trainer}} and overwrite methods, this is only feasible for the low\sphinxhyphen{}level methods that define single steps. Even tiny adjustments to the high\sphinxhyphen{}level methods can require copying code or rewriting certain parts.


\chapter{PyTorch Lightning}
\label{\detokenize{PyTorchLightning:pytorch-lightning}}\label{\detokenize{PyTorchLightning::doc}}
\sphinxAtStartPar
In contrast to the Huggingface \sphinxcode{\sphinxupquote{Trainer}}, which handles the complete training itself, PyTorch Lightning (Falcon {[}\hyperlink{cite.Bibliography:id3}{2019}{]}) takes a different approach.
It not only aims at handling the training but also at structuring the creation of a model too.
Its main goal is not to hide complexity from the user but to provide a well\sphinxhyphen{}structured API for building neural networks of all kinds.
The most striking aspect of this is that in PyTorch Lightning’s philosophy, a model and its inference\sphinxhyphen{}, training and prediction logic are not separate things that can be exchanged independently.
Instead, it binds all these parts directly to the model itself.
In doing so, PyTorch\sphinxhyphen{}Lightning does not make any assumptions on the nature of the model or the training itself. Thus it allows covering many tasks and domains with maximum flexibility.

\sphinxAtStartPar
However, this approach comes at the cost that the user again must implement many things manually.
Naturally, this approach is keener to researchers who implement and test custom models, while practitioners who only want to employ pre\sphinxhyphen{}built models must deal with some implementational overhead.
PyTorch Lightning’s steep learning curve compounds this issue.
However, there is exhaustive documentation with many tutorials (as texts and videos), best practices, and user guides on building various models across different domains.
Also, if an experiment is implemented in PyTorch Lightning, there are a lot of helpful tweaks and techniques to improve or speed up the training.
So that it can be worthwhile even when using pre\sphinxhyphen{}built models.
These facilitation features include tweaks like training with half\sphinxhyphen{}precision, automatic tuning of the learning rate, and integrations into hyperparameter tuning frameworks or creating command\sphinxhyphen{}line interfaces to control the parameters.
In addition to that, there is support for different computational backends that help to dispatch the training on multiple accelerators like GPUs and TPUs.
If these features are not enough, there is a growing ecosystem of third\sphinxhyphen{}party extensions, widening the scope and functionality of the framework.


\section{Classes}
\label{\detokenize{PyTorchLightning:classes}}
\sphinxAtStartPar
From a technical point of view, Pytorch Lightning provides an API composed of three different main classes, dividing the training process into a sequence of single atomic steps.
These classes implement the model, the logic for storing and processing the training data, and the training process itself.


\subsection{\sphinxstyleliteralintitle{\sphinxupquote{LightningModule}}}
\label{\detokenize{PyTorchLightning:lightningmodule}}
\sphinxAtStartPar
A subclass of a \sphinxcode{\sphinxupquote{LightningModule}} implements the model.
A \sphinxcode{\sphinxupquote{LightningModule}} is an extended version of PyTorch’s \sphinxcode{\sphinxupquote{nn.Module}} class.
\sphinxcode{\sphinxupquote{nn.Modules}} are the basic building blocks of neural networks in PyTorch. In essence, they store a set of parameters, for example, weights of a single layer alongside with \sphinxcode{\sphinxupquote{.forward}}\sphinxhyphen{}method that defines the computational logic when data flows through the module. They are designed to work recursively. One module can be composed of several submodules so that each building block of a neural network, starting from single layers up to a complete network, can be implemented in this one class.
The following listing shows an exemplary implementation of a simple linear layer as \sphinxcode{\sphinxupquote{nn.Module}}.
By chaining multiple instances of the dense layer in a \sphinxcode{\sphinxupquote{nn. Sequential}} class, it is possible to create a simple feed\sphinxhyphen{}forward network.
This network is again a subclass of the \sphinxcode{\sphinxupquote{nn.Module}} class.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{torch} 
\PYG{k+kn}{from} \PYG{n+nn}{torch} \PYG{k+kn}{import} \PYG{n}{nn}

\PYG{k}{class} \PYG{n+nc}{DenseLayer}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Fully connected linear layer.\PYGZdq{}\PYGZdq{}\PYGZdq{}}
    
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{in\PYGZus{}shape}\PYG{p}{,} \PYG{n}{out\PYGZus{}shape}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weights} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Parameter}\PYG{p}{(}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{in\PYGZus{}shape}\PYG{p}{,} \PYG{n}{out\PYGZus{}shape}\PYG{p}{)}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        
    \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{matmul}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{weights}\PYG{p}{)}

\PYG{n}{network} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Sequential}\PYG{p}{(}
    \PYG{n}{DenseLayer}\PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{l+m+mi}{16}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{nn}\PYG{o}{.}\PYG{n}{ReLU}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{DenseLayer}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{nn}\PYG{o}{.}\PYG{n}{ReLU}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{DenseLayer}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{p}{)}

\PYG{n}{inputs} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{512}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Batchsize 8}
\PYG{n}{outputs} \PYG{o}{=} \PYG{n}{network}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{outputs}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{issubclass}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Sequential}\PYG{p}{,} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
torch.Size([8, 2])
True
\end{sphinxVerbatim}
\end{sphinxVerbatimOutput}

\sphinxAtStartPar
A \sphinxcode{\sphinxupquote{LightningModule}} is intended to replace the outmost \sphinxcode{\sphinxupquote{nn.Module}} instance of a model, which holds the complete network.
It extends the \sphinxcode{\sphinxupquote{nn.Module}} class with new methods, designed to structure not only the logic of a single forward pass but also other steps like a complete train\sphinxhyphen{}, test\sphinxhyphen{} or validation\sphinxhyphen{}steps.
With this extension, it becomes possible to define a single forward step through the network and how the models should be trained and tested as well.
In essence, it provides a way to incorporate the training loop into the model itself.
This strategy has one massive advantage over the standard PyTorch practice of writing an external function that implements the training loop.
It helps to make the model self\sphinxhyphen{}contained, meaning that it holds all necessary logic itself. This property alleviates sharing models since only one class carries all information to train and test the model.


\subsubsection{Training, Testing, Validation}
\label{\detokenize{PyTorchLightning:training-testing-validation}}
\sphinxAtStartPar
The methods for training, testing and prediction are called \sphinxcode{\sphinxupquote{.train\_step}},  \sphinxcode{\sphinxupquote{.validation\_step}} and \sphinxcode{\sphinxupquote{.test\_step}} respectively.
They all define how a single batch of data should be handled for these steps.
Typically the \sphinxcode{\sphinxupquote{.train\_step}}\sphinxhyphen{}method computes the loss score, and the other two methods compute other validation metrics.
Design\sphinxhyphen{}wise, only the \sphinxcode{\sphinxupquote{.train\_step}}\sphinxhyphen{}method is required to return the loss averaged loss score for the current batch.
The test\sphinxhyphen{} and validation methods are not required to return anything. Instead, they can use the built\sphinxhyphen{}in logging capabilities of the \sphinxcode{\sphinxupquote{LightningModule}}.
Similar to the \sphinxcode{\sphinxupquote{.train\_step}}\sphinxhyphen{}method, they should return their scores averaged over the complete batch too.


\subsubsection{Model Hyperparameters and checkpoints}
\label{\detokenize{PyTorchLightning:model-hyperparameters-and-checkpoints}}
\sphinxAtStartPar
Much effort has been put into organizing the hyperparameters of an experiment.
Like the train and test routines, PyTorch Lightning binds all hyperparameters that control the model directly to the object.
This strategy ensures that saved models also contain the combination of parameters used for training.
There are two ways to define the hyperparameters of a model.
By default, each argument in the signature of the model’s constructor is regarded as a hyperparameter.
By calling a \sphinxcode{\sphinxupquote{.save\_hyperparameters}}\sphinxhyphen{}method in the constructor, these arguments get serialized into a \sphinxcode{\sphinxupquote{.hparams}}\sphinxhyphen{}attribute.
The \sphinxcode{\sphinxupquote{.hparams}}\sphinxhyphen{}attribute is saved as a \sphinxcode{\sphinxupquote{YAML}}\sphinxhyphen{}file for each checkpoint of the model, making it easy to see which parameters were used without loading the whole model.
If the constructor contains non\sphinxhyphen{}hyperparameters arguments, these can be excluded from serialization using the saving method’s \sphinxcode{\sphinxupquote{ignore}} flag.

\sphinxAtStartPar
Another more explicit way of defining the parameters of a model is to store them all in a dictionary into the constructor and to pass this dictionary to the \sphinxcode{\sphinxupquote{\sphinxhyphen{}save\_hyperparameters}}\sphinxhyphen{}method.
This strategy is suitable in cases where many arguments of the constructor are non\sphinxhyphen{}hyperparameters.


\subsubsection{Logging}
\label{\detokenize{PyTorchLightning:logging}}
\sphinxAtStartPar
Logging in PyTorch\sphinxhyphen{}Lightning is a two\sphinxhyphen{}stage procedure.
Inside the \sphinxcode{\sphinxupquote{LightningModule}}, various metrics can be logged at different steps while training using the \sphinxcode{\sphinxupquote{.log}}\sphinxhyphen{} or \sphinxcode{\sphinxupquote{log\_dict}}\sphinxhyphen{}methods.
The \sphinxcode{\sphinxupquote{.log}}\sphinxhyphen{}methods can log a single score, while the \sphinxcode{\sphinxupquote{log\sphinxhyphen{}dict}}\sphinxhyphen{}method can log multiple scores stored in a dictionary (with names as keys and the score as values).
These logs are extracted by \sphinxcode{\sphinxupquote{Trainer}} and written out in various formats (see Trainer section for further details.)
One benefit of using an autonomous logging function is that it gives flexibility to the user in deciding when to log which metrics, for example, making it possible to log something only when a condition applies.


\subsection{\sphinxstyleliteralintitle{\sphinxupquote{LighningDataModule}}}
\label{\detokenize{PyTorchLightning:lighningdatamodule}}
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{PyTorchLightning}} also comes with a custom solution to bundle data\sphinxhyphen{}related operations into a single object.
It is called \sphinxcode{\sphinxupquote{LighningDataModule}} and should contain the code to load and prepare the data for training and testing.
A class derived from \sphinxcode{\sphinxupquote{LighntningDataModule}} must implement four required methods.
The \sphinxcode{\sphinxupquote{.prepare\_data}}\sphinxhyphen{}method should implement all steps required to load the data and convert it into a correct representation for the model.
To return the splits for training, testing and evaluation, there are \sphinxcode{\sphinxupquote{.train|.test|.val\_loader}}\sphinxhyphen{}methods. Each of them has to return a \sphinxcode{\sphinxupquote{DataLoader}} object.
Like the \sphinxcode{\sphinxupquote{LightningModule}}, its data counterpart has the advantage of holding all code to load and prepare the data, which alleviates distribution and publication.
Another key feature of the \sphinxcode{\sphinxupquote{LightningDataModule}} is its ability to adapt to distributed environments.
While the \sphinxcode{\sphinxupquote{.prepare\_data}}\sphinxhyphen{}method is called once at the beginning of the training, there are also additional \sphinxcode{\sphinxupquote{.setup}}\sphinxhyphen{} and \sphinxcode{\sphinxupquote{teardown}}\sphinxhyphen{}methods.
These methods can define operations pre\sphinxhyphen{} or post\sphinxhyphen{}training data\sphinxhyphen{}preparation steps that must be performed independently on each accelerator.


\subsection{\sphinxstyleliteralintitle{\sphinxupquote{Trainer}}}
\label{\detokenize{PyTorchLightning:trainer}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{Trainer}} object handles the actual training.
It receives the model and data (wrapped in Lightning modules) alongside all training\sphinxhyphen{}specific hyperparameters, like the number of epochs, the devices to train on, or a list of loggers to log the progress.

\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{Trainer}} exposes four high\sphinxhyphen{}level methods to the user. Each of them triggers either the training, the validation, the prediction of unseen instances, and the hyperparameter\sphinxhyphen{}tuning.
Like the \sphinxcode{\sphinxupquote{LightningModule}}, an instance of the \sphinxcode{\sphinxupquote{Trainer}} is initialized with all hyperparameters relevant to the training, like the batch size or a number of epochs.

\sphinxAtStartPar
The different stages of the training (training and testing)


\subsubsection{Extending the \sphinxstyleliteralintitle{\sphinxupquote{Trainer}}}
\label{\detokenize{PyTorchLightning:extending-the-trainer}}
\sphinxAtStartPar
In contrast to the \sphinxcode{\sphinxupquote{LightningModule}} and \sphinxcode{\sphinxupquote{LightningDataModule}} the \sphinxcode{\sphinxupquote{Trainer}} itself is not intended to be customized in any way. Since their respective objects contain all model or data\sphinxhyphen{}related code, the \sphinxcode{\sphinxupquote{Trainer}} is better kept untouched.
Instead, if necessary, the functions of the \sphinxcode{\sphinxupquote{Trainer}} can be extended with callbacks and plugins. Both of them can add custom operations to different stages of the training.
Callbacks implement steps that are not strictly necessary for training. Instead, they can be used to define things like logging or applying non\sphinxhyphen{}essential operations to the model (i.e., weigh pruning after each epoch) that add new functions but are not required to perform training.
On the other hand, plugins are meant to extend the \sphinxcode{\sphinxupquote{Trainer}} with new functionalities like adding support for new accelerators or computational backends. So by their scope, they are meant to be used by experienced users who need to extend the Trainer.
However, since their API is still in beta and subject to changes in the future, it should be used with caution.

\sphinxAtStartPar
Also, it contains a handful of tweaks to improve the results, like gradient accumulation or gradient .
In addition to that, the \sphinxcode{\sphinxupquote{Trainer}} also supports tuning the learning rate and batch size out of the box. Both tuning features must be enabled while initializing the \sphinxcode{\sphinxupquote{Trainer}} and can be invoked by calling the \sphinxcode{\sphinxupquote{.tune}}\sphinxhyphen{}method.


\subsubsection{Logging}
\label{\detokenize{PyTorchLightning:id2}}
\sphinxAtStartPar
While the model defines what measures are logged, the \sphinxcode{\sphinxupquote{Trainer}} is responsible for writing out these logs.
By default, it logs the standard output.
In addition to that, it can be extended with additional loggers.
PyTorch Lightning provides built\sphinxhyphen{}in loggers that log the progress to Tensorboard or other services like Weights and Biases.
Further loggers, can be implemented using the Logger base\sphinxhyphen{}class.
Multiple loggers are passed to the \sphinxcode{\sphinxupquote{Trainer}} during initialization as a list.


\subsection{CLI Interface}
\label{\detokenize{PyTorchLightning:cli-interface}}
\sphinxAtStartPar
PyTorch Lightning supports the creation of command\sphinxhyphen{}line interfaces through the \sphinxcode{\sphinxupquote{LightninArgumentParser}} class.
This class is an extended version of the parser from the \sphinxcode{\sphinxupquote{jsonargparse}} module, and it can parse the arguments of Lightning classes and other classes out\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}box.
This feature enables adding parameters of different modules to the parser effortlessly.

\sphinxAtStartPar
If more flexibility is needed, for example, when only some parameters of an object should be added to the parser, the best practice is to add a method to the object, which adds these arguments to the parser.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}lightning\PYGZus{}class\PYGZus{}args}\PYG{p}{(}\PYG{n}{ModelCheckpoint}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{checkpoint}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}class\PYGZus{}arguments}\PYG{p}{(}\PYG{n}{TensorBoardLogger}\PYG{p}{,} \PYG{n}{nested\PYGZus{}key}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensorboard}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}lightning\PYGZus{}class\PYGZus{}args}\PYG{p}{(}\PYG{n}{Trainer}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{trainer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{parser} \PYG{o}{=} \PYG{n}{PlLanguageModelForSequenceOrdering}\PYG{o}{.}\PYG{n}{add\PYGZus{}model\PYGZus{}specific\PYGZus{}args}\PYG{p}{(}\PYG{n}{parser}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\section{Implementation}
\label{\detokenize{PyTorchLightning:implementation}}

\subsection{Model}
\label{\detokenize{PyTorchLightning:model}}
\sphinxAtStartPar
Since we do not build our model from scratch, we need to load the pretrained transformer in the LightningModules’s constructor.
To be able to load different models, we introduce the name of the model as hyperparameters.
Since the model is pretrained, we only have to specify two other hyperparameters, namely the learning rate and the id of the target token.
Because Huggingface models are also subclasses of the  \sphinxcode{\sphinxupquote{nn.Module}} class, loading the transformer model works flawlessly, and the language model is recognized as a submodule of the \sphinxcode{\sphinxupquote{PlLanguageModelForSequenceOrdering}} class.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{PlLanguageModelForSequenceOrdering}\PYG{p}{(}\PYG{n}{LightningModule}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{hparams}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{save\PYGZus{}hyperparameters}\PYG{p}{(}\PYG{n}{hparams}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{base\PYGZus{}model} \PYG{o}{=} \PYG{n}{AutoModelForTokenClassification}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hparams}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{return\PYGZus{}dict}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
            \PYG{n}{output\PYGZus{}hidden\PYGZus{}states}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
            \PYG{n}{num\PYGZus{}labels}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
        \PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Next, we define a single forward step. Again, the logic is pretty simple since we only need to exclude the labels from the inputs for the language model and pass the rest of the input data to the language model to obtain the outputs.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{:} \PYG{n}{Dict}\PYG{p}{[}\PYG{n}{Any}\PYG{p}{,} \PYG{n}{Any}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Dict}\PYG{p}{[}\PYG{n}{Any}\PYG{p}{,} \PYG{n}{Any}\PYG{p}{]}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} We do not want to compute token classification loss, so we remove the labels temporarily}
        \PYG{n}{labels} \PYG{o}{=} \PYG{n}{inputs}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{outputs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{base\PYGZus{}model}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{inputs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} And reattach them later on ...}
        \PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{labels}
        \PYG{k}{return} \PYG{n}{outputs}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Because we want to compute the loss while training and validating the model, we factor out the loss function into a separate method.
Implementation\sphinxhyphen{}wise, the loss function is only slightly variated from the original implementation. The only changes are that we retrieve the target token id from the hyperparameters of the model.
Also, we draw inspiration from the \sphinxcode{\sphinxupquote{transformers}} API and add a custom version of the forward method. This method computes both the forward step and the loss. The loss is then attached to the output of the model.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}compute\PYGZus{}loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch\PYGZus{}labels}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{float}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Since we have varying number of labels per instance, }
        \PYG{c+c1}{\PYGZsh{} we need to compute the loss manually for each one.}
        \PYG{n}{loss\PYGZus{}fn} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{MSELoss}\PYG{p}{(}\PYG{n}{reduction}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sum}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{labels}\PYG{p}{,} \PYG{n}{logits}\PYG{p}{,} \PYG{n}{input\PYGZus{}ids} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}
            \PYG{n}{batch\PYGZus{}labels}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}
        \PYG{p}{)}\PYG{p}{:}

            \PYG{c+c1}{\PYGZsh{} Firstly, we need to convert the sentence indices to regression targets.}
            \PYG{c+c1}{\PYGZsh{} To avoid exploding gradients, we norm them to be in range 0 \PYGZlt{}\PYGZhy{}\PYGZgt{} 1.}
            \PYG{c+c1}{\PYGZsh{} labels = labels / labels.max()}
            \PYG{c+c1}{\PYGZsh{} Also we need to remove the padding entries (\PYGZhy{}100).}
            \PYG{n}{true\PYGZus{}labels} \PYG{o}{=} \PYG{n}{labels}\PYG{p}{[}\PYG{n}{labels} \PYG{o}{!=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{targets} \PYG{o}{=} \PYG{n}{true\PYGZus{}labels}\PYG{o}{.}\PYG{n}{float}\PYG{p}{(}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Secondly, we need to get the logits }
            \PYG{c+c1}{\PYGZsh{} from each target token in the input sequence}
            \PYG{n}{target\PYGZus{}logits} \PYG{o}{=} \PYG{n}{logits}\PYG{p}{[}
                \PYG{n}{input\PYGZus{}ids} \PYG{o}{==} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hparams}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target\PYGZus{}token\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
            \PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Sometimes we will have less target\PYGZus{}logits }
            \PYG{c+c1}{\PYGZsh{} than targets due to truncation of the input.}
            \PYG{c+c1}{\PYGZsh{} In this case, we just consider as many targets as we have logit.}
            \PYG{k}{if} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{targets}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{targets} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{p}{:} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}

            \PYG{c+c1}{\PYGZsh{} Finally we compute the loss for the current instance }
            \PYG{c+c1}{\PYGZsh{} and add it to the batch loss.}
            \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{+} \PYG{n}{loss\PYGZus{}fn}\PYG{p}{(}\PYG{n}{targets}\PYG{p}{,} \PYG{n}{target\PYGZus{}logits}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} The final loss is obtained by averaging }
        \PYG{c+c1}{\PYGZsh{} over the number of instances per batch.}
        \PYG{n}{loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{/} \PYG{n}{batch\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

        \PYG{k}{return} \PYG{n}{loss}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}forward\PYGZus{}with\PYGZus{}loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{outputs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Get sentence indices}
        \PYG{n}{batch\PYGZus{}labels} \PYG{o}{=} \PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} Get logits from model}
        \PYG{n}{batch\PYGZus{}logits} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} Get logits for all cls tokens}
        \PYG{n}{batch\PYGZus{}input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

        \PYG{n}{loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}compute\PYGZus{}loss}\PYG{p}{(}
            \PYG{n}{batch\PYGZus{}labels}\PYG{o}{=}\PYG{n}{batch\PYGZus{}labels}\PYG{p}{,}
            \PYG{n}{batch\PYGZus{}logits}\PYG{o}{=}\PYG{n}{batch\PYGZus{}logits}\PYG{p}{,}
            \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}\PYG{o}{=}\PYG{n}{batch\PYGZus{}input\PYGZus{}ids}\PYG{p}{,}
        \PYG{p}{)}
        \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{loss}

        \PYG{k}{return} \PYG{n}{outputs}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Using the \sphinxcode{\sphinxupquote{\_foward\_with\_loss}}\sphinxhyphen{}method implementing the \sphinxcode{\sphinxupquote{training\_step}}\sphinxhyphen{}method becomes relatively simple.
The only thing left to do inside this method is to log the training loss in order to be able to monitor the progress during training.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{k}{def} \PYG{n+nf}{training\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{:} \PYG{n}{Dict}\PYG{p}{[}\PYG{n}{Any}\PYG{p}{,} \PYG{n}{Any}\PYG{p}{]}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{:} \PYG{n+nb}{int}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{float}\PYG{p}{:}
        \PYG{n}{outputs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}forward\PYGZus{}with\PYGZus{}loss}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
        \PYG{n}{loss} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{loss}\PYG{p}{,} \PYG{n}{logger}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{loss}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Like the \sphinxcode{\sphinxupquote{\_compute\_loss}}\sphinxhyphen{}method, we only need to slightly adapt the validation metrics’ computation to use the model’s hyperparameters.
Since we want to compute the identical scores for testing and validation, we can also use the \sphinxcode{\sphinxupquote{validation\_step}}\sphinxhyphen{}method for testing.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{k}{def} \PYG{n+nf}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{outputs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}forward\PYGZus{}with\PYGZus{}loss}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Detach all torch.tensors and convert them to np.arrays.}
        \PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{outputs}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{if} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{value}\PYG{p}{,} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{outputs}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{=} \PYG{n}{value}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{inputs}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
            \PYG{k}{if} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{value}\PYG{p}{,} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{Tensor}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{inputs}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]} \PYG{o}{=} \PYG{n}{value}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Get sentence indices}
        \PYG{n}{batch\PYGZus{}labels} \PYG{o}{=} \PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} Get logits from model}
        \PYG{n}{batch\PYGZus{}logits} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} Get logits for all cls tokens}
        \PYG{n}{batch\PYGZus{}input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{inputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

        \PYG{n}{metrics} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{logits} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}
            \PYG{n}{batch\PYGZus{}labels}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}
        \PYG{p}{)}\PYG{p}{:}
            \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{input\PYGZus{}ids}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{logits} \PYG{o}{=} \PYG{n}{logits}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

            \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{[}\PYG{n}{sent\PYGZus{}idx} \PYG{o}{!=} \PYG{l+m+mi}{100}\PYG{p}{]}
            \PYG{n}{target\PYGZus{}logits} \PYG{o}{=} \PYG{n}{logits}\PYG{p}{[}\PYG{n}{input\PYGZus{}ids} \PYG{o}{==} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hparams}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target\PYGZus{}token\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{]}
            \PYG{k}{if} \PYG{n}{sent\PYGZus{}idx}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{:}
                \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{[}\PYG{p}{:} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}

            \PYG{c+c1}{\PYGZsh{} Calling argsort twice on the logits }
            \PYG{c+c1}{\PYGZsh{} gives us their ranking in ascending order.}
            \PYG{n}{predicted\PYGZus{}idx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{target\PYGZus{}logits}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{tau}\PYG{p}{,} \PYG{n}{pvalue} \PYG{o}{=} \PYG{n}{kendalltau}\PYG{p}{(}\PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}idx}\PYG{p}{)}
            \PYG{n}{acc} \PYG{o}{=} \PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}idx}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{kendalls\PYGZus{}tau}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{tau}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{acc}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{acc}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{std\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

        \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{loss}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Add val prefix to each metric name and compute mean over the batch.}
        \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{\PYGZob{}}
            \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{metric}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{o}{.}\PYG{n}{item}\PYG{p}{(}\PYG{p}{)}
            \PYG{k}{for} \PYG{n}{metric}\PYG{p}{,} \PYG{n}{scores} \PYG{o+ow}{in} \PYG{n}{metrics}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}
        \PYG{p}{\PYGZcb{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}dict}\PYG{p}{(}\PYG{n}{metrics}\PYG{p}{,} \PYG{n}{prog\PYGZus{}bar}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{logger}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{on\PYGZus{}epoch}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{on\PYGZus{}step}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{metrics}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{validation\PYGZus{}step}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{,} \PYG{n}{batch\PYGZus{}idx}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Lastly, we need to implement the \sphinxcode{\sphinxupquote{configure\_optimizers}}\sphinxhyphen{}method and add the model’s hyperparameter to the parser via the \sphinxcode{\sphinxupquote{add\_model\_specific\_args}}\sphinxhyphen{}method.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
    \PYG{k}{def} \PYG{n+nf}{configure\PYGZus{}optimizers}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{optim}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{params}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{hparams}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}

    \PYG{n+nd}{@staticmethod}
    \PYG{k}{def} \PYG{n+nf}{add\PYGZus{}model\PYGZus{}specific\PYGZus{}args}\PYG{p}{(}\PYG{n}{parent\PYGZus{}parser}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{parser} \PYG{o}{=} \PYG{n}{parent\PYGZus{}parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument\PYGZus{}group}\PYG{p}{(}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{PlLanguageModelForSequenceOrdering}\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{p}{)}
        \PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}model.model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{)}
        \PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}model.lr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{float}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{l+m+mf}{3e\PYGZhy{}5}\PYG{p}{)}
        \PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}model.target\PYGZus{}token\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{l+m+mi}{101}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{parent\PYGZus{}parser}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\subsection{Data}
\label{\detokenize{PyTorchLightning:data}}
\sphinxAtStartPar
In contrast to the model class, we design our version of the \sphinxcode{\sphinxupquote{LightningDataModule}} to work with any Huggingface \sphinxcode{\sphinxupquote{Dataset}}.
Most of the work is done by the \sphinxcode{\sphinxupquote{.prepare\_data}}\sphinxhyphen{}method, which implements the processing pipeline for the contained dataset
Firstly, it applies all functions to prepare the data via the \sphinxcode{\sphinxupquote{.map}}\sphinxhyphen{}method of the \sphinxcode{\sphinxupquote{Dataset}} class.
Afterward, the text data will be tokenized using the passed instance of the tokenizer.
Lastly, it is ensured that the dataset’s column containing the target is named \sphinxcode{\sphinxupquote{labels}} to be compliant with standard \sphinxcode{\sphinxupquote{transformers}} models.
Additionally, we implement a method to use the map functionalities of the contained dataset directly.
This method allows the manipulation of the data manually since the \sphinxcode{\sphinxupquote{.prepare\_data}}\sphinxhyphen{}method is automatically executed by the \sphinxcode{\sphinxupquote{Trainer}}.
The datasets wrapped in this class should already contain train\sphinxhyphen{}/ test\sphinxhyphen{} and validation\sphinxhyphen{}splits.
To create batches of the data, we use the default collation function of the transformers library but allow passing a custom collation function.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{HuggingfaceDatasetWrapper}\PYG{p}{(}\PYG{n}{LightningDataModule}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}
        \PYG{n+nb+bp}{self}\PYG{p}{,}
        \PYG{n}{dataset}\PYG{p}{:} \PYG{n}{Dataset}\PYG{p}{,}
        \PYG{n}{text\PYGZus{}column}\PYG{p}{:} \PYG{n+nb}{str}\PYG{p}{,}
        \PYG{n}{target\PYGZus{}column}\PYG{p}{:} \PYG{n+nb}{str}\PYG{p}{,}
        \PYG{n}{tokenizer}\PYG{p}{:} \PYG{n}{PreTrainedTokenizerBase}\PYG{p}{,}
        \PYG{n}{train\PYGZus{}batch\PYGZus{}size}\PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{l+m+mi}{8}\PYG{p}{,}
        \PYG{n}{eval\PYGZus{}batch\PYGZus{}size}\PYG{p}{:} \PYG{n+nb}{int} \PYG{o}{=} \PYG{l+m+mi}{16}\PYG{p}{,}
        \PYG{n}{mapping\PYGZus{}funcs}\PYG{p}{:} \PYG{n}{List}\PYG{p}{[}\PYG{n}{Callable}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,}
        \PYG{n}{collate\PYGZus{}fn}\PYG{p}{:} \PYG{n}{Callable} \PYG{o}{=} \PYG{n}{default\PYGZus{}data\PYGZus{}collator}\PYG{p}{,}
        \PYG{n}{train\PYGZus{}split\PYGZus{}name}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{eval\PYGZus{}split\PYGZus{}name}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{test\PYGZus{}split\PYGZus{}name}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{text\PYGZus{}column} \PYG{o}{=} \PYG{n}{text\PYGZus{}column}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}column} \PYG{o}{=} \PYG{n}{target\PYGZus{}column}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{tokenizer}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{train\PYGZus{}batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{train\PYGZus{}batch\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eval\PYGZus{}batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{eval\PYGZus{}batch\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mapping\PYGZus{}funcs} \PYG{o}{=} \PYG{n}{mapping\PYGZus{}funcs}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{collate\PYGZus{}fn} \PYG{o}{=} \PYG{n}{collate\PYGZus{}fn}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{train\PYGZus{}split\PYGZus{}name} \PYG{o}{=} \PYG{n}{train\PYGZus{}split\PYGZus{}name}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eval\PYGZus{}split\PYGZus{}name} \PYG{o}{=} \PYG{n}{eval\PYGZus{}split\PYGZus{}name}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{test\PYGZus{}split\PYGZus{}name} \PYG{o}{=} \PYG{n}{test\PYGZus{}split\PYGZus{}name}

    \PYG{k}{def} \PYG{n+nf}{prepare\PYGZus{}data}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{tokenizer\PYGZus{}kwargs}\PYG{p}{:} \PYG{n}{Dict}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n+nb}{str}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} 1. Apply user defined preparation functions}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mapping\PYGZus{}funcs}\PYG{p}{:}
            \PYG{k}{for} \PYG{n}{mapping\PYGZus{}func} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mapping\PYGZus{}funcs}\PYG{p}{:}
                \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{mapping\PYGZus{}func}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} 2. Tokenize the text}
        \PYG{k}{if} \PYG{n}{tokenizer\PYGZus{}kwargs} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{tokenizer\PYGZus{}kwargs} \PYG{o}{=} \PYG{p}{\PYGZob{}}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{truncation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{True}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{padding}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}length}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{add\PYGZus{}special\PYGZus{}tokens}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{k+kc}{False}\PYG{p}{,}
            \PYG{p}{\PYGZcb{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}
            \PYG{k}{lambda} \PYG{n}{e}\PYG{p}{:} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tokenizer}\PYG{p}{(}\PYG{n}{e}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{text\PYGZus{}column}\PYG{p}{]}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{tokenizer\PYGZus{}kwargs}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
        \PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} 3. Set format of important columns to torch}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{set\PYGZus{}format}\PYG{p}{(}
            \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{torch}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{attention\PYGZus{}mask}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}column}\PYG{p}{]}
        \PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} 4. If the target columns is not named \PYGZsq{}labels\PYGZsq{} rename it}
        \PYG{k}{try}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{rename\PYGZus{}column}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}column}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{k}{except} \PYG{n+ne}{ValueError}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} target column should already have correct name}
            \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{train\PYGZus{}dataloader}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{DataLoader}\PYG{p}{(}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{train\PYGZus{}split\PYGZus{}name}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{train\PYGZus{}batch\PYGZus{}size}\PYG{p}{,}
            \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{collate\PYGZus{}fn}\PYG{p}{,}
        \PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{val\PYGZus{}dataloader}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{DataLoader}\PYG{p}{(}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eval\PYGZus{}split\PYGZus{}name}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eval\PYGZus{}batch\PYGZus{}size}\PYG{p}{,}
            \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{collate\PYGZus{}fn}\PYG{p}{,}
        \PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{test\PYGZus{}dataloader}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{DataLoader}\PYG{p}{(}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{test\PYGZus{}split\PYGZus{}name}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eval\PYGZus{}batch\PYGZus{}size}\PYG{p}{,}
            \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{collate\PYGZus{}fn}\PYG{p}{,}
        \PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{map}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
        \PYG{k}{return} \PYG{n+nb+bp}{self}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\subsection{Complete code}
\label{\detokenize{PyTorchLightning:complete-code}}
\sphinxAtStartPar
Once again, after factoring out the custom modules, the actual experiment can be implemented in relatively few lines of code.
To control the experiment via the command line, we use the \sphinxcode{\sphinxupquote{LightningArgumentParser}}.
We initialize the parser with all arguments from the \sphinxcode{\sphinxupquote{Trainer,}} \sphinxcode{\sphinxupquote{PlLanguageModelForSequenceOrdering}}, and \sphinxcode{\sphinxupquote{HuggingfaceDatasetWrapper}}.
Additionally, we add more parameters to give each run a name and control the batch sizes for training and testing.
Similar to implementing the experiment with the Huggingface \sphinxcode{\sphinxupquote{Trainer}}, we need to ensure that the sentences contain the correct special tokens.
Replacing these tokens if necessary can be done using the \sphinxcode{\sphinxupquote{.map}}\sphinxhyphen{}method of the \sphinxcode{\sphinxupquote{HuggingfaceDatasetWrapper}}
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{from} \PYG{n+nn}{os}\PYG{n+nn}{.}\PYG{n+nn}{path} \PYG{k+kn}{import} \PYG{n}{basename}
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}
\PYG{k+kn}{from} \PYG{n+nn}{pytorch\PYGZus{}lightning} \PYG{k+kn}{import} \PYG{n}{Trainer}\PYG{p}{,} \PYG{n}{seed\PYGZus{}everything}
\PYG{k+kn}{from} \PYG{n+nn}{pytorch\PYGZus{}lightning}\PYG{n+nn}{.}\PYG{n+nn}{loggers}\PYG{n+nn}{.}\PYG{n+nn}{tensorboard} \PYG{k+kn}{import} \PYG{n}{TensorBoardLogger}
\PYG{k+kn}{from} \PYG{n+nn}{pytorch\PYGZus{}lightning}\PYG{n+nn}{.}\PYG{n+nn}{callbacks} \PYG{k+kn}{import} \PYG{n}{ModelCheckpoint}
\PYG{k+kn}{from} \PYG{n+nn}{pytorch\PYGZus{}lightning}\PYG{n+nn}{.}\PYG{n+nn}{utilities}\PYG{n+nn}{.}\PYG{n+nn}{cli} \PYG{k+kn}{import} \PYG{n}{LightningArgumentParser}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoTokenizer}

\PYG{k+kn}{from} \PYG{n+nn}{pl\PYGZus{}modules} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{HuggingfaceDatasetWrapper}\PYG{p}{,}
    \PYG{n}{PlLanguageModelForSequenceOrdering}\PYG{p}{,}
    \PYG{n}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{,}
\PYG{p}{)}


\PYG{k}{def} \PYG{n+nf}{main}\PYG{p}{(}\PYG{n}{model\PYGZus{}args}\PYG{p}{,} \PYG{n}{trainer\PYGZus{}args}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}args}\PYG{p}{,} \PYG{n}{tensorboard\PYGZus{}args}\PYG{p}{,} \PYG{n}{run\PYGZus{}args}\PYG{p}{)}\PYG{p}{:}

    \PYG{n}{seed\PYGZus{}everything}\PYG{p}{(}\PYG{n}{run\PYGZus{}args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{seed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loading tokenizer.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{model\PYGZus{}args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loading datasets.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{data} \PYG{o}{=} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/rocstories}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Downsampling for debugging...}
    \PYG{c+c1}{\PYGZsh{} data = data.filter(lambda \PYGZus{}, index: index \PYGZlt{} 10000, with\PYGZus{}indices=True)}

    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{HuggingfaceDatasetWrapper}\PYG{p}{(}
        \PYG{n}{data}\PYG{p}{,}
        \PYG{n}{text\PYGZus{}column}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{target\PYGZus{}column}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{so\PYGZus{}targets}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{tokenizer}\PYG{o}{=}\PYG{n}{tokenizer}\PYG{p}{,}
        \PYG{n}{mapping\PYGZus{}funcs}\PYG{o}{=}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{,}
        \PYG{n}{train\PYGZus{}batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{run\PYGZus{}args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{train\PYGZus{}batch\PYGZus{}size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{eval\PYGZus{}batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{run\PYGZus{}args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val\PYGZus{}batch\PYGZus{}size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{p}{)}

    \PYG{k}{if} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token} \PYG{o}{!=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[CLS]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}
            \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Model does not a have a [CLS] token. Updating the data with token }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ ...}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{replace\PYGZus{}cls\PYGZus{}token}\PYG{p}{(}\PYG{n}{entry}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{texts} \PYG{o}{=} \PYG{n}{entry}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
            \PYG{n}{replaced\PYGZus{}texts} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
            \PYG{k}{for} \PYG{n}{text} \PYG{o+ow}{in} \PYG{n}{texts}\PYG{p}{:}
                \PYG{n}{replaced\PYGZus{}texts}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{text}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[CLS]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{entry}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{replaced\PYGZus{}texts}
            \PYG{k}{return} \PYG{n}{entry}

        \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{replace\PYGZus{}cls\PYGZus{}token}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{n}{model\PYGZus{}args}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{target\PYGZus{}token\PYGZus{}id}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token\PYGZus{}id}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loading model.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{PlLanguageModelForSequenceOrdering}\PYG{p}{(}\PYG{n}{hparams}\PYG{o}{=}\PYG{n}{model\PYGZus{}args}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Initializing trainer.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Init logger}
    \PYG{n}{tensorboard\PYGZus{}logger} \PYG{o}{=} \PYG{n}{TensorBoardLogger}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{tensorboard\PYGZus{}args}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Init callbacks}
    \PYG{n}{callbacks} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{checkpoint\PYGZus{}callback} \PYG{o}{=} \PYG{n}{ModelCheckpoint}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{checkpoint\PYGZus{}args}\PYG{p}{)}
    \PYG{n}{callbacks}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{checkpoint\PYGZus{}callback}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Remove default args}
    \PYG{n}{trainer\PYGZus{}args}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logger}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{trainer\PYGZus{}args}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{callbacks}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{trainer} \PYG{o}{=} \PYG{n}{Trainer}\PYG{p}{(}\PYG{n}{logger}\PYG{o}{=}\PYG{n}{tensorboard\PYGZus{}logger}\PYG{p}{,} \PYG{n}{callbacks}\PYG{o}{=}\PYG{n}{callbacks}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{trainer\PYGZus{}args}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Start training.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{trainer}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{model}\PYG{o}{=}\PYG{n}{model}\PYG{p}{,} \PYG{n}{datamodule}\PYG{o}{=}\PYG{n}{dataset}\PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Start testing.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{test\PYGZus{}results} \PYG{o}{=} \PYG{n}{trainer}\PYG{o}{.}\PYG{n}{test}\PYG{p}{(}\PYG{n}{model}\PYG{o}{=}\PYG{n}{model}\PYG{p}{,} \PYG{n}{datamodule}\PYG{o}{=}\PYG{n}{dataset}\PYG{p}{,} \PYG{n}{ckpt\PYGZus{}path}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
    \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test\PYGZus{}results\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{model\PYGZus{}args}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{model\PYGZus{}name\PYGZus{}or\PYGZus{}path}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{w}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
        \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{test\PYGZus{}results}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{n}{parser} \PYG{o}{=} \PYG{n}{LightningArgumentParser}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{group} \PYG{o}{=} \PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument\PYGZus{}group}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{group}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}run.run\PYGZus{}name}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{n}{basename}\PYG{p}{(}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}file\PYGZus{}\PYGZus{}}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{group}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}run.seed}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{group}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}run.train\PYGZus{}batch\PYGZus{}size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}
    \PYG{n}{group}\PYG{o}{.}\PYG{n}{add\PYGZus{}argument}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}run.val\PYGZus{}batch\PYGZus{}size}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n+nb}{type}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{,} \PYG{n}{default}\PYG{o}{=}\PYG{l+m+mi}{16}\PYG{p}{)}

    \PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}lightning\PYGZus{}class\PYGZus{}args}\PYG{p}{(}\PYG{n}{ModelCheckpoint}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{checkpoint}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}class\PYGZus{}arguments}\PYG{p}{(}\PYG{n}{TensorBoardLogger}\PYG{p}{,} \PYG{n}{nested\PYGZus{}key}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensorboard}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{parser}\PYG{o}{.}\PYG{n}{add\PYGZus{}lightning\PYGZus{}class\PYGZus{}args}\PYG{p}{(}\PYG{n}{Trainer}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{trainer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{parser} \PYG{o}{=} \PYG{n}{PlLanguageModelForSequenceOrdering}\PYG{o}{.}\PYG{n}{add\PYGZus{}model\PYGZus{}specific\PYGZus{}args}\PYG{p}{(}\PYG{n}{parser}\PYG{p}{)}

    \PYG{n}{args} \PYG{o}{=} \PYG{n}{parser}\PYG{o}{.}\PYG{n}{parse\PYGZus{}args}\PYG{p}{(}\PYG{p}{)}

    \PYG{n}{model\PYGZus{}args} \PYG{o}{=} \PYG{n}{args}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{model}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{trainer\PYGZus{}args} \PYG{o}{=} \PYG{n}{args}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{trainer}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{checkpoint\PYGZus{}args} \PYG{o}{=} \PYG{n}{args}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{checkpoint}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{tensorboard\PYGZus{}args} \PYG{o}{=} \PYG{n}{args}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{tensorboard}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
    \PYG{n}{run\PYGZus{}args} \PYG{o}{=} \PYG{n}{args}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{run}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

    \PYG{n}{main}\PYG{p}{(}\PYG{n}{model\PYGZus{}args}\PYG{p}{,} \PYG{n}{trainer\PYGZus{}args}\PYG{p}{,} \PYG{n}{checkpoint\PYGZus{}args}\PYG{p}{,} \PYG{n}{tensorboard\PYGZus{}args}\PYG{p}{,} \PYG{n}{run\PYGZus{}args}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\section{Conclusion}
\label{\detokenize{PyTorchLightning:conclusion}}
\sphinxAtStartPar
Pytorch Lightning’s goal is not to hide complexity from the user. Instead, it provides an API that helps to structure the complexity into a sequence of single steps.
This approach is constructive when designing custom models from scratch or implementing new training regimes that differ from the standard training loop.
This flexibility comes at the cost of friendliness to beginners. People who have little experience with PyTorch itself will quickly be overwhelmed by PyTorch Lightning API with vast possibilities to customize steps manually.
Even though the documentation is extensive and covers nearly all aspects of the library in great detail, it can be frustrating sometimes that there are multiple ways to achieve the same behavior, and there is little to no guidance in choosing between the different parts.
Like most modern deep learning frameworks, PyTorch Lightning is rapidly evolving, and thus many parts of it are either in beta and subject to significant changes in the future or deprecated. Unfortunately, this is also noticeable when searching the web for further advice since many tips or tutorials quickly become outdated.
Nevertheless, despite these limitations for beginners, experienced users can benefit from using PyTorch Lightning. Not only because of the additional features like built\sphinxhyphen{}in logging, tuning, or other tweaks but mainly because the well\sphinxhyphen{}thought API enforces them to write self\sphinxhyphen{}contained models that contain all the logic for experimenting with them.
This approach effortlessly enables sharing of models and also alleviates maintainability.


\chapter{Poutyne}
\label{\detokenize{Poutyne:poutyne}}\label{\detokenize{Poutyne::doc}}
\sphinxAtStartPar
Compared to the other two frameworks, Poutyne (Paradis \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id5}{2020}{]}) has a more narrow scope.
Instead of trying to make the training of a fixed set of models as easy as possible like Huggingface \sphinxcode{\sphinxupquote{Trainer}}, or facilitating the creation and training of custom models like PyTorch Lightning, it tries to bring the ease of the Keras API from the realms of Tensorflow to the world of PyTorch.
The benefits of the Keras API are its simplicity and orientation at well\sphinxhyphen{}established machine learning frameworks like Scikit\sphinxhyphen{}Learn.
This simplicity lowers the barrier of entry for beginners because it lowers the amount of time needed to get hands\sphinxhyphen{}on training for their first model.
The following exemplary listing shows the typical workflow in Poutyne.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{poutyne} \PYG{k+kn}{import} \PYG{n}{Model}

\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}

\PYG{n}{network} \PYG{o}{=} \PYG{n}{make\PYGZus{}network}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train} \PYG{o}{=} \PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{X\PYGZus{}val}\PYG{p}{,} \PYG{n}{y\PYGZus{}val} \PYG{o}{=} \PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{validation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{load\PYGZus{}data}\PYG{p}{(}\PYG{n}{subset}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{Model}\PYG{p}{(}
    \PYG{n}{network}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sgd}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cross\PYGZus{}entropy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{n}{batch\PYGZus{}metrics}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{accuracy}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{epoch\PYGZus{}metrics}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{f1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
    \PYG{n}{device}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{cuda:0}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{)}

\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}
    \PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,}
    \PYG{n}{validation\PYGZus{}data}\PYG{o}{=}\PYG{p}{(}\PYG{n}{X\PYGZus{}val}\PYG{p}{,} \PYG{n}{y\PYGZus{}val}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
    \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{64}
\PYG{p}{)}

\PYG{n}{results} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{evaluate}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{128}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Like Keras, Poutyne automates many steps for standard cases like the optimizer configuration or the loss function.
However, Poutyne does not mimic the whole Keras API but only the training part.
The model’s creation still has to be done in plain PyTorch, which is generally a bit trickier than Keras because the dimensions of all layers have to be chosen manually.
In addition to the training functions, Poutyne also provides utilities to conduct and save whole experiments and utilities for creating checkpoints, logging, scheduling of the learning rate, and multi\sphinxhyphen{}device training.


\section{Classes}
\label{\detokenize{Poutyne:classes}}

\subsection{Model}
\label{\detokenize{Poutyne:model}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{Model}} class wraps a neural network alongside an optimizer, loss function, and validation metrics.
It exposes \sphinxcode{\sphinxupquote{.fit}}\sphinxhyphen{}, \sphinxcode{\sphinxupquote{.evaluate}}\sphinxhyphen{}, and \sphinxcode{\sphinxupquote{.predict}}\sphinxhyphen{}methods for training, evaluation, and inference.
Each of these methods exits in different variations that consume the data either as a list of batches, PyTorch \sphinxcode{\sphinxupquote{Dataset}}, or as a generator yielding batches.
Additional hyperparameters, like the batch size, or the number of epochs to train, can be passed the methods directly.


\subsection{Experiment}
\label{\detokenize{Poutyne:experiment}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{Experiment}} class is an extended version of the \sphinxcode{\sphinxupquote{Model}} class that comes with helpful additions for conducting deep learning experiments.
Like the \sphinxcode{\sphinxupquote{Model}} class, an \sphinxcode{\sphinxupquote{Experiment}} is equipped with the neural network, optimizer, loss function, and metrics into a single object and has methods to start the training, evaluation, or prediction.
In contrast to the \sphinxcode{\sphinxupquote{Model}} class, which only intends to do basic training, the \sphinxcode{\sphinxupquote{Experiment}} class provides additional features to organize and track the progress.
For example, it supports logging the progress to various formats, like a CSV table or Tensorboard.
Monitoring allows the \sphinxcode{\sphinxupquote{Experiment}} class to save checkpoints of the model that perform best concerning one of the validation metrics.
Also, it saves all the intermediate results and tracked values to the disk by default.
The \sphinxcode{\sphinxupquote{Experiment}} class can automatically configure all metrics and the loss function for the two primary task types, classification, and regression.


\subsection{Data}
\label{\detokenize{Poutyne:data}}
\sphinxAtStartPar
Poutyne is data agnostic meaning, that it does not provide any tooling to load, process, and store the training data.
The only requirements are that the data comes in one of the supported formats and that each batch consists of two objects: one that holds the training data and one that contains the label.


\section{Additional Features}
\label{\detokenize{Poutyne:additional-features}}

\subsection{Metrics}
\label{\detokenize{Poutyne:metrics}}
\sphinxAtStartPar
Poutyne has a custom API for implementing metrics.
It distinguishes between two types of metrics, batch metric, and epoch metrics.
Batch metrics are computed per batch and averaged to obtain the results for one single epoch.
Epoch metrics are computed on the gathered results of one entire epoch. Thus, they are a good choice for measures that would suffer from averaging over the batch results, like the F\sphinxhyphen{}score.
Poutyne provides predefined metrics for both types. But, unfortunately, they only cover classification tasks.
There are two options to add other metrics. Either they have to be implemented manually or taken from Scikit\sphinxhyphen{}Learn and made compatible using a built\sphinxhyphen{}in wrapper class.
Metrics are passed to \sphinxcode{\sphinxupquote{Model}} or \sphinxcode{\sphinxupquote{Experiment}} during their initialization.


\subsection{Callbacks}
\label{\detokenize{Poutyne:callbacks}}
\sphinxAtStartPar
Callbacks are intended to extend the functions of the \sphinxcode{\sphinxupquote{Model}} or \sphinxcode{\sphinxupquote{Experiment}} class. Like the callbacks from the other frameworks, they have access to the model’s current state and can perform actions at various steps while training.
There are many predefined callbacks available that perform all kinds of tasks, ranging from logging, keeping track of gradients, scheduling the learning, creating checkpoints, to sending notifications to inform clients about the progress of the training.


\section{Implementation}
\label{\detokenize{Poutyne:implementation}}
\sphinxAtStartPar
Even though the \sphinxcode{\sphinxupquote{transformers}} models are incompatible with vanilla Poutyne, integrating it does not require complicated changes.
Most of the required adaptions change the data in order to convert betweeN the dictionary\sphinxhyphen{}based data model of the \sphinxcode{\sphinxupquote{transformers}} library and Poutyne’s more classical \sphinxcode{\sphinxupquote{X, y}} format for input data and targets.
Since these changes are task agnostic, we factored most of these adaption tools out of the project into a small standalone library. %
\begin{footnote}[1]\sphinxAtStartFootnote
\sphinxhref{https://github.com/LennartKeller/poutyne-transformers}{poutyne\sphinxhyphen{}transformers}
%
\end{footnote}


\subsection{Data}
\label{\detokenize{Poutyne:id3}}
\sphinxAtStartPar
We create a custom data collator to convert the data for an experiment from the Hugginface \sphinxcode{\sphinxupquote{Dataset}} format into a Poutyne compliant representation.
The main task of the collator is to convert each batch of dictionaries into batches containing tuples of training data and targets.
To do so, the \sphinxcode{\sphinxupquote{TransformersCollator}} copies one or multiple entries from the input dictionaries into the target objects. Depending on the number of keys, this object is either a single tensor or a dictionary.
Additionally, with the \sphinxcode{\sphinxupquote{remove\_labels}}\sphinxhyphen{}parameter, the fields that get copied to the target object can be removed from the model’s input. By default, they are retained in the input data. This functionality enables using the internal computation of the loss of standard models while also being able to use the built\sphinxhyphen{}in metrics of Poutyne for monitoring the training.
Other collation operations are handled by the default collator from \sphinxcode{\sphinxupquote{transformers}} or by a custom function.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{Any}\PYG{p}{,} \PYG{n}{Callable}\PYG{p}{,} \PYG{n}{Dict}\PYG{p}{,} \PYG{n}{List}\PYG{p}{,} \PYG{n}{Tuple}\PYG{p}{,} \PYG{n}{Union}

\PYG{k+kn}{import} \PYG{n+nn}{torch}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{default\PYGZus{}data\PYGZus{}collator}


\PYG{k}{class} \PYG{n+nc}{TransformerCollator}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}
        \PYG{n+nb+bp}{self}\PYG{p}{,}
        \PYG{n}{y\PYGZus{}keys}\PYG{p}{:} \PYG{n}{Union}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{List}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,}
        \PYG{n}{custom\PYGZus{}collator}\PYG{p}{:} \PYG{n}{Callable} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,}
        \PYG{n}{remove\PYGZus{}labels}\PYG{p}{:} \PYG{n+nb}{bool} \PYG{o}{=} \PYG{k+kc}{False}\PYG{p}{,}
    \PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}keys} \PYG{o}{=} \PYG{n}{y\PYGZus{}keys}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{custom\PYGZus{}collator} \PYG{o}{=} \PYG{p}{(}
            \PYG{n}{custom\PYGZus{}collator} \PYG{k}{if} \PYG{n}{custom\PYGZus{}collator} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{k}{else} \PYG{n}{default\PYGZus{}data\PYGZus{}collator}
        \PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{remove\PYGZus{}labels} \PYG{o}{=} \PYG{n}{remove\PYGZus{}labels}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{:} \PYG{n}{Tuple}\PYG{p}{[}\PYG{n}{Dict}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Tuple}\PYG{p}{[}\PYG{n}{Dict}\PYG{p}{,} \PYG{n}{Any}\PYG{p}{]}\PYG{p}{:}
        \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
        \PYG{n}{batch} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{custom\PYGZus{}collator}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}keys} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{y} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{n+nb}{float}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{nan}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
        \PYG{k}{elif} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}keys}\PYG{p}{,} \PYG{n+nb}{list}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{y} \PYG{o}{=} \PYG{p}{\PYGZob{}}
                \PYG{n}{key}\PYG{p}{:} \PYG{n}{batch}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}
                \PYG{k}{if} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o+ow}{in} \PYG{n}{key} \PYG{o+ow}{and} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{remove\PYGZus{}labels}
                \PYG{k}{else} \PYG{n}{batch}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n}{key}\PYG{p}{)}
                \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}keys}
            \PYG{p}{\PYGZcb{}}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{y} \PYG{o}{=} \PYG{n}{batch}\PYG{o}{.}\PYG{n}{pop}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}keys}\PYG{p}{)} \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{remove\PYGZus{}labels} \PYG{k}{else} \PYG{n}{batch}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}keys}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{batch}\PYG{p}{,} \PYG{n}{y}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\subsection{Model}
\label{\detokenize{Poutyne:id4}}
\sphinxAtStartPar
As stated in the Prerequisites chapter, tokenizers return a dictionary of data that contains all data required to be fed into the language model unpacked as keyword arguments.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoModel}\PYG{p}{,} \PYG{n}{AutoTokenizer}

\PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{AutoModel}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{inputs} \PYG{o}{=} \PYG{n}{tokenizer}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Poutyne is inspired by Keras}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{return\PYGZus{}tensors}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{pt}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{inputs}\PYG{p}{)}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
odict\PYGZus{}keys([\PYGZsq{}last\PYGZus{}hidden\PYGZus{}state\PYGZsq{}, \PYGZsq{}pooler\PYGZus{}output\PYGZsq{}])
\end{sphinxVerbatim}
\end{sphinxVerbatimOutput}

\sphinxAtStartPar
Poutyne instead passes the data to the model in the same format it receives it.
To make sure that the data is unpacked correctly, we create a wrapper class.
It is also a subclass of the \sphinxcode{\sphinxupquote{nn. Module}} to ensure that all parameters of the capsulated model can be accessed.
Apart from the data handling, this class also exposes the custom \sphinxcode{\sphinxupquote{save\_pretrained}}\sphinxhyphen{}model of the underlying \sphinxcode{\sphinxupquote{transformers}} model.
This way, it is possible to create checkpoints of the trained model that can be loaded and used in the \sphinxcode{\sphinxupquote{transformers}} ecosystem.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{Any}\PYG{p}{,} \PYG{n}{Dict}
\PYG{k+kn}{from} \PYG{n+nn}{torch} \PYG{k+kn}{import} \PYG{n}{nn}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{PreTrainedModel}


\PYG{k}{class} \PYG{n+nc}{ModelWrapper}\PYG{p}{(}\PYG{n}{nn}\PYG{o}{.}\PYG{n}{Module}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{transformer}\PYG{p}{:} \PYG{n}{PreTrainedModel}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{transformer} \PYG{o}{=} \PYG{n}{transformer}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}repr\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{str}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}class\PYGZus{}\PYGZus{}}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{(}\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{repr}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{transformer}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{)}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Dict}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{Any}\PYG{p}{]}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{transformer}\PYG{p}{(}\PYG{o}{*}\PYG{o}{*}\PYG{n}{inputs}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{save\PYGZus{}pretrained}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{transformer}\PYG{o}{.}\PYG{n}{save\PYGZus{}pretrained}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\subsection{Loss}
\label{\detokenize{Poutyne:loss}}
\sphinxAtStartPar
In Poutyne the loss function receives the output of the model and the targets.
When using default models, neither of both has to be used to obtain the loss, since we can extract the internal loss from the model’s output.
In our case we have to implement a function that computes the loss on our own.
Since we do not have access to the model or the tokenizer, we have to create a loss function that stores the id of the current target token. For that, we opt for creating a class that holds this id as an attribute and computes the loss via its \sphinxcode{\sphinxupquote{\_\_call\_\_}} method.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{class} \PYG{n+nc}{PoutyneSequenceOrderingLoss}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}token\PYGZus{}id} \PYG{o}{=} \PYG{n}{target\PYGZus{}token\PYGZus{}id}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{outputs}\PYG{p}{,} \PYG{n}{targets}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n+nb}{float}\PYG{p}{:}
        \PYG{n}{batch\PYGZus{}labels} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{n}{batch\PYGZus{}logits} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
        \PYG{n}{batch\PYGZus{}input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{} Since we have varying number of labels per instance, we need to compute the loss manually for each one.}
        \PYG{n}{loss\PYGZus{}fn} \PYG{o}{=} \PYG{n}{nn}\PYG{o}{.}\PYG{n}{MSELoss}\PYG{p}{(}\PYG{n}{reduction}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sum}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{torch}\PYG{o}{.}\PYG{n}{tensor}\PYG{p}{(}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{torch}\PYG{o}{.}\PYG{n}{float64}\PYG{p}{,} \PYG{n}{requires\PYGZus{}grad}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{labels}\PYG{p}{,} \PYG{n}{logits}\PYG{p}{,} \PYG{n}{input\PYGZus{}ids} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}
            \PYG{n}{batch\PYGZus{}labels}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}
        \PYG{p}{)}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Firstly, we need to convert the sentence indices to regression targets.}
            \PYG{c+c1}{\PYGZsh{} To avoid exploding gradients, we norm them to be in range 0 \PYGZlt{}\PYGZhy{}\PYGZgt{} 1}
            \PYG{c+c1}{\PYGZsh{} Also we need to remove the padding entries (\PYGZhy{}100)}
            \PYG{n}{true\PYGZus{}labels} \PYG{o}{=} \PYG{n}{labels}\PYG{p}{[}\PYG{n}{labels} \PYG{o}{!=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{targets} \PYG{o}{=} \PYG{n}{true\PYGZus{}labels}\PYG{o}{.}\PYG{n}{float}\PYG{p}{(}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Secondly, we need to get the logits from each target token in the input sequence}
            \PYG{n}{target\PYGZus{}logits} \PYG{o}{=} \PYG{n}{logits}\PYG{p}{[}\PYG{n}{input\PYGZus{}ids} \PYG{o}{==} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{]}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Sometimes we will have less target\PYGZus{}logits than targets due to trunction of the input}
            \PYG{c+c1}{\PYGZsh{} In this case, we just consider as many targets as we have logits}
            \PYG{k}{if} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{targets}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{targets} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{p}{:} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}

            \PYG{c+c1}{\PYGZsh{} Finally we compute the loss for the current instance and add it to the batch loss}
            \PYG{n}{batch\PYGZus{}loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{+} \PYG{n}{loss\PYGZus{}fn}\PYG{p}{(}\PYG{n}{targets}\PYG{p}{,} \PYG{n}{target\PYGZus{}logits}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} The final loss is obtained by averaging over the number of instances per batch}
        \PYG{n}{loss} \PYG{o}{=} \PYG{n}{batch\PYGZus{}loss} \PYG{o}{/} \PYG{n}{batch\PYGZus{}logits}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}

        \PYG{k}{return} \PYG{n}{loss}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\subsection{Metrics}
\label{\detokenize{Poutyne:id5}}
\sphinxAtStartPar
Unlike the Huggingface \sphinxcode{\sphinxupquote{Trainer}}, which expects all external metrics a single function to compute them all at once, in Poutyne, the \sphinxcode{\sphinxupquote{Model}} or \sphinxcode{\sphinxupquote{Experiment}} classes are equipped with multiple single functions for each metric.
Like the loss, functions that compute other performance metrics receive the model’s output alongside the targets (extracted by collation function).
Because \sphinxcode{\sphinxupquote{transformer}} models return not only the logits or predictions of a model but also other things, it is not possible to use Poutynes built\sphinxhyphen{}in metrics out of the box.
They expect the output to be a single tensor containing the logits of the model, so we create a wrapper for metric functions that extracts them from the output and passes them to the metric.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{typing} \PYG{k+kn}{import} \PYG{n}{Any}\PYG{p}{,} \PYG{n}{Callable}\PYG{p}{,} \PYG{n}{Dict}

\PYG{k}{class} \PYG{n+nc}{MetricWrapper}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{metric}\PYG{p}{:} \PYG{n}{Callable}\PYG{p}{,} \PYG{n}{pred\PYGZus{}key}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{y\PYGZus{}key}\PYG{p}{:} \PYG{n+nb}{str} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{metric} \PYG{o}{=} \PYG{n}{metric}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pred\PYGZus{}key} \PYG{o}{=} \PYG{n}{pred\PYGZus{}key}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}key} \PYG{o}{=} \PYG{n}{y\PYGZus{}key}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}set\PYGZus{}metric\PYGZus{}name}\PYG{p}{(}\PYG{n}{metric}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}set\PYGZus{}metric\PYGZus{}name}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{metric}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{=} \PYG{n}{metric}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{outputs}\PYG{p}{:} \PYG{n}{Dict}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n}{Any}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y\PYGZus{}true}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{pred\PYGZus{}key}\PYG{p}{]}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}key} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{y\PYGZus{}true} \PYG{o}{=} \PYG{n}{outputs}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{y\PYGZus{}key}\PYG{p}{]}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{metric}\PYG{p}{(}\PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{y\PYGZus{}true}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}

\sphinxAtStartPar
Since the logging components of Poutyne infer the name of the metric by assessing the class name of their functions, we need to set the \sphinxcode{\sphinxupquote{\_\_name\_\_}}\sphinxhyphen{}attribute of our wrapper instance with the name of the contained metric.

\sphinxAtStartPar
To implement our sentence ordering metrics, we adopt our existing code to return a function for each metric.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{defaultdict}
\PYG{k+kn}{from} \PYG{n+nn}{functools} \PYG{k+kn}{import} \PYG{n}{partial}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{accuracy\PYGZus{}score}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{kendalltau}

\PYG{k}{def} \PYG{n+nf}{make\PYGZus{}compute\PYGZus{}metrics\PYGZus{}functions}\PYG{p}{(}\PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Callable}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}ranking\PYGZus{}func}\PYG{p}{(}
        \PYG{n}{outputs}\PYG{p}{:} \PYG{n}{Dict}\PYG{p}{,} \PYG{n}{targets}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{,} \PYG{n}{metric\PYGZus{}key}\PYG{p}{:} \PYG{n+nb}{str}
    \PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Dict}\PYG{p}{[}\PYG{n+nb}{str}\PYG{p}{,} \PYG{n+nb}{float}\PYG{p}{]}\PYG{p}{:}
        \PYG{n}{batch\PYGZus{}sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{batch\PYGZus{}input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{targets}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{batch\PYGZus{}logits} \PYG{o}{=} \PYG{n}{outputs}\PYG{o}{.}\PYG{n}{detach}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{cpu}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}

        \PYG{n}{metrics} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{n+nb}{list}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{logits} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}
            \PYG{n}{batch\PYGZus{}sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{batch\PYGZus{}input\PYGZus{}ids}\PYG{p}{,} \PYG{n}{batch\PYGZus{}logits}
        \PYG{p}{)}\PYG{p}{:}
            \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{input\PYGZus{}ids} \PYG{o}{=} \PYG{n}{input\PYGZus{}ids}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{logits} \PYG{o}{=} \PYG{n}{logits}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}

            \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{[}\PYG{n}{sent\PYGZus{}idx} \PYG{o}{!=} \PYG{l+m+mi}{100}\PYG{p}{]}
            \PYG{n}{target\PYGZus{}logits} \PYG{o}{=} \PYG{n}{logits}\PYG{p}{[}\PYG{n}{input\PYGZus{}ids} \PYG{o}{==} \PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{p}{]}
            \PYG{k}{if} \PYG{n}{sent\PYGZus{}idx}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{:}
                \PYG{n}{sent\PYGZus{}idx} \PYG{o}{=} \PYG{n}{sent\PYGZus{}idx}\PYG{p}{[}\PYG{p}{:} \PYG{n}{target\PYGZus{}logits}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]}
            \PYG{c+c1}{\PYGZsh{} Calling argsort twice on the logits gives us their ranking in ascending order}
            \PYG{n}{predicted\PYGZus{}idx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{argsort}\PYG{p}{(}\PYG{n}{target\PYGZus{}logits}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{tau}\PYG{p}{,} \PYG{n}{pvalue} \PYG{o}{=} \PYG{n}{kendalltau}\PYG{p}{(}\PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}idx}\PYG{p}{)}
            \PYG{n}{acc} \PYG{o}{=} \PYG{n}{accuracy\PYGZus{}score}\PYG{p}{(}\PYG{n}{sent\PYGZus{}idx}\PYG{p}{,} \PYG{n}{predicted\PYGZus{}idx}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{kendalls\PYGZus{}tau}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{tau}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{acc}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{acc}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{metrics}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{std\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{logits}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{metric}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)} \PYG{k}{for} \PYG{n}{metric}\PYG{p}{,} \PYG{n}{scores} \PYG{o+ow}{in} \PYG{n}{metrics}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
        \PYG{k}{return} \PYG{n}{metrics}\PYG{p}{[}\PYG{n}{metric\PYGZus{}key}\PYG{p}{]}

    \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{metric} \PYG{o+ow}{in} \PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{acc}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{kendalls\PYGZus{}tau}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{mean\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{std\PYGZus{}logits}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{metric\PYGZus{}func} \PYG{o}{=} \PYG{n}{partial}\PYG{p}{(}\PYG{n}{compute\PYGZus{}ranking\PYGZus{}func}\PYG{p}{,} \PYG{n}{metric\PYGZus{}key}\PYG{o}{=}\PYG{n}{metric}\PYG{p}{)}
        \PYG{n}{metric\PYGZus{}func}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{=} \PYG{n}{metric}
        \PYG{n}{metrics}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{metric\PYGZus{}func}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{metrics}

\PYG{n}{metrics} \PYG{o}{=} \PYG{p}{[}
        \PYG{n}{MetricWrapper}\PYG{p}{(}\PYG{n}{func}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{func} \PYG{o+ow}{in} \PYG{n}{make\PYGZus{}compute\PYGZus{}metrics\PYGZus{}functions}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{p}{]}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{[}\PYG{n}{metric}\PYG{o}{.}\PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{k}{for} \PYG{n}{metric} \PYG{o+ow}{in} \PYG{n}{metrics}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
[\PYGZsq{}acc\PYGZsq{}, \PYGZsq{}kendalls\PYGZus{}tau\PYGZsq{}, \PYGZsq{}mean\PYGZus{}logits\PYGZsq{}, \PYGZsq{}std\PYGZus{}logits\PYGZsq{}]
\end{sphinxVerbatim}
\end{sphinxVerbatimOutput}

\sphinxAtStartPar
Additionally, we add two functions to track the mean and standard deviation of the logits to monitor whether the regression can fit the desired indices or only learns their average, which lies around \sphinxcode{\sphinxupquote{2.5}}.


\subsection{Complete code}
\label{\detokenize{Poutyne:complete-code}}
\sphinxAtStartPar
Once again, we factor out our adaptions into an external module and implement the rest of the experiment.
Due to Poutyne’s lack of tooling for creating a command\sphinxhyphen{}line interface, this experiment is only configurable via hard\sphinxhyphen{}coding the parameters into the source.
The rest of the code is mainly similar to the other two frameworks.
\begin{sphinxVerbatimInput}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{json}
\PYG{k+kn}{from} \PYG{n+nn}{poutyne}\PYG{n+nn}{.}\PYG{n+nn}{framework} \PYG{k+kn}{import} \PYG{n}{experiment}
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{optim} \PYG{k+kn}{import} \PYG{n}{AdamW}
\PYG{k+kn}{from} \PYG{n+nn}{poutyne} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{set\PYGZus{}seeds}\PYG{p}{,}
    \PYG{n}{TensorBoardLogger}\PYG{p}{,}
    \PYG{n}{TensorBoardGradientTracker}\PYG{p}{,}
    \PYG{n}{Experiment}\PYG{p}{,}
\PYG{p}{)}
\PYG{k+kn}{from} \PYG{n+nn}{poutyne\PYGZus{}transformers} \PYG{k+kn}{import} \PYG{n}{ModelWrapper}\PYG{p}{,} \PYG{n}{MetricWrapper}\PYG{p}{,} \PYG{n}{TransformerCollator}
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{tensorboard} \PYG{k+kn}{import} \PYG{n}{SummaryWriter}
\PYG{k+kn}{from} \PYG{n+nn}{torch}\PYG{n+nn}{.}\PYG{n+nn}{utils}\PYG{n+nn}{.}\PYG{n+nn}{data} \PYG{k+kn}{import} \PYG{n}{DataLoader}
\PYG{k+kn}{from} \PYG{n+nn}{transformers} \PYG{k+kn}{import} \PYG{n}{AutoTokenizer}\PYG{p}{,} \PYG{n}{AutoModelForTokenClassification}
\PYG{k+kn}{from} \PYG{n+nn}{datasets} \PYG{k+kn}{import} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}
\PYG{k+kn}{from} \PYG{n+nn}{poutyne\PYGZus{}modules} \PYG{k+kn}{import} \PYG{p}{(}
    \PYG{n}{make\PYGZus{}tokenization\PYGZus{}func}\PYG{p}{,}
    \PYG{n}{PoutyneSequenceOrderingLoss}\PYG{p}{,}
    \PYG{n}{make\PYGZus{}compute\PYGZus{}metrics\PYGZus{}functions}\PYG{p}{,}
    \PYG{n}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{,}
\PYG{p}{)}


\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{n}{set\PYGZus{}seeds}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

    \PYG{n}{MODEL\PYGZus{}NAME\PYGZus{}OR\PYGZus{}PATH} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{bert\PYGZhy{}base\PYGZhy{}cased}\PYG{l+s+s2}{\PYGZdq{}}
    \PYG{n}{LEARNING\PYGZus{}RATE} \PYG{o}{=} \PYG{l+m+mf}{3e\PYGZhy{}5}
    \PYG{n}{TRAIN\PYGZus{}BATCH\PYGZus{}SIZE} \PYG{o}{=} \PYG{l+m+mi}{8}
    \PYG{n}{VAL\PYGZus{}BATCH\PYGZus{}SIZE} \PYG{o}{=} \PYG{l+m+mi}{16}
    \PYG{n}{DEVICE} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{N\PYGZus{}EPOCHS} \PYG{o}{=} \PYG{l+m+mi}{3}
    \PYG{n}{SAVE\PYGZus{}DIR} \PYG{o}{=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{experiments/rocstories/bert}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loading model \PYGZam{} tokenizer.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{tokenizer} \PYG{o}{=} \PYG{n}{AutoTokenizer}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}\PYG{n}{MODEL\PYGZus{}NAME\PYGZus{}OR\PYGZus{}PATH}\PYG{p}{)}
    \PYG{n}{transformer} \PYG{o}{=} \PYG{n}{AutoModelForTokenClassification}\PYG{o}{.}\PYG{n}{from\PYGZus{}pretrained}\PYG{p}{(}
        \PYG{n}{MODEL\PYGZus{}NAME\PYGZus{}OR\PYGZus{}PATH}\PYG{p}{,} \PYG{n}{return\PYGZus{}dict}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{num\PYGZus{}labels}\PYG{o}{=}\PYG{l+m+mi}{1}
    \PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Loading \PYGZam{} preparing data.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{load\PYGZus{}from\PYGZus{}disk}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{../data/rocstories/}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{k}{if} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token} \PYG{o}{!=} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[CLS]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}
            \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Model does not a have a [CLS] token. Updating the data with token }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ ...}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{p}{)}

        \PYG{k}{def} \PYG{n+nf}{replace\PYGZus{}cls\PYGZus{}token}\PYG{p}{(}\PYG{n}{entry}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{texts} \PYG{o}{=} \PYG{n}{entry}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
            \PYG{n}{replaced\PYGZus{}texts} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
            \PYG{k}{for} \PYG{n}{text} \PYG{o+ow}{in} \PYG{n}{texts}\PYG{p}{:}
                \PYG{n}{replaced\PYGZus{}texts}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{text}\PYG{o}{.}\PYG{n}{replace}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{[CLS]}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{entry}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{replaced\PYGZus{}texts}
            \PYG{k}{return} \PYG{n}{entry}

        \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{replace\PYGZus{}cls\PYGZus{}token}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{n}{tokenization\PYGZus{}func} \PYG{o}{=} \PYG{n}{make\PYGZus{}tokenization\PYGZus{}func}\PYG{p}{(}
        \PYG{n}{tokenizer}\PYG{o}{=}\PYG{n}{tokenizer}\PYG{p}{,}
        \PYG{n}{text\PYGZus{}column}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{add\PYGZus{}special\PYGZus{}tokens}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,}
        \PYG{n}{padding}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}length}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
        \PYG{n}{truncation}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
    \PYG{p}{)}
    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{map}\PYG{p}{(}\PYG{n}{tokenization\PYGZus{}func}\PYG{p}{,} \PYG{n}{batched}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{rename\PYGZus{}column}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{so\PYGZus{}targets}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{dataset} \PYG{o}{=} \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{remove\PYGZus{}columns}\PYG{p}{(}
        \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{text}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{storyid}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{storytitle}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{+} \PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sentence}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{]}
    \PYG{p}{)}
    \PYG{n}{dataset}\PYG{o}{.}\PYG{n}{set\PYGZus{}format}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{torch}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{collate\PYGZus{}fn} \PYG{o}{=} \PYG{n}{TransformerCollator}\PYG{p}{(}
        \PYG{n}{y\PYGZus{}keys}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{labels}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{input\PYGZus{}ids}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,}
        \PYG{n}{custom\PYGZus{}collator}\PYG{o}{=}\PYG{n}{so\PYGZus{}data\PYGZus{}collator}\PYG{p}{,}
        \PYG{n}{remove\PYGZus{}labels}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
    \PYG{p}{)}

    \PYG{n}{train\PYGZus{}dataloader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}
        \PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{train}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{TRAIN\PYGZus{}BATCH\PYGZus{}SIZE}\PYG{p}{,} \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n}{collate\PYGZus{}fn}
    \PYG{p}{)}
    \PYG{n}{val\PYGZus{}dataloader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}
        \PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{val}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{VAL\PYGZus{}BATCH\PYGZus{}SIZE}\PYG{p}{,} \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n}{collate\PYGZus{}fn}
    \PYG{p}{)}
    \PYG{n}{test\PYGZus{}dataloader} \PYG{o}{=} \PYG{n}{DataLoader}\PYG{p}{(}
        \PYG{n}{dataset}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{VAL\PYGZus{}BATCH\PYGZus{}SIZE}\PYG{p}{,} \PYG{n}{collate\PYGZus{}fn}\PYG{o}{=}\PYG{n}{collate\PYGZus{}fn}
    \PYG{p}{)}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Preparing training.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{wrapped\PYGZus{}transformer} \PYG{o}{=} \PYG{n}{ModelWrapper}\PYG{p}{(}\PYG{n}{transformer}\PYG{p}{)}
    \PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{AdamW}\PYG{p}{(}\PYG{n}{wrapped\PYGZus{}transformer}\PYG{o}{.}\PYG{n}{parameters}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n}{LEARNING\PYGZus{}RATE}\PYG{p}{)}
    \PYG{n}{loss\PYGZus{}fn} \PYG{o}{=} \PYG{n}{PoutyneSequenceOrderingLoss}\PYG{p}{(}\PYG{n}{target\PYGZus{}token\PYGZus{}id}\PYG{o}{=}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token\PYGZus{}id}\PYG{p}{)}

    \PYG{n}{metrics} \PYG{o}{=} \PYG{p}{[}
        \PYG{n}{MetricWrapper}\PYG{p}{(}\PYG{n}{func}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{func} \PYG{o+ow}{in} \PYG{n}{make\PYGZus{}compute\PYGZus{}metrics\PYGZus{}functions}\PYG{p}{(}\PYG{n}{tokenizer}\PYG{o}{.}\PYG{n}{cls\PYGZus{}token\PYGZus{}id}\PYG{p}{)}
    \PYG{p}{]}

    \PYG{n}{writer} \PYG{o}{=} \PYG{n}{SummaryWriter}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{runs/roberta/1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{tensorboard\PYGZus{}logger} \PYG{o}{=} \PYG{n}{TensorBoardLogger}\PYG{p}{(}\PYG{n}{writer}\PYG{p}{)}
    \PYG{n}{gradient\PYGZus{}logger} \PYG{o}{=} \PYG{n}{TensorBoardGradientTracker}\PYG{p}{(}\PYG{n}{writer}\PYG{p}{)}

    \PYG{n}{experiment} \PYG{o}{=} \PYG{n}{Experiment}\PYG{p}{(}
        \PYG{n}{directory}\PYG{o}{=}\PYG{n}{SAVE\PYGZus{}DIR}\PYG{p}{,}
        \PYG{n}{network}\PYG{o}{=}\PYG{n}{wrapped\PYGZus{}transformer}\PYG{p}{,}
        \PYG{n}{device}\PYG{o}{=}\PYG{n}{DEVICE}\PYG{p}{,}
        \PYG{n}{logging}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
        \PYG{n}{optimizer}\PYG{o}{=}\PYG{n}{optimizer}\PYG{p}{,}
        \PYG{n}{loss\PYGZus{}function}\PYG{o}{=}\PYG{n}{loss\PYGZus{}fn}\PYG{p}{,}
        \PYG{n}{batch\PYGZus{}metrics}\PYG{o}{=}\PYG{n}{metrics}\PYG{p}{,}
    \PYG{p}{)}

    \PYG{n}{experiment}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}
        \PYG{n}{train\PYGZus{}generator}\PYG{o}{=}\PYG{n}{train\PYGZus{}dataloader}\PYG{p}{,}
        \PYG{n}{valid\PYGZus{}generator}\PYG{o}{=}\PYG{n}{val\PYGZus{}dataloader}\PYG{p}{,}
        \PYG{n}{epochs}\PYG{o}{=}\PYG{n}{N\PYGZus{}EPOCHS}\PYG{p}{,}
        \PYG{n}{save\PYGZus{}every\PYGZus{}epoch}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,}
    \PYG{p}{)}

    \PYG{n}{test\PYGZus{}results} \PYG{o}{=} \PYG{n}{experiment}\PYG{o}{.}\PYG{n}{test}\PYG{p}{(}\PYG{n}{test\PYGZus{}generator}\PYG{o}{=}\PYG{n}{test\PYGZus{}dataloader}\PYG{p}{)}
    \PYG{k}{with} \PYG{n+nb}{open}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{test\PYGZus{}results\PYGZus{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{MODEL\PYGZus{}NAME\PYGZus{}OR\PYGZus{}PATH}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{.json}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{w}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)} \PYG{k}{as} \PYG{n}{f}\PYG{p}{:}
        \PYG{n}{json}\PYG{o}{.}\PYG{n}{dump}\PYG{p}{(}\PYG{n}{test\PYGZus{}results}\PYG{p}{,} \PYG{n}{f}\PYG{p}{)}
\end{sphinxVerbatim}
\end{sphinxVerbatimInput}


\section{Conclusion}
\label{\detokenize{Poutyne:conclusion}}
\sphinxAtStartPar
Poutyne provides a well thought and, most of all easy to understand framework to train neural networks.
Like its conceptual role model Keras, this simplicity is achieved by strict design decisions, like the \sphinxcode{\sphinxupquote{X, y}} format for data.
While this strictness is helpful for beginners because they only have to learn one way of doing things, it comes at the cost of being hard to adapt to other frameworks or unintended tasks.
Luckily, the necessary steps to adapt it to \sphinxcode{\sphinxupquote{transformers}} and our task are simple and can be reused for most other cases.
Since Poutynes mimics the Keras\sphinxhyphen{}API, its additional features are much more limited than the other frameworks. Even basic techniques like gradient accumulation are not supported.
Depending on the use case, this limited scope might be a deal\sphinxhyphen{}breaker for experienced users or complex tasks, but on the other hand, it makes getting started with the framework much more manageable.
This accessibility is underlined by the documentation’s quality, which covers all aspects of the framework in concise and easily understandable manners without losing itself in the depths of technical details.
Yet, there is also potential for further improvements.
The lack of support for creating\sphinxhyphen{}command line interfaces could force users to migrate to another framework as soon as they need to retrain a model regularly.
Currently, the scope of the framework is heavily skewed towards sequence classification tasks. For example, all built\sphinxhyphen{}in metrics measure the quality of a classification model.
Widening the range of tasks that could be implemented without further extensions would help beginners get into deep learning.
A possible improvement that falls more into the category of wishful thinking would be that Poutyne would mimic not only the training parts of the Keras API.
If Poutyne would also introduce the ease of building neural networks without manually adjusting each layer’s dimensionality, it would significantly contribute to the community.


\bigskip\hrule\bigskip



\chapter{Experimental Results}
\label{\detokenize{ExperimentalResults:experimental-results}}\label{\detokenize{ExperimentalResults::doc}}
\sphinxAtStartPar
To see if our custom task architecture is able to order the sentences, we run the experiment using the ROCStories Dataset with three different pretrained language models.
Also, we run the experiment using the same set of parameters with each framework to check if they perform consistently or if any under\sphinxhyphen{}the\sphinxhyphen{}hood magic influences the scores.


\section{Hyperparemters}
\label{\detokenize{ExperimentalResults:hyperparemters}}
\sphinxAtStartPar
We employ \sphinxcode{\sphinxupquote{distilbert\sphinxhyphen{}base\sphinxhyphen{}cased}}, \sphinxcode{\sphinxupquote{bert\sphinxhyphen{}base\sphinxhyphen{}cased}}, and \sphinxcode{\sphinxupquote{roberta\sphinxhyphen{}base}} as pretrained models.
Intuitively, we expect \sphinxcode{\sphinxupquote{roberta\sphinxhyphen{}Base}} to perform best, with \sphinxcode{\sphinxupquote{bert\sphinxhyphen{}case}} reaching a tight second place and \sphinxcode{\sphinxupquote{distilbert}} to fall short behind the large two models.
We use the AdamW optimizer with a learning rate of \(3e-5\). We finetune for \(3\) epochs and validate our models on the test set while the validation set is only used for tracking the progress during training.
Furthermore, we use the same random seed across all models and frameworks. Due to varying model sizes, we use different batch sizes to fit the model on the GPU. To ensure that the batch sizes do not affect the performance, we use gradient accumulation with the Huggingface \sphinxcode{\sphinxupquote{Trainer}} and PyTorch Lightning to ensure that each model makes the same number of backward steps. For Poutyne, which does not support gradient accumulation, we chose the largest batch size possible for each model.
In addition to these basic parameters, each framework has a set custom parameter that we leave untouched and use the default configuration.


\section{Results}
\label{\detokenize{ExperimentalResults:results}}



\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Results}\label{\detokenize{ExperimentalResults:results-table}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
Framework
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Model
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Loss
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Accuracy
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\(\tau_{\textrm{Kendall}}\)
\\
\hline
\sphinxAtStartPar
Huggingface \sphinxcode{\sphinxupquote{Trainer}}
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{bert\sphinxhyphen{}base\sphinxhyphen{}cased}}
&
\sphinxAtStartPar
2.447
&
\sphinxAtStartPar
0.699
&
\sphinxAtStartPar
0.792
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{roberta\sphinxhyphen{}base}}
&
\sphinxAtStartPar
1.619
&
\sphinxAtStartPar
0.786
&
\sphinxAtStartPar
0.861
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{distilbert\sphinxhyphen{}base\sphinxhyphen{}cased}}
&
\sphinxAtStartPar
2.800
&
\sphinxAtStartPar
0.653
&
\sphinxAtStartPar
0.753
\\
\hline
\sphinxAtStartPar
PyTorch Lightning
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{bert\sphinxhyphen{}base\sphinxhyphen{}cased}}
&
\sphinxAtStartPar
2.621
&
\sphinxAtStartPar
0.696
&
\sphinxAtStartPar
0.785
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{roberta\sphinxhyphen{}base}}
&
\sphinxAtStartPar
1.755
&
\sphinxAtStartPar
0.776
&
\sphinxAtStartPar
0.853
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{distilbert\sphinxhyphen{}base\sphinxhyphen{}cased}}
&
\sphinxAtStartPar
2.933
&
\sphinxAtStartPar
0.651
&
\sphinxAtStartPar
0.748
\\
\hline
\sphinxAtStartPar
Poutyne
&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{bert\sphinxhyphen{}base\sphinxhyphen{}cased}}
&
\sphinxAtStartPar
2.714
&
\sphinxAtStartPar
0.687
&
\sphinxAtStartPar
0.776
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{roberta\sphinxhyphen{}base}}
&
\sphinxAtStartPar
2.106
&
\sphinxAtStartPar
0.748
&
\sphinxAtStartPar
0.830
\\
\hline
\sphinxAtStartPar

&
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{distilbert\sphinxhyphen{}base\sphinxhyphen{}cased}}
&
\sphinxAtStartPar
3.024
&
\sphinxAtStartPar
0.645
&
\sphinxAtStartPar
0.742
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
\hyperref[\detokenize{ExperimentalResults:results-table}]{Table \ref{\detokenize{ExperimentalResults:results-table}}} shows the results of the runs. As expected, \sphinxcode{\sphinxupquote{roberta\sphinxhyphen{}base}} achieved the best results. However, the margin by which it outperforms the standard \sphinxcode{\sphinxupquote{bert\sphinxhyphen{}base\sphinxhyphen{}cased}} model is surprising. In contrast to Bert models, Roberta models do not use the next sentence prediction objective during their finetuning stages. Next sentence prediction is the task of deciding if two consecutive sentences in the input sequence are natural successors or not. Intuitively, the knowledge obtained from this task should help order sentences too. Obviously, without additional inspection, each further presumption is mere speculation. Still, it seems like the fact that Roberta models are pretrained on a larger dataset outweighs the missing next sentence objective during pretraining.



\sphinxAtStartPar
Comparing the results by each framework, the Huggingface \sphinxcode{\sphinxupquote{Trainer}} achieves a clear victory. It reaches the best results across all models and, depending on the metric, outperforms the other frameworks significantly. However, while PyTorch Lightning manages to keep up the the \sphinxcode{\sphinxupquote{Trainer,}} Poutyne falls clear behind its competitors, especially when training the larger models.
This difference in performance indicates the benefit of gradient accumulation when training models that only allow small batch sizes.
However, although more subtle, the gap between the Huggingface \sphinxcode{\sphinxupquote{Trainer}} and PyTorch Lightning is surprising since both frameworks offer roughly the same feature set.
Different parameters can probably explain it since we only changed some parameters and used the default values provided by the developers for the rest.
However, due to its focus on language models, the \sphinxcode{\sphinxupquote{Trainer}} should have a preset of parameters well optimized for language models.
The most plausible candidate for explaining the observed difference is the learning schedule. While PyTorch Lightning uses a fixed learning rate if not specified otherwise, the Huggingface \sphinxcode{\sphinxupquote{Trainer}} employs a learning scheme with a linear decay and additional warmup steps. This strategy, which is proven to be helpful when training large models, might cause the superior results of the \sphinxcode{\sphinxupquote{Trainer}}.

\sphinxAtStartPar
Despite their differences, it has to be stated that all frameworks achieved robust results.
The relative order of the different models is consistent across all frameworks. Therefore, the results of each framework are sufficient not to shade the capabilities of the models or language models in general for this task.

\sphinxAtStartPar
Comparing to the other works, which employ a pretrained language model, we beat the regression baseline from Kumar \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id9}{2020}{]} (\(\tau_{\textrm{Kendall}}=0.736\)) by quite a large margin but still fall clearly behind the current state of art model by Zhu \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.Bibliography:id7}{2021}{]} (\(\tau_{\textrm{Kendall}}=0.849\)). Since the results of both papers were achieved using a standard Bert model, we also have to compare them against our Bert scores. By looking at the Roberta scores, it becomes clear that choosing a larger or better language model seems to have a large influence regardless of the approach. But all of these results have to be taken with a  grain of salt since we only ran the experiment once using a fixed random seed and a custom train\sphinxhyphen{}test\sphinxhyphen{}validation split, making our results less reliable.


\chapter{Conclusion}
\label{\detokenize{Conclusion:conclusion}}\label{\detokenize{Conclusion::doc}}
\sphinxAtStartPar
Due to their different scopes labeling one of the presented frameworks as the “best one” would paint a misleading picture.
Obviously, the built\sphinxhyphen{}in \sphinxcode{\sphinxupquote{Trainer}} of the \sphinxcode{\sphinxupquote{transformers}} library is optimally aligned with the rest of the library, which facilitates the training of language models in many cases.
This ease, combined with its optimized set of predefined parameters, makes it the best choice when training standard models.
Even in cases like ours, where a standard model is combined with a custom loss, the \sphinxcode{\sphinxupquote{Trainer}} requires few adaptions.
However, the goal to allow those adaptions with as few lines of code as possible also has some drawbacks. Conceptually, the separation of concerns between integral parts of the model and additional logic is less strict.
For example, by default, \sphinxcode{\sphinxupquote{transformers}} models incorporate the loss function into their heads. But if this loss should be discarded, the custom loss function is bound to the custom subclass of the \sphinxcode{\sphinxupquote{Trainer}}. This scattering leads to an implicit separation of the model and its loss.
Without access to the \sphinxcode{\sphinxupquote{Trainer}} subclass, continuing the model’s training is impossible. More gravely, this fact is hidden too, since loading the model looks like a standard token classification model and even returns a loss score when fed with the correct data.
Of course, one could argue that it is possible to create proper custom models with an own head, which would be a cleaner way to implement such a model. But since making a custom model is a rather complex process, it would also render the advantage of the \sphinxcode{\sphinxupquote{Trainer}} irrelevant.

\sphinxAtStartPar
From a conceptual point of view, PyTorch Lightning’s approach is far more sustainable since its API forces to structure the code into mostly self\sphinxhyphen{}contained modules.
While requiring more manual implementation upfront, this approach leads to better code quality and makes models and datasets easily interchangeable and thus more reusable.
However, it also expects the user to have a profound knowledge of PyTorch itself, alongside a deep mental model of how a neural network is trained since there are no shortcuts.
Users who fit these requirements can benefit from PyTorch Lightning’s strict specifications.
While these specifications determine the whole process of building neural networks, users do not lose much freedom because the framework is highly flexible and can be modified to a great extend.
Yet, there are drawbacks when working with \sphinxcode{\sphinxupquote{transformers}} and PyTorch Lightning.
Most notably, the differences in the serialization of models complicate the process of creating checkpoints that can be used interchangeably between PyTorch Lightning and the Huggingface ecosystem.
Also, PyTorch Lightning, like most deep learning frameworks, evolves fast and is updated frequently.
This speed can lead to issues that are hard to understand and fix.
For example, on the machine used to run the experiments of this work, only one of the six available computational backends that govern Multi\sphinxhyphen{}GPU training worked reliably.
Using the other ones either led to freezes while training, exceptions that aborted the training complete, or degraded results because the data was corrupted.
These problems were exacerbated because PyTorch code is notoriously hard to debug since most of the computations are done outside the Python runtime and thus hard to access with standard debuggers.
Nonetheless, all the available extension and hyperparameter tuning functions justify the usage of PyTorch Lightning because once the initial setup is running, improving the results is easy and does not require much work.

\sphinxAtStartPar
To be fair, it has to be stated that including Poutyne in this work is unfair since its scope is much more narrow, and, by its intention, it does not try to offer the same set of functions as both other frameworks.
Instead of focusing on its lacks, it becomes clear why Poutyne can be helpful by looking at its conceptual design.
Its conceptual paragon Keras is the most beginner\sphinxhyphen{}friendly library to get started with deep learning.
Since Keras dropped support for other backends like PyTorch, this easy is only available for Tensorflow.
However, since most deep learning research is done in PyTorch nowadays, learning Tensorflow has become less attractive because beginners will have to switch from Tensorflow to PyTorch at some point as they progress.
So because learning PyTorch is inevitable for most users, it would be beneficial to be able to start right away with it.
Without being anywhere near as mature as Keras, Poutyne has the potential of offering a real alternative for it in PyTorch.
Additionally, a high\sphinxhyphen{}level API that enables the quick training of models effortlessly is also attractive for experienced users who want to test a prototype.
As our results showed, the results won’t be as best as possible, but they are sufficient enough to give a first impression.
Like Keras, Poutyne is designed to work best with models built from scratch, but it is easy to adapt the framework to work with pretrained models of all sorts.
The library \sphinxcode{\sphinxupquote{poutyne\sphinxhyphen{}transformers}} might act as a proof\sphinxhyphen{}of\sphinxhyphen{}concept for further adjustments.

\sphinxAtStartPar
In conclusion, this work showed that all three of the frameworks have a reason for existence. Choosing between PyTorch Lightning and the Hugginface \sphinxcode{\sphinxupquote{Trainer}} highly depends on the requirements of the project. PyTorch Lightning is a good choice for models that are intended to go into production. At the same time, the \sphinxcode{\sphinxupquote{Trainer}} is a great choice when doing research where the speed of iteration outweighs the sustainability of the code.
Poutyne is an excellent choice for beginners who want to get into deep learning and start to train models quickly without having to learn complex frameworks.


\chapter{Bibliography}
\label{\detokenize{Bibliography:bibliography}}\label{\detokenize{Bibliography::doc}}
\sphinxAtStartPar


\begin{sphinxthebibliography}{MOBrienR}
\bibitem[Fal19]{Bibliography:id3}
\sphinxAtStartPar
WA Falcon. Pytorch lightning. \sphinxstyleemphasis{GitHub. Note: https://github.com/PyTorchLightning/pytorch\sphinxhyphen{}lightning Cited by}, 2019.
\bibitem[KBKR20]{Bibliography:id9}
\sphinxAtStartPar
Pawan Kumar, Dhanajit Brahma, Harish Karnick, and Piyush Rai. Deep attentive ranking networks for learning to order sentences. \sphinxstyleemphasis{CoRR}, 2020.
\bibitem[MOBrienR18]{Bibliography:id8}
\sphinxAtStartPar
David McClure, Shayne O'Brien, and Deb K. Roy. Context is key: new approaches to neural coherence modeling. \sphinxstyleemphasis{ArXiv}, 2018.
\bibitem[MCH+16]{Bibliography:id2}
\sphinxAtStartPar
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and evaluation framework for deeper understanding of commonsense stories. 2016. \sphinxhref{https://arxiv.org/abs/1604.01696}{arXiv:1604.01696}.
\bibitem[PBG+20]{Bibliography:id5}
\sphinxAtStartPar
Frédérik Paradis, David Beauchemin, Mathieu Godbout, Mathieu Alain, Nicolas Garneau, Stefan Otte, Alexis Tremblay, Marc\sphinxhyphen{}Antoine Bélanger, and François Laviolette. Poutyne: A Simplified Framework for Deep Learning. 2020. https://poutyne.org.
\bibitem[WDS+19]{Bibliography:id10}
\sphinxAtStartPar
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface's transformers: state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art natural language processing. \sphinxstyleemphasis{CoRR}, 2019. URL: \sphinxurl{https://huggingface.co/transformers/index.html}.
\bibitem[YSS+19]{Bibliography:id6}
\sphinxAtStartPar
Yongjing Yin, Linfeng Song, Jinsong Su, Jiali Zeng, Chulun Zhou, and Jiebo Luo. Graph\sphinxhyphen{}based neural sentence ordering. In \sphinxstyleemphasis{Proceedings of the Twenty\sphinxhyphen{}Eighth International Joint Conference on Artificial Intelligence, IJCAI\sphinxhyphen{}19}, 5387–5393. International Joint Conferences on Artificial Intelligence Organization, 7 2019. URL: \sphinxurl{https://doi.org/10.24963/ijcai.2019/748}, \sphinxhref{https://doi.org/10.24963/ijcai.2019/748}{doi:10.24963/ijcai.2019/748}.
\bibitem[ZNZ+21]{Bibliography:id7}
\sphinxAtStartPar
Yutao Zhu, Jian\sphinxhyphen{}Yun Nie, Kun Zhou, Shengchao Liu, and Pan Du. BERT4SO: neural sentence ordering by fine\sphinxhyphen{}tuning BERT. \sphinxstyleemphasis{CoRR}, 2021.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}