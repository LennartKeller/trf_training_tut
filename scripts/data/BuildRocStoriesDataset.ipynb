{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9249009e-87e9-4766-a57e-f1571fef4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996b54d-688c-47d9-a5ad-712add87eb0a",
   "metadata": {},
   "source": [
    "At first, we load the dataset in its original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8723a37d-c298-460f-8725-e88821acd24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ab73c4f3571a1168\n",
      "Reusing dataset csv (/mnt/data/users/keller/.cache/csv/default-ab73c4f3571a1168/0.0.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['storyid', 'storytitle', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5'],\n",
       "    num_rows: 52665\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_csv('/mnt/data/users/keller/ROCStories/ROCStories_winter2017 - ROCStories_winter2017.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de13ccb-00fc-41b5-a2c4-8f9792f26b6f",
   "metadata": {},
   "source": [
    "We got 52.665 stories. Each one has a length of five sentences. Also we have short title for each story, but we discard them and won't include it in our shuffled texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f82a59b-a50a-4b70-8f4a-3a639e28ffbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52665"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414bb82e-3ab8-4c7d-ae78-d45a836b3bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'David noticed he had put on a lot of weight recently.',\n",
      " 'sentence2': 'He examined his habits to try and figure out the reason.',\n",
      " 'sentence3': \"He realized he'd been eating too much fast food lately.\",\n",
      " 'sentence4': 'He stopped going to burger places and started a vegetarian '\n",
      "              'diet.',\n",
      " 'sentence5': 'After a few weeks, he started to feel much better.',\n",
      " 'storyid': '8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd',\n",
      " 'storytitle': 'David Drops the Weight'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef1a40-4c18-4ba9-92a2-d93fcac58245",
   "metadata": {},
   "source": [
    "As next step, we prepare the text by shuffling the sentences and creating labels for each entry indicating the original order. Also we add special tokens as prefix for each sentence.\n",
    "\n",
    "To apply this steps to the data, we use the `.map`-method of the `Dataset`-class. Like nearly all other methods of this class it works out-of-place, meaning that it returns a new dataset with the changes instead of changing the datset it was called on.\n",
    "\n",
    "The `.map`-method works in two modes: bacht or non-batched. In either mode it receives a dictionary as input where each key represents a column if the dataset.\n",
    "In non-bachted mode the values of the input-dictionary are the values of one entry in the dataset. In batched mode they are lists with multiple entries.\n",
    "The following funcation only works in both modes since it converts both input formats to the same intermediate format, but in terms of speed batched-mode should be preffered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24ac3ce-9523-434c-9f40-beaf1a2fa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from random import seed as set_seed\n",
    "\n",
    "def make_shuffle_func(sep_token):\n",
    "    def shuffle_stories(entries, seed=42):\n",
    "        set_seed(seed)\n",
    "        entries_as_dicts = [\n",
    "            dict(zip(entries, values))\n",
    "            for values in zip(*entries.values())\n",
    "        ]\n",
    "        converted_entries = []\n",
    "        for entry in entries_as_dicts:\n",
    "            sents = [\n",
    "                entry[key]\n",
    "                for key in sorted(\n",
    "                    [key for key in entry.keys() if key.startswith('sentence')\n",
    "                    ], key=lambda x: int(x[-1])\n",
    "                )\n",
    "            ]\n",
    "            sent_idx = list(range(len(sents)))\n",
    "            sents_with_idx = list(zip(sents, sent_idx))\n",
    "            shuffle(sents_with_idx)\n",
    "            text = f'{sep_token} ' + f' {sep_token} '.join([s[0] for s in sents_with_idx]) \n",
    "            so_targets = [s[1] for s in sents_with_idx]\n",
    "            shuffled_entry = {'text': text, 'so_targets': so_targets}\n",
    "            converted_entries.append(shuffled_entry)\n",
    "        new_entry = {\n",
    "            key: [entry[key] for entry in converted_entries]\n",
    "            for key in converted_entries[0]\n",
    "        }\n",
    "        return new_entry\n",
    "    return shuffle_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724f4ac-8ea1-4eb4-b952-a80f9c3d4cb1",
   "metadata": {},
   "source": [
    "`[CLS]` is the special token of BERT directly derived models, which while pretraining learns a represenation of the whole input sequence. We will use it as our sentence-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b932d011-0ed5-4601-8641-a5f2c439bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func = make_shuffle_func('[CLS]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2de85a-330c-48af-aab8-62f2288ae71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46750b45bc574e9c9a0ae17ccbcd0249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(map_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7508f-8bb5-474a-b2ca-071f03fa1937",
   "metadata": {},
   "source": [
    "Now our dataset has two additional columns: The `text`column contains the shuffled and concatenated sentences and the `so_targets` columnm contains the indicies of the sentences in the original order. For example in the first text in the dataset the first sentence in the shuffled text is at the 4th place in the original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c96932-d8e3-45dc-a731-9aaddfe37728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'David noticed he had put on a lot of weight recently.',\n",
      " 'sentence2': 'He examined his habits to try and figure out the reason.',\n",
      " 'sentence3': \"He realized he'd been eating too much fast food lately.\",\n",
      " 'sentence4': 'He stopped going to burger places and started a vegetarian '\n",
      "              'diet.',\n",
      " 'sentence5': 'After a few weeks, he started to feel much better.',\n",
      " 'so_targets': [3, 1, 2, 4, 0],\n",
      " 'storyid': '8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd',\n",
      " 'storytitle': 'David Drops the Weight',\n",
      " 'text': '[CLS] He stopped going to burger places and started a vegetarian '\n",
      "         'diet. [CLS] He examined his habits to try and figure out the reason. '\n",
      "         \"[CLS] He realized he'd been eating too much fast food lately. [CLS] \"\n",
      "         'After a few weeks, he started to feel much better. [CLS] David '\n",
      "         'noticed he had put on a lot of weight recently.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb7f4a-fde1-4bf0-8468-32337668facd",
   "metadata": {},
   "source": [
    "Lastly we want to split our dataset into the subset: The train-set will be used for training, the validation set can be used to validate the performance while the training or for hyperparamter optimazation and the testset will be used for the final evulation of a trained and optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec97c46a-83d3-40d0-bd33-9bcfbbf5b38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'so_targets', 'storyid', 'storytitle', 'text'],\n",
       "        num_rows: 42132\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'so_targets', 'storyid', 'storytitle', 'text'],\n",
       "        num_rows: 7373\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'so_targets', 'storyid', 'storytitle', 'text'],\n",
       "        num_rows: 3160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "test_validation = train_test['test'].train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_validation['train'],\n",
    "    'val': test_validation['test']})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f60433-b35c-4565-a2b5-72599b79002d",
   "metadata": {},
   "source": [
    "Finally, we save the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd17171e-09a3-45ce-be73-90d4df7e4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('rocstories')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
