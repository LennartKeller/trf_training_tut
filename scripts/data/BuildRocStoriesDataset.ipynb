{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9249009e-87e9-4766-a57e-f1571fef4411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from datasets import Dataset, DatasetDict\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996b54d-688c-47d9-a5ad-712add87eb0a",
   "metadata": {},
   "source": [
    "At first, we load the dataset in its original format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "674f4141-acee-4c86-b38d-2532a35c9178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(\n",
    "    \"~/Uni/trf_training_tut/scripts/data/ROCStories_winter2017 - ROCStories_winter2017.csv\"\n",
    ").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8723a37d-c298-460f-8725-e88821acd24d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/users/keller/Uni/trf_training_tut/scripts/data/~/Uni/trf_training_tut/scripts/data/ROCStories_winter2017 - ROCStories_winter2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22495/3372927933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'~/Uni/trf_training_tut/scripts/data/ROCStories_winter2017 - ROCStories_winter2017.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[0;34m(path_or_paths, split, features, cache_dir, keep_in_memory, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCsvDatasetReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         return CsvDatasetReader(\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mpath_or_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         ).read()\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/datasets/io/csv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, split, features, cache_dir, keep_in_memory, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         )\n\u001b[1;32m     24\u001b[0m         \u001b[0mpath_or_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_or_paths\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpath_or_paths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         self.builder = Csv(\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_paths\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cache_dir, name, hash, base_path, features, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"data_dir\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_kwargs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         self.config, self.config_id = self._create_builder_config(\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mcustom_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m_create_builder_config\u001b[0;34m(self, name, custom_features, **config_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0;31m# compute the config id that is going to be used for caching\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         config_id = builder_config.create_config_id(\n\u001b[0m\u001b[1;32m    405\u001b[0m             \u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mcustom_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36mcreate_config_id\u001b[0;34m(self, config_kwargs, custom_features, use_auth_token, base_path)\u001b[0m\n\u001b[1;32m    193\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetmtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/genericpath.py\u001b[0m in \u001b[0;36mgetmtime\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetmtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;34m\"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/users/keller/Uni/trf_training_tut/scripts/data/~/Uni/trf_training_tut/scripts/data/ROCStories_winter2017 - ROCStories_winter2017.csv'"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_csv()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de13ccb-00fc-41b5-a2c4-8f9792f26b6f",
   "metadata": {},
   "source": [
    "We got 52.665 stories. Each one has a length of five sentences. Also we have short title for each story, but we discard them and won't include it in our shuffled texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f82a59b-a50a-4b70-8f4a-3a639e28ffbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52665"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414bb82e-3ab8-4c7d-ae78-d45a836b3bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'David noticed he had put on a lot of weight recently.',\n",
      " 'sentence2': 'He examined his habits to try and figure out the reason.',\n",
      " 'sentence3': \"He realized he'd been eating too much fast food lately.\",\n",
      " 'sentence4': 'He stopped going to burger places and started a vegetarian '\n",
      "              'diet.',\n",
      " 'sentence5': 'After a few weeks, he started to feel much better.',\n",
      " 'storyid': '8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd',\n",
      " 'storytitle': 'David Drops the Weight'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef1a40-4c18-4ba9-92a2-d93fcac58245",
   "metadata": {},
   "source": [
    "As next step, we prepare the text by shuffling the sentences and creating labels for each entry indicating the original order. Also we add special tokens as prefix for each sentence.\n",
    "\n",
    "To apply this steps to the data, we use the `.map`-method of the `Dataset`-class. Like nearly all other methods of this class it works out-of-place, meaning that it returns a new dataset with the changes instead of changing the datset it was called on.\n",
    "\n",
    "The `.map`-method works in two modes: bacht or non-batched. In either mode it receives a dictionary as input where each key represents a column if the dataset.\n",
    "In non-bachted mode the values of the input-dictionary are the values of one entry in the dataset. In batched mode they are lists with multiple entries.\n",
    "The following funcation only works in both modes since it converts both input formats to the same intermediate format, but in terms of speed batched-mode should be preffered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24ac3ce-9523-434c-9f40-beaf1a2fa796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from random import seed as set_seed\n",
    "\n",
    "\n",
    "def make_shuffle_func(sep_token):\n",
    "    def shuffle_stories(entries, seed=42):\n",
    "        set_seed(seed)\n",
    "        entries_as_dicts = [\n",
    "            dict(zip(entries, values)) for values in zip(*entries.values())\n",
    "        ]\n",
    "        converted_entries = []\n",
    "        for entry in entries_as_dicts:\n",
    "            sents = [\n",
    "                entry[key]\n",
    "                for key in sorted(\n",
    "                    [key for key in entry.keys() if key.startswith(\"sentence\")],\n",
    "                    key=lambda x: int(x[-1]),\n",
    "                )\n",
    "            ]\n",
    "            sent_idx = list(range(len(sents)))\n",
    "            sents_with_idx = list(zip(sents, sent_idx))\n",
    "            shuffle(sents_with_idx)\n",
    "            text = f\"{sep_token} \" + f\" {sep_token} \".join(\n",
    "                [s[0] for s in sents_with_idx]\n",
    "            )\n",
    "            so_targets = [s[1] for s in sents_with_idx]\n",
    "            shuffled_entry = {\"text\": text, \"so_targets\": so_targets}\n",
    "            converted_entries.append(shuffled_entry)\n",
    "        new_entry = {\n",
    "            key: [entry[key] for entry in converted_entries]\n",
    "            for key in converted_entries[0]\n",
    "        }\n",
    "        return new_entry\n",
    "\n",
    "    return shuffle_stories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b724f4ac-8ea1-4eb4-b952-a80f9c3d4cb1",
   "metadata": {},
   "source": [
    "`[CLS]` is the special token of BERT directly derived models, which while pretraining learns a represenation of the whole input sequence. We will use it as our sentence-token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b932d011-0ed5-4601-8641-a5f2c439bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_func = make_shuffle_func(\"[CLS]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a2de85a-330c-48af-aab8-62f2288ae71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46750b45bc574e9c9a0ae17ccbcd0249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(map_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7508f-8bb5-474a-b2ca-071f03fa1937",
   "metadata": {},
   "source": [
    "Now our dataset has two additional columns: The `text`column contains the shuffled and concatenated sentences and the `so_targets` columnm contains the indicies of the sentences in the original order. For example in the first text in the dataset the first sentence in the shuffled text is at the 4th place in the original order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c96932-d8e3-45dc-a731-9aaddfe37728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence1': 'David noticed he had put on a lot of weight recently.',\n",
      " 'sentence2': 'He examined his habits to try and figure out the reason.',\n",
      " 'sentence3': \"He realized he'd been eating too much fast food lately.\",\n",
      " 'sentence4': 'He stopped going to burger places and started a vegetarian '\n",
      "              'diet.',\n",
      " 'sentence5': 'After a few weeks, he started to feel much better.',\n",
      " 'so_targets': [3, 1, 2, 4, 0],\n",
      " 'storyid': '8bbe6d11-1e2e-413c-bf81-eaea05f4f1bd',\n",
      " 'storytitle': 'David Drops the Weight',\n",
      " 'text': '[CLS] He stopped going to burger places and started a vegetarian '\n",
      "         'diet. [CLS] He examined his habits to try and figure out the reason. '\n",
      "         \"[CLS] He realized he'd been eating too much fast food lately. [CLS] \"\n",
      "         'After a few weeks, he started to feel much better. [CLS] David '\n",
      "         'noticed he had put on a lot of weight recently.'}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cb7f4a-fde1-4bf0-8468-32337668facd",
   "metadata": {},
   "source": [
    "Lastly we want to split our dataset into the subset: The train-set will be used for training, the validation set can be used to validate the performance while the training or for hyperparamter optimazation and the testset will be used for the final evulation of a trained and optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec97c46a-83d3-40d0-bd33-9bcfbbf5b38e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'so_targets', 'storyid', 'storytitle', 'text'],\n",
       "        num_rows: 42132\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'so_targets', 'storyid', 'storytitle', 'text'],\n",
       "        num_rows: 7373\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'so_targets', 'storyid', 'storytitle', 'text'],\n",
       "        num_rows: 3160\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "test_validation = train_test[\"test\"].train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": train_test[\"train\"],\n",
    "        \"test\": test_validation[\"train\"],\n",
    "        \"val\": test_validation[\"test\"],\n",
    "    }\n",
    ")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f60433-b35c-4565-a2b5-72599b79002d",
   "metadata": {},
   "source": [
    "Finally, we save the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
