{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61bdcab-7419-4000-a599-cdffceb281e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ca11c71-1279-4f80-8060-3a7cdc592be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novellen\n",
    "\n",
    "files = list(Path('/mnt/data/corpora/novellenschatz/novellenschatz3/Novellen im txt-Format/').glob('*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d81789-8a12-4076-bdaa-cb33e9eb4acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import re\n",
    "filename_splitter_regex = r'(.+[^,]),*_(.+?)-(.+)'\n",
    "\n",
    "def build_dataframe(files: List[Path]) -> pd.DataFrame:\n",
    "    data = []\n",
    "    for file in files:\n",
    "        filename = file.stem\n",
    "        match = re.search(filename_splitter_regex, filename)\n",
    "        if match is None:\n",
    "            print(filename)\n",
    "        lastname, firstname, title = match.group(1), match.group(2), match.group(3)\n",
    "        text = file.read_text()\n",
    "        data.append({'title': title, 'author_firstname': firstname, 'author_lastname': lastname, 'filename': filename, 'text': text})\n",
    "    return pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ab21ce-868c-4a01-9531-b35955c8f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_dataframe(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17a2a0eb-74bf-485c-a08c-4baab0670e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('novellenschatz.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ecce9f-d209-4a5c-a8ca-ad8ca27fa5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       86.000000\n",
       "mean     18649.186047\n",
       "std      11330.957496\n",
       "min       2933.000000\n",
       "25%      12478.750000\n",
       "50%      16604.000000\n",
       "75%      22906.250000\n",
       "max      65327.000000\n",
       "Name: text, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.str.split().apply(len).describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef2de2b1-2165-4d6a-9218-511f82ca1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d236962d-86b9-425c-a34c-fab4a665190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_sents'] = df.text.apply(lambda text: sent_tokenize(text, language='german')).apply(len)\n",
    "df['mean_sent_length'] = df.text.apply(lambda text: np.mean([len(s) for s in sent_tokenize(text, language='german')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a809716e-6bd4-42f2-9b74-6815f3937982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      86.000000\n",
       "mean      956.197674\n",
       "std       619.520855\n",
       "min       101.000000\n",
       "25%       534.000000\n",
       "50%       833.000000\n",
       "75%      1279.750000\n",
       "max      3709.000000\n",
       "Name: n_sents, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['n_sents'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e245330a-2b81-4c39-874e-03ff7c1adf25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     86.000000\n",
       "mean     128.497126\n",
       "std       34.126777\n",
       "min       64.378049\n",
       "25%      101.693678\n",
       "50%      124.939084\n",
       "75%      143.892011\n",
       "max      248.660422\n",
       "Name: mean_sent_length, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['mean_sent_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be01d24-57f6-49f6-ad52-525ea8b96e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df.text.apply(lambda text: sent_tokenize(text, language='german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478c9f89-011a-41ac-867a-cadcff34619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f5a72b3-8cc7-43fb-8514-85079c131410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "\n",
    "def shuffle_sentences(sents: List[str], window_size_mean = 10, window_size_std: int = 3, random_state: int = 42) -> Tuple[List[str], List[int]]: \n",
    "    \"\"\"\n",
    "    Splits a text (represented as list of sentences) into chunks with random size and shuffles the sentences within each chunk.\n",
    "    Returns:\n",
    "        List of tuples: The shuffled senteces and their indices in the original text order.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    orig_sents = np.array(sents.copy())\n",
    "    sents = sents.copy()\n",
    "    idx = list(range(len(sents)))\n",
    "    shuffled_sents = []\n",
    "    shuffled_idx = []\n",
    "    while sents:\n",
    "        n_sents = int(round(np.abs(np.random.normal(loc=window_size_mean, scale=window_size_std))))\n",
    "        n_sents = n_sents if n_sents >= 2 else 2\n",
    "        n_sents = min(n_sents, len(sents))\n",
    "        selected_idx = np.array([idx.pop(0) for _ in range(n_sents)])\n",
    "        selected_sents = np.array([sents.pop(0) for _ in range(n_sents)])\n",
    "        np.random.shuffle(selected_idx)\n",
    "        shuffled_selected_sents = orig_sents[selected_idx]\n",
    "        shuffled_sents.append(shuffled_selected_sents)\n",
    "        shuffled_idx.append(selected_idx)\n",
    "        # TODO Remove me\n",
    "        assert len(shuffled_selected_sents) == len(selected_idx)\n",
    "    return list(zip(shuffled_sents, shuffled_idx))\n",
    "\n",
    "\n",
    "def make_prepare_function(tokenizer):\n",
    "    def prepare_sentence_ordering_dataset(entries):\n",
    "\n",
    "        # Convert to list format [{k0: value00, k1: value10}, {k0: value01, k1: value11}]\n",
    "        entries_as_dicts = [dict(zip(entries, values)) for values in zip(*entries.values())]\n",
    "\n",
    "        converted_entries = []\n",
    "        for entry in entries_as_dicts:\n",
    "            text = entry['text']\n",
    "            sents = sent_tokenize(text, language='german')\n",
    "            shuffled = shuffle_sentences(sents)\n",
    "            train_instances = []\n",
    "            for shuffled_sents, shuffled_orig_idx in shuffled:\n",
    "                train_instance = entry.copy()\n",
    "                train_instance.pop('text')\n",
    "                train_instance['orig_idx'] = shuffled_orig_idx\n",
    "                train_text = f'{tokenizer.cls_token} ' + f' {tokenizer.cls_token} '.join(shuffled_sents)\n",
    "                # TODO Remove me\n",
    "                assert train_text.count(tokenizer.cls_token) == len(shuffled_orig_idx)\n",
    "                train_instance['text'] = train_text\n",
    "                train_instance['so_targets'] = shuffled_orig_idx.argsort()\n",
    "                train_instances.append(train_instance)\n",
    "            converted_entries.extend(train_instances)\n",
    "        \n",
    "        new_entry = {key: [entry[key] for entry in converted_entries] for key in converted_entries[0]}\n",
    "        return new_entry\n",
    "\n",
    "    \n",
    "    return prepare_sentence_ordering_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c13a0f3-3c7e-4718-aaa2-d9653b5c57a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def make_prepare_function(tokenizer):\n",
    "#    def prepare_sentence_ordering_dataset(entries):\n",
    "#        \n",
    "#        text = entry['text']\n",
    "#        sents = sent_tokenize(text, language='german')\n",
    "#        shuffled = shuffle_sentences(sents)\n",
    "#        train_instances = []\n",
    "#        for shuffled_sents, shuffled_orig_idx in shuffled:\n",
    "#            train_instance = entry.copy()\n",
    "#            train_instance.pop('text')\n",
    "#            train_instance['orig_idx'] = shuffled_orig_idx\n",
    "#            train_text = tokenizer.cls_token.join(shuffled_sents)\n",
    "#            train_instance['text'] = train_text\n",
    "#            #train_instances['so_targets'] = 1 / (shuffled_orig_idx.argsort() + 1) use later\n",
    "#            \n",
    "#            train_instance['so_targets'] = shuffled_orig_idx.argsort()\n",
    "#            train_instances.append(train_instance)\n",
    "#        return train_instances\n",
    "#    return prepare_sentence_ordering_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25349d7a-6a37-46cf-a678-c646abd65d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d4fbefc-7b85-4169-a2c3-a75288d978ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_sentence_ordering_dataset = make_prepare_function(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7d5731b-4d9c-4720-b44b-fa6d114f36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-aa38f69bc6e823bc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /mnt/data/users/keller/.cache/csv/default-aa38f69bc6e823bc/0.0.0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keller/.conda/envs/huggingface/lib/python3.8/site-packages/tqdm/std.py:1185: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  for obj in iterable:\n",
      "/home/keller/.conda/envs/huggingface/lib/python3.8/site-packages/tqdm/std.py:1185: FutureWarning: The warn_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  for obj in iterable:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /mnt/data/users/keller/.cache/csv/default-aa38f69bc6e823bc/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08879427995c444ca0500ce1d364b76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "dataset = Dataset.from_csv('novellenschatz.csv')\n",
    "\n",
    "# Because for each row we return multiple new ones we need to use batched mode ..\n",
    "dataset = dataset.map(lambda x: prepare_sentence_ordering_dataset(x), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beb29072-f7eb-4d0b-b6e6-534021e0dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "test_validation = train_test['test'].train_test_split(test_size=0.3, seed=42)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'test': test_validation['train'],\n",
    "    'val': test_validation['test']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2768e7f-ae6e-4e73-907c-8c7491deaacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['author_firstname', 'author_lastname', 'filename', 'orig_idx', 'so_targets', 'text', 'title'],\n",
       "        num_rows: 6432\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['author_firstname', 'author_lastname', 'filename', 'orig_idx', 'so_targets', 'text', 'title'],\n",
       "        num_rows: 1125\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['author_firstname', 'author_lastname', 'filename', 'orig_idx', 'so_targets', 'text', 'title'],\n",
       "        num_rows: 483\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96047b08-ad5c-4961-8b72-bef06e81b379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('novellenschatz4so')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c051507b-7ad5-4ba4-b69b-15bef9081b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r novellenschatz4so/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b4647-11e0-4dd7-984e-48f397ce96d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
