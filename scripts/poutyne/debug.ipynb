{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from poutyne import Model\n",
    "from poutyne_transformers import ModelWrapper, MetricWrapper, TransformerCollator\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import load_from_disk\n",
    "from poutyne_modules import (\n",
    "    so_data_collator,\n",
    "    make_tokenization_func,\n",
    "    make_rename_func,\n",
    "    PoutyneSequenceOrderingLoss,\n",
    "    make_compute_metrics_functions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_OR_PATH = \"bert-base-cased\"\n",
    "LEARNING_RATE = 3e-5\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VAL_BATCH_SIZE = 16\n",
    "DEVICE = \"cuda:0\"\n",
    "N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "transformer = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME_OR_PATH, return_dict=True, num_labels=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/rocstories/train/cache-b0bad00b2d348ae1.arrow\n",
      "Loading cached processed dataset at ../data/rocstories/test/cache-59718d4747a1198d.arrow\n",
      "Loading cached processed dataset at ../data/rocstories/val/cache-a08ecbcb0ab07ce7.arrow\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(\"../data/rocstories/\")\n",
    "# Downsampling for debugging...\n",
    "# dataset = dataset.filter(lambda _, index: index < 300, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/rocstories/train/cache-53c3e65c13787504.arrow\n",
      "Loading cached processed dataset at ../data/rocstories/test/cache-dfaaa7d79a46d423.arrow\n",
      "Loading cached processed dataset at ../data/rocstories/val/cache-ce1c1014001a7d07.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenization_func = make_tokenization_func(\n",
    "    tokenizer=tokenizer,\n",
    "    text_column=\"text\",\n",
    "    add_special_tokens=False,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "dataset = dataset.map(tokenization_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/rocstories/train/cache-5b833dd8c1859483.arrow\n",
      "Loading cached processed dataset at ../data/rocstories/test/cache-b96a85dd76e25ba5.arrow\n",
      "Loading cached processed dataset at ../data/rocstories/val/cache-4d9643d138ffa502.arrow\n"
     ]
    }
   ],
   "source": [
    "rename_target_column = make_rename_func({\"so_targets\": \"labels\"}, remove_src=True)\n",
    "dataset = dataset.map(rename_target_column, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'storyid', 'storytitle', 'text', 'token_type_ids'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'storyid', 'storytitle', 'text', 'token_type_ids'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['attention_mask', 'input_ids', 'labels', 'sentence1', 'sentence2', 'sentence3', 'sentence4', 'sentence5', 'storyid', 'storytitle', 'text', 'token_type_ids'],\n",
       "        num_rows: 300\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(\n",
    "    [\"text\", \"storyid\", \"storytitle\"] + [f\"sentence{i}\" for i in range(1, 6)]\n",
    ")\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = TransformerCollator(\n",
    "    y_keys=[\"labels\", \"input_ids\"], custom_collator=so_data_collator, remove_labels=True\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset[\"train\"], batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset[\"val\"], batch_size=VAL_BATCH_SIZE, collate_fn=collate_fn\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset[\"test\"], batch_size=VAL_BATCH_SIZE, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapped_transformer = ModelWrapper(transformer)\n",
    "\n",
    "optimizer = AdamW(wrapped_transformer.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = PoutyneSequenceOrderingLoss(target_token_id=tokenizer.cls_token_id)\n",
    "\n",
    "metrics = [\n",
    "    MetricWrapper(func)\n",
    "    for func in make_compute_metrics_functions(tokenizer.cls_token_id)\n",
    "]\n",
    "\n",
    "model = Model(\n",
    "    wrapped_transformer, optimizer, loss_fn, batch_metrics=metrics, device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenClassifierOutput(loss=None, logits=tensor([[[-6.6118e-01],\n",
      "         [-5.6550e-01],\n",
      "         [-7.2178e-01],\n",
      "         ...,\n",
      "         [-6.8481e-01],\n",
      "         [-6.3319e-01],\n",
      "         [-6.7567e-01]],\n",
      "\n",
      "        [[-6.0914e-01],\n",
      "         [-5.1531e-01],\n",
      "         [-7.3804e-01],\n",
      "         ...,\n",
      "         [-6.1154e-01],\n",
      "         [-6.5949e-01],\n",
      "         [-8.4530e-01]],\n",
      "\n",
      "        [[-5.9103e-01],\n",
      "         [-3.2422e-01],\n",
      "         [-7.1828e-01],\n",
      "         ...,\n",
      "         [-5.8666e-01],\n",
      "         [-2.6737e-01],\n",
      "         [-6.8603e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-5.6462e-02],\n",
      "         [-7.6171e-01],\n",
      "         [-7.4970e-01],\n",
      "         ...,\n",
      "         [-8.2083e-01],\n",
      "         [ 4.2890e-02],\n",
      "         [-6.1885e-01]],\n",
      "\n",
      "        [[-6.4896e-01],\n",
      "         [-5.5668e-01],\n",
      "         [-7.3285e-01],\n",
      "         ...,\n",
      "         [ 1.0430e-03],\n",
      "         [-6.8947e-01],\n",
      "         [-7.2553e-01]],\n",
      "\n",
      "        [[-9.4805e-01],\n",
      "         [-7.6723e-01],\n",
      "         [-1.0490e+00],\n",
      "         ...,\n",
      "         [-7.1508e-01],\n",
      "         [-8.2326e-01],\n",
      "         [-7.7747e-01]]], device='cuda:0', grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "{'labels': tensor([[3, 1, 2, 4, 0],\n",
      "        [3, 2, 0, 4, 1],\n",
      "        [3, 1, 2, 0, 4],\n",
      "        [1, 2, 3, 4, 0],\n",
      "        [4, 3, 2, 0, 1],\n",
      "        [2, 1, 0, 3, 4],\n",
      "        [1, 3, 0, 2, 4],\n",
      "        [4, 0, 1, 2, 3]], device='cuda:0'), 'input_ids': tensor([[ 101, 1124, 2141,  ...,    0,    0,    0],\n",
      "        [ 101, 2545,  112,  ...,    0,    0,    0],\n",
      "        [ 101, 1109, 3111,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 1332, 1152,  ...,    0,    0,    0],\n",
      "        [ 101, 1799, 1199,  ...,    0,    0,    0],\n",
      "        [ 101, 1284, 1879,  ...,    0,    0,    0]], device='cuda:0')}\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11049/971857692.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, train_generator, valid_generator, epochs, steps_per_epoch, validation_steps, batches_per_step, initial_epoch, verbose, progress_options, callbacks)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_generator_n_batches_per_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatches_per_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_generator_one_batch_per_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36m_fit_generator_one_batch_per_step\u001b[0;34m(self, epoch_iterator, callback_list)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_step_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36m_fit_batch\u001b[0;34m(self, x, y, callback, step, return_pred)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m         loss_tensor, metrics, pred_y = self._compute_loss_and_metrics(x,\n\u001b[0m\u001b[1;32m    634\u001b[0m                                                                       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m                                                                       \u001b[0mreturn_loss_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/huggingface_latest/lib/python3.9/site-packages/poutyne/framework/model.py\u001b[0m in \u001b[0;36m_compute_loss_and_metrics\u001b[0;34m(self, x, y, return_loss_tensor, return_pred)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0mpred_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_loss_tensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/data/users/keller/Uni/trf_training_tut/scripts/poutyne/poutyne_modules.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, outputs, targets)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# To avoid exploding gradients, we norm them to be in range 0 <-> 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Also we need to remove the padding entries (-100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "model.fit_generator(train_dataloader, val_dataloader, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mTest steps: \u001b[36m19 \u001b[32m3.72s \u001b[35mtest_loss:\u001b[94m 1.883897\u001b[0m                                                \n"
     ]
    }
   ],
   "source": [
    "test_data = model.evaluate_generator(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8838973759114743"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8177ced577e5e7ca1f23d2becaf3e3ba70a92964f79ac2de86d55d71bd01756a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
